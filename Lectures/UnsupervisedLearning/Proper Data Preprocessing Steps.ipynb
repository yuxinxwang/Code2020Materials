{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper Data Preprocessing Steps\n",
    "\n",
    "In this notebook we take a quick aside on how we should preprocess both our training and testing data. We'll review some more advanced pipeline techniques that were touched upon in Regression Notebook 7 as well.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "We'll:\n",
    "<ul>\n",
    "    <li>emphasize the importance of fitting transformers to the training data not the test data,</li>\n",
    "    <li>show why pipelines really are useful,</li>\n",
    "    <li>give a quick review of more advanced pipeline techniques that are also covered in Regression Notebook 7.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit, transform, fit_transform, and train vs test data\n",
    "\n",
    "Many people are confused about how to properly preprocess data for example here is a image with over ten questions about the proper application of `StandardScaler` alone.\n",
    "<img src=\"train_test_question.png\" style=\"width:70%;\"></img>\n",
    "\n",
    "While this may be review for many of you, it is such an important concept that it bears repeating now that we've got a large array of preprocessing techniques.\n",
    "\n",
    "Let's start with a simple `StandardScaler` example.\n",
    "\n",
    "Recall that `StandardScaler` takes in a data set and subtracts off the arithmetic mean and divides that by the sample standard deviation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a>, i.e.\n",
    "$$\n",
    "\\frac{\\bullet - \\overline{X}}{s_X}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We now load some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 10*np.random.randn(1000) + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(X, test_size = .25, random_state = 440, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you scale the training set you subtract off the training mean and divide by the training sample deviaition. What about for the test set? \n",
    "\n",
    "This is where people often get confused. While your instinct may be to scale the test data by subtracting off the test mean and dividing by the test sample deviation, this is NOT the correct approach. Instead you scale the test data by subtracting off the training mean and dividing by the training deviation. Counterituitive I know, but this is the scaling procedure that we used to train our algorithm, so we have to repeat it when we predict on new data, like the test set.\n",
    "\n",
    "This is exactly why `sklearn` `transformers` have a `fit`, a `transform`, and a `fit_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the means of the test and train\n",
    "print(\"Train Mean\",np.mean(X_train))\n",
    "print(\"Test Mean\",np.mean(X_test))\n",
    "print(\"Train SD\",np.std(X_train))\n",
    "print(\"Test SD\",np.std(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scale = scaler.fit_transform(X_train.reshape(-1,1))\n",
    "X_test_scale = scaler.transform(X_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Mean\",np.mean(X_train_scale))\n",
    "print(\"Test Mean\",np.mean(X_test_scale))\n",
    "print(\"Train SD\",np.std(X_train_scale))\n",
    "print(\"Test SD\",np.std(X_test_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the slight difference here, the scale training set has a mean that is essentially $0$, but not the test set. Let's see what happens if I perturb the test set a little. Go ahead and play around with the value of perturb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "perturb = 100\n",
    "\n",
    "X_train_scale = scaler.fit_transform(X_train.reshape(-1,1))\n",
    "X_test_scale = scaler.transform(X_test.reshape(-1,1) + perturb)\n",
    "\n",
    "print(\"Train Mean\",np.mean(X_train_scale))\n",
    "print(\"Test Mean\",np.mean(X_test_scale))\n",
    "print(\"Train SD\",np.std(X_train_scale))\n",
    "print(\"Test SD\",np.std(X_test_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "On your own time go through and find what is wrong with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "X = np.array([2,4])*np.random.randn(100,2) + [-1,2]\n",
    "\n",
    "# I need to scale my data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Now I do a train test split\n",
    "X_train, X_test = train_test_split(X_scaled,test_size=.2,random_state=44,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This line of code is correct!\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Data\n",
    "X = np.random.randn(1000,50) + np.random.randint(-100,100,(1000,50))\n",
    "\n",
    "# train test split\n",
    "X_train, X_test = train_test_split(X,test_size=.1,shuffle=True,random_state=44)\n",
    "\n",
    "# I want to perform PCA\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This chunk of code is correct!\n",
    "from sklearn.preprocessing import PolynomialFeatures,FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a processing function\n",
    "def process(X):\n",
    "    process_X = np.zeros((np.shape(X)[0],4))\n",
    "    \n",
    "    process_X[:,0] = X[:,0]\n",
    "    process_X[:,1] = np.sqrt(X[:,1])\n",
    "    process_X[:,2] = X[:,2]\n",
    "    \n",
    "    scale = StandardScaler()\n",
    "    process_X[:,1:3] = scale.fit_transform(X[:,1:3])\n",
    "    \n",
    "    process_X[:,3] = process_X[:,0]*process_X[:,1]\n",
    "    \n",
    "    \n",
    "    return process_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some data\n",
    "X = np.zeros((1000,3))\n",
    "\n",
    "X[:,0] = np.random.randint(0,2,1000)\n",
    "X[:,1] = 5*np.random.random(1000) + 10\n",
    "X[:,2] = 10*np.random.randn(1000) - 12\n",
    "\n",
    "# train test split\n",
    "X_train, X_test = train_test_split(X,test_size=.1,shuffle=True,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pipe\n",
    "pipe = Pipeline([('process',FunctionTransformer(process))])\n",
    "\n",
    "process_train = pipe.fit_transform(X_train)\n",
    "process_test = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Reminder on More Advanced Pipelines\n",
    "\n",
    "The last example in the practice set illustrates the need for more advanced pipelins.\n",
    "\n",
    "Luckily we introduced these back in Notebook 7.\n",
    "\n",
    "We quickly review them now before signing off.\n",
    "\n",
    "The key features we require are that things like scalers, imputers, pca, and other transformers need a `fit`, a `transform`, and a `fit_transform` method.\n",
    "\n",
    "This can all be done with `sklearn`.\n",
    "\n",
    "We'll end with an example that features categorical and continuous variables.\n",
    "\n",
    "We want to one hot encode the categorical variables and we want to scale the continuous then put them through PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "X = np.zeros((1000,9))\n",
    "\n",
    "X[:,0] = np.random.randint(0,3,1000)\n",
    "X[:,1:8] = np.random.randn(1000,7)\n",
    "X[:,8] = X[:,1] + 2*X[:,3] - 4*X[:,6] + np.random.randn(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create this function that takes in \n",
    "# X and makes one hot encoded columns\n",
    "def get_X_ready(X):\n",
    "    new_X = np.zeros((np.shape(X)[0],10))\n",
    "    \n",
    "    # one hot encode\n",
    "    new_X[X[:,0]==0,0] = 1\n",
    "    new_X[X[:,0]==1,1] = 1\n",
    "\n",
    "    # copy the rest\n",
    "    new_X[:,2:] = X[:,1:]\n",
    "    \n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows you to maek\n",
    "# a custom transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our custom transformer\n",
    "# It should take in our X with the one hot encoded columns\n",
    "# and return the scaled continuous columns \n",
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    #Class Constructor \n",
    "    # This allows you to initiate the class when you call\n",
    "    # Scaler\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with\n",
    "        # the StandardScaler method\n",
    "        self.StandardScaler = StandardScaler()\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\"\n",
    "    # StandardScaler's fit method using only the\n",
    "    # columns I want\n",
    "    def fit(self, X, y = None ):\n",
    "        self.StandardScaler.fit(X[:,2:])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns I want\n",
    "    # and return it with scaled columns\n",
    "    def transform(self, X, y = None):\n",
    "        X[:,2:] = self.StandardScaler.transform(X[:,2:])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# we now make a custom PCA transform\n",
    "class CustomPCA(BaseEstimator, TransformerMixin):\n",
    "    #Class Constructor \n",
    "    # This allows you to initiate the class when you call\n",
    "    # CustomPCA\n",
    "    def __init__(self):\n",
    "        # I want to initiate each object with\n",
    "        # the PCA method\n",
    "        self.PCA = PCA()\n",
    "    \n",
    "    # For my fit method I'm just going to \"steal\"\n",
    "    # PCA's fit method using only the\n",
    "    # columns I want\n",
    "    def fit(self, X, y = None ):\n",
    "        self.PCA.fit(X[:,2:])\n",
    "        return self\n",
    "    \n",
    "    # Now I want to transform the columns\n",
    "    # and return it with PCA\n",
    "    def transform(self, X, y = None):\n",
    "        X[:,2:] = self.PCA.transform(X[:,2:])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we put it all together with a pipe\n",
    "pipe = Pipeline([('get_X_ready',FunctionTransformer(get_X_ready)),\n",
    "                ('scale',Scaler()),\n",
    "                ('pca',CustomPCA())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test = train_test_split(X,test_size=.1,shuffle=True,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed training set\n",
    "X_train_processed = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed testing set\n",
    "X_test_processed = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check the means again\n",
    "np.mean(X_train_processed[:,2:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mean\n",
    "np.mean(X_test_processed[:,2:],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more advanced pipeline is more complicated python than what we've covered up to this point. It's okay if you don't get it right away!\n",
    "\n",
    "I encourage you to review this and Regression Notebook 7 to get more practice. It may also help to review object oriented programming in python, this is a helpful resource <a href=\"https://python.swaroopch.com/oop.html\">https://python.swaroopch.com/oop.html</a>.\n",
    "\n",
    "That's it for this aside, I hope this notebook was helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
