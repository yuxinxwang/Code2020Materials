{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction II, Manifold Learning\n",
    "\n",
    "In this notebook we end our material on unsupervised learning by introducing a large field of dimension reduction techniques, <i>manifold learning</i>.\n",
    "\n",
    "We'll first discuss the need for manifold learning then demonstrate a few popular techniques. A disclaimer that this is a large subfield with many techniques. You may want to check out the Wikipedia entry to see what other techniques exist, <a href=\"https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction\">https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction</a>.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "We'll\n",
    "<ul>\n",
    "    <li>work through a simple example where techniques like PCA and Multi-Dimensional Scaling fail,</li>\n",
    "    <li>discuss why those techniques fail, and how manifold techniques address those inadequacies,</li>\n",
    "    <li>see local linear embeddings,</li>\n",
    "    <li>see isomap, and</li>\n",
    "    <li>tSNE.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we'll be working through this excerpt, <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html\">https://jakevdp.github.io/PythonDataScienceHandbook/05.10-manifold-learning.html</a>, of the book <a href=\"http://shop.oreilly.com/product/0636920034919.do\">Python Data Science Handbook</a>.\n",
    "\n",
    "## Hello\n",
    "\n",
    "As I mentioned we'll demonstrate the need for manifold learning with a simple example.\n",
    "\n",
    "The following code was written by Jake VanderPlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hello(N=1000, rseed=42):\n",
    "    # Make a plot with \"HELLO\" text; save as PNG\n",
    "    fig, ax = plt.subplots(figsize=(4, 1))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)\n",
    "    fig.savefig('hello.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Open this PNG and draw random points from it\n",
    "    from matplotlib.image import imread\n",
    "    data = imread('hello.png')[::-1, :, 0].T\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(4 * N, 2)\n",
    "    i, j = (X * data.shape).astype(int).T\n",
    "    mask = (data[i, j] < 1)\n",
    "    X = X[mask]\n",
    "    X[:, 0] *= (data.shape[0] / data.shape[1])\n",
    "    X = X[:N]\n",
    "    return X[np.argsort(X[:, 0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our data\n",
    "X = make_hello(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it in 2D\n",
    "colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5), s = 20)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recognize this as HELLO under a number of different transformations: rotations, scalings, flips, and translations. All that needs to be preserved is the distance between pairwise points.\n",
    "\n",
    "For example if we rotate it it will still look like HELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More code from Jake\n",
    "def rotate(X, angle):\n",
    "    theta = np.deg2rad(angle)\n",
    "    R = [[np.cos(theta), np.sin(theta)],\n",
    "         [-np.sin(theta), np.cos(theta)]]\n",
    "    return np.dot(X, R)\n",
    "    \n",
    "X2 = rotate(X, 20) + 5\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X2[:, 0], X2[:, 1], **colorize)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind all of the techniques we review in this notebook is the thought that while we reduce the dimension of the data we want to preserve the \"distance\" between points, where our concept of distance will change depending on the technique.\n",
    "\n",
    "## Multidimensional Scaling (MDS)\n",
    "\n",
    "MDS looks at the problem of preserving distances as a problem of preserving pairwise Euclidean distances between each pair of points in the data set. It does this by computing an $n$ by $n$ matrix where the $i,j$ entry is the Euclidean distance between data point $i$ and data point $j$.\n",
    "\n",
    "For the mathematics behind how this distance is preserved I encourage you to read the wikipedia entry on MDS, <a href=\"https://en.wikipedia.org/wiki/Multidimensional_scaling\">https://en.wikipedia.org/wiki/Multidimensional_scaling</a>.\n",
    "\n",
    "We'll work through a silly example performing MDS on the rotated 2D data so you can see how it is implemented. Then you'll work on applying it to two 3D examples where our 2D data is embedded into a 3D space.\n",
    "\n",
    "Here is the `sklearn` docs page on MDS, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html\">https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS is stored in\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of output dimensions to be 2\n",
    "mds = MDS(n_components = 2)\n",
    "X_mds = mds.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_mds[:, 0], X_mds[:, 1], **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about for 1D?\n",
    "mds = MDS(n_components = 1)\n",
    "X_mds = mds.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_mds[:, 0], np.ones(len(X[:,0])), **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Below you'll work through applying MDS to two separate 3D embeddings of our 2D HELLO data. One embeds the data onto a 2D plane in 3-space. The other embeds into onto a 3D S surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More code from Jake\n",
    "def random_projection(X, dimension=3, rseed=42):\n",
    "    assert dimension >= X.shape[1]\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    C = rng.randn(dimension, dimension)\n",
    "    e, V = np.linalg.eigh(np.dot(C, C.T))\n",
    "    return np.dot(X, V[:X.shape[1]])\n",
    "    \n",
    "X3 = random_projection(X, 3)\n",
    "X3.shape\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],\n",
    "             **colorize)\n",
    "ax.view_init(azim=70, elev=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply MDS to X3 here.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result in 2D here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks again for more code Jake\n",
    "def make_hello_s_curve(X):\n",
    "    t = (X[:, 0] - 2) * 0.75 * np.pi\n",
    "    x = np.sin(t)\n",
    "    y = X[:, 1]\n",
    "    z = np.sign(t) * (np.cos(t) - 1)\n",
    "    return np.vstack((x, y, z)).T\n",
    "\n",
    "XS = make_hello_s_curve(X)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],\n",
    "             **colorize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply MDS to XS here.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result in 2D here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a comparison perform PCA on X3 and XS\n",
    "# plot the comparisons below in 2D\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the PCA transform on X3 here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the PCA transform on XS here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both MDS and PCA are good at dimensionality reduction on low dimensional data that has been embedded into a higher dimensional space in some \"linear\" way. For example, how we put HELLO into 3D by placing it on a plane.\n",
    "\n",
    "However, they tend to fail on lower dimensional data that has been embedded into higher dimensions in a nonlinear way, like with the S surface data.\n",
    "\n",
    "The underlying reason is that these techniques perform on the global scale (for MDS this meant looking at the pairwise Euclidean distances between all points), which inherently has an assumption that the embedding is onto a Euclidean space. However, this as we saw with the S surface this isn't always the case.\n",
    "\n",
    "So what to do? Enter the manifold of manifold learning.\n",
    "\n",
    "### <i>Manifold</i> Learning\n",
    "\n",
    "While the S shape is not Euclidean space, it is locally Euclidean. If you have ever asked what a manifold is, you've probably heard something like \"it's a space that you can think of as being locally Euclidean\". So where PCA and MDS fail is performing global measures of closeness.\n",
    "\n",
    "However, if we restrict ourselves to local measures of closeness we may be able to preserve the inherent structure of our data. Thinking in terms of our HELLO data set if we use local measures we may be able to recover the HELLO off the S curve when shrinking back to 2D.\n",
    "\n",
    "We now introduce three different techniques each of which measures local relationships between points in a slightly different way. We'll give a brief intuition into the technique but leave reading on the exact method to you.\n",
    "\n",
    "### Local Linear Embeddings\n",
    "\n",
    "To see a more thorough discussion read this introduction to the topic from Saul and Roweis, <a href=\"https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf\">https://cs.nyu.edu/~roweis/lle/papers/lleintro.pdf</a>.\n",
    "\n",
    "Local Linear Embeddings are in essence a $k$ nearest neighbors method. Using some sort of distance measure, for each observation, $X_i$, you attempt to reconstruct $X_i$ with a weighted sum of its $k$ neighbors. The algorithm then attempts to choose the $W$ that minimizes:\n",
    "$$\n",
    "\\sum_{i=1}^n \\left| X_i - \\sum_{j=1}^n W_{ij} X_j \\right|^2\n",
    "$$\n",
    "where $W_{ij} = 0$ for any $j$ that is not the index of one of $X_i$'s $k$ nearest neighbors, and $\\sum_j W_{ij} = 1$.\n",
    "\n",
    "To get the lower dimension (say it is $\\tilde{m}$), we choose an $n$ by $\\tilde{m}$ matrix $Y$ that minimizes:\n",
    "$$\n",
    "\\sum_{i=1}^n \\left| Y_i - \\sum_{j=1}^n W_{ij} Y_j \\right|^2\n",
    "$$.\n",
    "\n",
    "Let's see how to implement it using `sklearn`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html\">https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import it from manifold\n",
    "from sklearn.manifold import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a modified version of the algorithm to get nice\n",
    "# results. Check out the docs to see what that means\n",
    "lle = LocallyLinearEmbedding(n_neighbors=100, n_components=2,method='modified',\n",
    "                               eigen_solver='dense')\n",
    "X_lle = lle.fit_transform(XS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1], **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap\n",
    "\n",
    "To see more in depth discussions of the algorithm see here, <a href=\"https://web.mit.edu/cocosci/isomap/isomap.html\">https://web.mit.edu/cocosci/isomap/isomap.html</a> and <a href=\"https://blog.paperspace.com/dimension-reduction-with-isomap/\">https://blog.paperspace.com/dimension-reduction-with-isomap/</a>.\n",
    "    \n",
    "Isomap stands for isometric mapping. Here is the algorithm in a nutshell.\n",
    "\n",
    "The algorithm takes in the dataset and constructs \"neighborhood graphs\", where here we mean graph in the <a href=\"https://en.wikipedia.org/wiki/Graph_theory\">graph theoretic sense</a>. Then local distance is determined using distance measured using graph distance instead of Euclidean distance in the data space. The algorithm then tries to reduce the dimensionality while preserving the geodesic distance.\n",
    "\n",
    "Again for a more mathematical explanation check out the MIT link above.\n",
    "\n",
    "We now implement it on the HELLO S surface data. Here are the `sklearn` docs on `Isomap`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html\">https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import isomap from manifold\n",
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you control the number\n",
    "iso = Isomap(n_neighbors=160, n_components=2)\n",
    "X_iso = iso.fit_transform(XS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_iso[:, 0], X_iso[:, 1], **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Linear Embedding (LLE) and Isomap are quite similar. LLE often does not perform well on high dimensional data, but does quite well on lower dimensional data. In contrast Isomap has been demonstrated to perform quite well on high dimensional data.\n",
    "\n",
    "### t-distributed Stochastic Neighbor Embedding (tSNE)\n",
    "\n",
    "We'll give a brief overview of how the algorithm works here. For the rigorous treatment see Visualizing Data using tSNE by Laurens van der Maaten and Geoffrey Hinton, <a href=\"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\">http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf</a>.\n",
    "\n",
    "Here's a basic outline of the algorithm:\n",
    "1. For all points $x_i$, $x_j$, convert their Euclidean distance into a conditional probability $p_{j|i}$. This is done by imagining a Gaussian distribution around $x_i$ and then comparing the \"normal distance\" of $x_j$ vs the sum of all other \"normal distances\". Here's a precise mathematical formula:\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2/2\\sigma_i^2)}{\\sum_{k\\neq i} \\exp(-||x_i - x_k||^2/2\\sigma_i^2)}\n",
    "$$\n",
    "Think of $p_{j|i}$ as the probability that $x_i$ would choose $x_j$ as its neighbor. We take $p_{i|i} = 0$.\n",
    "2. For every point $x_i$ in high dimensional space we will have a low dimensional counterpart, $y_i$, to which we map $x_i$. Similar to $p_{j|i}$ we will have $q_{j|i}$ that gives the probability that $y_i$ would choose $y_j$ as a neighbor. Here is the precise mathematical formula they are using:\n",
    "$$\n",
    "q_{j|i} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k\\neq l} (1 + ||y_k - y_l||^2)^{-1}}\n",
    "$$\n",
    "We take $q_{i,i} = 0$. Note: this is where the $t$ comes from! The numerator and denominator of $q_{j|i}$ is from the probability density for the $t$ distribution with 1 degree of freedom.\n",
    "3. Now if we're preserving these pairwise distances well, then we should expect $p_{j|i}$ to be close to $q_{j|i}$. In order to do this a cost function that measures the difference between $p_{j|i}$ and $q_{j|i}$ is minimized using gradient descent. The optimal $y_i$s are then spit out by the algorithm.\n",
    "\n",
    "\n",
    "You may be wondering how $\\sigma_i$ is chosen. The idea behind letting $\\sigma_i$ vary with $i$ is that some regions of the data are much denser than others. Changing $\\sigma_i$ with $i$ allows the algorithm to essentially control the number of neighbors of $x_i$ it considers. We can control this in $tSNE$ by specifying a perplexity. Typical values for perplexity are in 5 to 50.\n",
    "\n",
    "If you'd like a more in depth non research paper explanation of tSNE check out this blog post, http://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/. I think the author does a great job explaining it.\n",
    "\n",
    "Let's apply the algorithm to HELLO S curve and see what happens. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\">https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set perplexity = 50\n",
    "tsne = TSNE(n_components = 2, perplexity=50, random_state = 440)\n",
    "X_tsne = tsne.fit_transform(XS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], **colorize)\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tSNE can only be used for data visualization, DO NOT use it for any sort of supervised learning task like classification or regression, as tempting as it might be. Check out this Stack Exchange post for a discussion as to why you can't <a href=\"https://stats.stackexchange.com/questions/340175/why-is-t-sne-not-used-as-a-dimensionality-reduction-technique-for-clustering-or\">https://stats.stackexchange.com/questions/340175/why-is-t-sne-not-used-as-a-dimensionality-reduction-technique-for-clustering-or</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold Learner Disadvantages\n",
    "\n",
    "Manifold Learners can do some pretty nifty things, but there are weaknesses to this approach of dimensionality reduction.\n",
    "\n",
    "<ul>\n",
    "    <li>Noise in the data can greatly impact the results,</li>\n",
    "    <li>The learners are highly dependent on the hyperparameters, in particular the number of neighbors chosen, we investigate this below,</li>\n",
    "    <li>These techniques don't often have an approach to determine the optimal dimension of the reduced data like the explained variance ratio for PCA,<li>\n",
    "    <li>They aren't as interpretable as PCA,</li>\n",
    "    <li>They can be computationally costly.</li>\n",
    "</ul>\n",
    "\n",
    "As such they are often used solely as a means of data exploration and visualization. You'll see in the HW that tSNE is quite excellent for data viz.\n",
    "\n",
    "## Practice\n",
    "\n",
    "Work through the following to gain more familiarity with these manifold learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore what happens when you change the number of \n",
    "## number of neighbors in the local linear embedding\n",
    "## on the XS data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore what happens when you change the number of \n",
    "## number of neighbors in the isomap on the XS data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore what happens when you change the perplexity \n",
    "## in the tSNE algorithm on the XS data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at the following swiss roll data\n",
    "X_swiss, _ = make_swiss_roll(5000)\n",
    "# Make it thinner\n",
    "X_swiss[:, 1] *= .5\n",
    "\n",
    "ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X_swiss)\n",
    "\n",
    "label = ward.labels_\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(15, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X_swiss[label == l, 0], X_swiss[label == l, 1], X_swiss[label == l, 2],\n",
    "               color=plt.cm.gist_rainbow(float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply whatever manifold learning techniques\n",
    "## you'd like to try and \"unfirl\" the roll\n",
    "## label the 2d data X_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this to plot when you're ready\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "for l in np.unique(label):\n",
    "    plt.scatter(X_2[label == l, 0], X_2[label == l, 1],\n",
    "               color=plt.cm.gist_rainbow(float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
