{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "Perceptrons are the fundamental building block of neural networks. They were introduced by Rosenblatt in 1960, <a href=\"https://ieeexplore.ieee.org/abstract/document/4066017\">https://ieeexplore.ieee.org/abstract/document/4066017</a>.\n",
    "\n",
    "The material in this notebook comes from the text <a href=\"https://link.springer.com/content/pdf/10.1007/978-3-319-94463-0.pdf\">Neural Networks and Deep Learning</a> by Charu C. Aggarawal.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "In this notebook you'll:\n",
    "<ul>\n",
    "    <li>Gain familiarity with the inspiration for neural networks,</li>\n",
    "    <li>See the formulation of the perceptron model,</li>\n",
    "    <li>Familiarize yourself with the jargon of neural nets,</li>\n",
    "    <li>Investigate how perceptrons are trained, and</li>\n",
    "    <li>Learn the linear limitations of perceptrons.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll import these, but I don't know how much \n",
    "## we'll use them\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Artificial Neural Network\n",
    "\n",
    "Neural Networks is a technique that that loosely tries to mimic the network of neurons that make up brains. The idea being that we're trying to create learning algorithms that copy in some sense how humans learn (although I think this explanation is starting to fall out of vogue, <a href=\"https://www.hilarispublisher.com/open-access/are-neural-networks-imitations-of-mind-jcsb-1000179.pdf\">https://www.hilarispublisher.com/open-access/are-neural-networks-imitations-of-mind-jcsb-1000179.pdf</a>).\n",
    "\n",
    "The building blocks of the vast complex network of the brain is a single neuron, while the building blocks of an artificial neural network is a <i>perceptron</i>.\n",
    "\n",
    "We start at the beginning with this simple building block and then expand into slightly more complex neural networks in Notebook 2.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "We'll learn in the setting of a supervised learning problem, but be aware that neural nets can be applied in both unsupervised learning and supervised learning. In particular we'll focus on classification. While we've been using $0$ and $1$ for our binary classification, today we'll keep things spicy by using $-1$ and $1$ instead.\n",
    "\n",
    "Suppose we have $n$ observations of $m$ features stored in $m$ $n$ by $1$ column vectors, $X_1, X_2, \\dots, X_m$, and as we've done throughout the course let $X = \\left(X_1 | X_2 | \\dots |X_m\\right)$, and we want to predict some target $y$. \n",
    "\n",
    "Let $\\sigma$ be some nonlinear function from $\\mathbb{R} \\rightarrow \\mathbb{R}$. For classification we take $\\sigma = \\text{sgn}$, the sign function, i.e. $\\sigma(x) = 1$ if $x>0$ and $\\sigma(x) = -1$ if $x<0$. In the language of neural networks we call $\\sigma$ an activation function.\n",
    "\n",
    "Recall in supervised learning we try and fit a model to approximate:\n",
    "$$\n",
    "y = f(X) + \\epsilon.\n",
    "$$\n",
    "\n",
    "Perceptrons do this like so:\n",
    "$$\n",
    "\\hat{y} = \\sigma(w_1 X_1 + w_2  X_2 + \\dots + w_m X_m) = \\sigma(Xw),\n",
    "$$\n",
    "where in a potential abuse of notation I take $\\sigma(Xw)$ to mean $\\sigma$ applied to each of the $n$ entries of $Xw$, and $w=\\left(w_1, w_2, \\dots, w_m\\right)^T$.\n",
    "\n",
    "If I let $x=\\left(x_1,x_2,\\dots,x_m\\right)$ denote a single observation then I can picture this like so.\n",
    "\n",
    "<img src = \"perceptron.png\" style=\"width:60%\"></img>\n",
    "\n",
    "This sort of diagram is known as the <i>architecture</i> of the neural network.\n",
    "\n",
    "The column of nodes are referred to as the input layer because these are the inputs into the perceptron, the output node has both $\\Sigma$ and $\\sigma$ because this is where our weighted sum ($\\Sigma$) and nonlinear transformation ($\\sigma$) occurs.  \n",
    "\n",
    "Note the while we did not include a bias term this can easily be done, we just left it out for simplicity.\n",
    "\n",
    "\n",
    "### Finding the $w$\n",
    "\n",
    "So how do we find the weights?\n",
    "\n",
    "First the weights are randomly selected. Then you use a single data point from the training set, $X^{(i)}$, and you calculate the error, $y^{(i)}-\\hat{y^{(i)}}$.\n",
    "\n",
    "You then update $w$ like so:\n",
    "$$\n",
    "w_{\\text{update}} = w_{\\text{current}} + \\alpha (y^{(i)}-\\hat{y^{(i)}}) X^{(i)} \n",
    "$$\n",
    "\n",
    "$\\alpha$ is the learning rate of the network.\n",
    "\n",
    "The perceptron will cycle through all of the training points and continue to adjust the weights until it converges to a weight vector $w$. Each cycle through the training set is called an <i>epoch</i>. \n",
    "\n",
    "Typically the training points are chosen at random without replacement.\n",
    "\n",
    "Note, that this can be performed small batches of training points, at which point the batches are chosen randomly without replacement.\n",
    "\n",
    "Also note that the algorithm can be rewritten to work in parallel, <a href=\"http://www.cs.cmu.edu/~wcohen/10-605/parallel-perceptrons.pdf\">http://www.cs.cmu.edu/~wcohen/10-605/parallel-perceptrons.pdf</a> and <a href=\"https://www.sciencedirect.com/science/article/pii/S0167819106800177\">https://www.sciencedirect.com/science/article/pii/S0167819106800177</a>.\n",
    "\n",
    "\n",
    "Also for a very simple example of training the perceptron check out this reference starting on page 10, <a href=\"http://hagan.okstate.edu/4_Perceptron.pdf\">http://hagan.okstate.edu/4_Perceptron.pdf</a>.\n",
    "\n",
    "## Practice\n",
    "\n",
    "Let's now get some brief practice implementing a perceptron in `sklearn`. We won't spend much time here as you'll see at the end of the practice.\n",
    "\n",
    "Here are the docs on `sklearn`'s implementation of perceptrons, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron is stored in sklearn.linear_model\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the perceptron to build a setosa classifier on the iris data set. The data is prepared for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Get the data and the targets here \n",
    "# Grab Features\n",
    "X = iris['data']\n",
    "\n",
    "# Grab Targets\n",
    "y = np.ones(np.shape(iris['target']))\n",
    "\n",
    "# Reclassify as setosa and not setosa\n",
    "y = np.where(iris['target']!=0, 0, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "y = np.array([1,-1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y == 1,0],X[y == 1,1], c = 'b', label = \"1\")\n",
    "plt.scatter(X[y == -1,0],X[y == -1,1], c = 'r', label = \"-1\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about a train test split. Just try to build a perceptron classifier on this data set. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final example of the Practice illustrates a huge limitation of the perceptron, it can't be used to calculate exclusive or, a key operation in logic.\n",
    "\n",
    "More generally a single perceptron is not capable of separating data sets that are not separable by a hyperplane, much like the maximal margin classifier. This severely hurt interest in the method back in the 1950s and 60s, <a href=\"https://en.wikipedia.org/wiki/Perceptron\">https://en.wikipedia.org/wiki/Perceptron</a>. \n",
    "\n",
    "However, if your data is linearly separable there is a proof that guarantees the perceptron will converge as well as an upper bound on the number of epochs it must endure to get there, see the previous wikipedia link.\n",
    "\n",
    "The linear limitation is precisely why we now end our time with the perceptron and move on to more complicated neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
