{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Networks\n",
    "\n",
    "The limitation on perceptron to linear decision boundaries stymied neural network development. However, eventually (the 1980s I believe) there was a break through that found an architecture that does allow for nonlinear decision boundaries, multilayer networks.\n",
    "\n",
    "In this notebook we review this model type and show you how to implement it in `sklearn`.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "In this notebook you'll:\n",
    "<ul>\n",
    "    <li>Observe the architecture of a multilayer network,</li>\n",
    "    <li>Be introduced to additional jargon like <i>hidden layer</i>,</li>\n",
    "    <li>Learn a theorem on the capabilities of multilayer networks,</li>\n",
    "    <li>See how these networks are fit,</li>\n",
    "    <li>Implement them in sklearn.</li>\n",
    "</ul>\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll import these, but I don't know how much \n",
    "## we'll use them\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network Architecture\n",
    "\n",
    "The class of multilayer networks with which we concern ourselves are known as <i>feed forward networks</i>. Let's first show an image and then use that image for our explanation.\n",
    "\n",
    "<img src=\"multilayer.png\" style=\"width:60%\"></img>\n",
    "\n",
    "As you might be able to see they are called feed forward networks because each layer feeds directly into the next one.\n",
    "\n",
    "In particular we have drawn a feed forward network with $2$ <i>hidden layers</i> each with dimension $3$, we call these hidden layers because we only see what is put into the input layer and what comes out of the output layer. So in some sense what goes on in the midden layers is \"hidden\" to us.\n",
    "\n",
    "Note that neural networks can have more complex architectures, but for both simplicity and practicality (`sklearn` only allows for feed forward networks) we stick with feed forward networks.\n",
    "\n",
    "## Mathematical Setup\n",
    "\n",
    "Let's now show how we can formulate the multilayer network mathematically. We get a bit notation heavy now.\n",
    "\n",
    "Suppose that we have $n$ observations of $m$ features, let $x$ represent a single observation as an $m$ by $1$ vector. We'll suppose we have $k$ hidden layers and that layer $l$ has $p_l$ nodes within it, and take $h_l$ to denote the vector corresponding to hidden layer $l$. Also suppose that the output layer has $o$ nodes. Let $W_1$ be a $p_1$ by $m$ weight matrix, for $l = 2,\\dots, k$ let $W_l$ be a $p_l$ by $p_{l-1}$ weight matrix, and let $W_{k+1}$ be an $o$ by $p_k$ weight matrix. Finally take $\\Phi$ to be some activation function.\n",
    "\n",
    "Then we can set up how the output of the network is calculated with these recursively defined equations:\n",
    "$$\n",
    "\\begin{array}{l l r}\n",
    "h_1 =  & \\Phi (W_1 x) & \\text{Input to Hidden Layer }1 \\\\\n",
    "h_{l+1} = & \\Phi (W_{l+1} h_l) \\ \\forall l = 1, 2, \\dots, k-1 & \\text{Hidden Layer } l \\text{ to Hidden Layer } l+1 \\\\\n",
    "\\hat{y} = & \\Phi(W_{k+1} h_k) & \\text{Hidden Layer } k \\text{ to Output Layer}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For now we'll stick with an abstract activation function, but we mention a number of options before ending the notebook.\n",
    "\n",
    "Also, sometimes you may see architecture diagrams that use represent the nodes as giant rectangles meant to represent the vectors in the recursive equations.\n",
    "\n",
    "<img src=\"multilayer2.png\" style=\"width:60%\"></img>\n",
    "\n",
    "## Implementing in `sklearn`\n",
    "\n",
    "Before worrying about how we can fit this algorihtm. Let's take a quick break from the math and show how we can fit a multilayer network in `sklearn`. We'll stick to classification with `MLPClassifier`, but you can also do regression tasks with `MLPRegressor`. Here are the documentation pages for both:\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</a>\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = make_moons(10000,noise=.3)\n",
    "X_test,y_test = make_moons(500,noise=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],c='c',label='0')\n",
    "plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],c='r',label='1')\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes an mlp classifier with 1 hidden layer\n",
    "# this layer has 500 nodes\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500,),max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-3,3,100);\n",
    "x2 = np.linspace(-2,2,100);\n",
    "x1v,x2v = np.meshgrid(x1,x2)\n",
    "\n",
    "X_grid = np.concatenate([x1v.reshape(-1,1),x2v.reshape(-1,1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mlp.predict(X_grid)\n",
    "\n",
    "test_acc = sum(mlp.predict(X_test) == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(X_grid[pred == 0,0],X_grid[pred==0,1],c='k',label=\"predicted 0\")\n",
    "plt.scatter(X_grid[pred == 1,0],X_grid[pred==1,1],c='antiquewhite',label=\"predicted 1\")\n",
    "plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],c='c',label='0',alpha=.5)\n",
    "plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],c='r',label='1',alpha=.5)\n",
    "\n",
    "plt.text(1,-1.75,\"Test Accuracy \" + str(np.round(test_acc,4)), fontsize=18)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Let's have you do a short break out session we'll work through two problems just so you get comfortable with the `sklearn` functionality then we'll go back to the mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's first show that mlps can\n",
    "## do what perceptrons can't\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "y = np.array([1,-1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit an MLPClassifier on these data,\n",
    "## can it separate them?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets fit a classifier on the make_circles data\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = make_circles(10000,noise=.4,factor=.2)\n",
    "X_test,y_test = make_circles(1000,noise=.4,factor=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(X_train[y_train==0,0],X_train[y_train==0,1],c='c',label='0')\n",
    "plt.scatter(X_train[y_train==1,0],X_train[y_train==1,1],c='r',label='1')\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit an MLP classifier to this data\n",
    "## and fit a svm with rbf kernel\n",
    "## Which do you expect to perform better using cv?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is a Multilayer Network Fit? Backpropagation\n",
    "\n",
    "We'll now introduce the process by which we fit a multilayer network, i.e. how do we find the optimal weights.\n",
    "\n",
    "Backpropagation is just a fancy name for the chain rule mixed with gradient descent. We'll demonstrate with two incredibly simple architectures. Here is the first.\n",
    "\n",
    "<img src=\"simple.png\" style=\"width:60%\">\n",
    "\n",
    "In this architecture we have: \n",
    "\n",
    "$$\n",
    "h_1 = \\Phi (w_1 x_1),\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\Phi (w_2 h_1),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\Phi(w_3 h_2).\n",
    "$$\n",
    "\n",
    "Backpropagation consists of a <i>forwards step</i> and a <i>backwards step</i>.\n",
    "\n",
    "### The Forwards Step\n",
    "\n",
    "\n",
    "Let $w = \\left(w_1, w_2, w_3\\right)^T$. As with the perceptron we initialize $w$ with random weights. Then run a randomly selected training point, $x^{(i)}$ through the network getting the values for each layer of the network along the way.\n",
    "\n",
    "So when the forward step is completed you have a $\\hat{y}$ and $h$s for each layer of the network.\n",
    "\n",
    "### The Backwards Step\n",
    "\n",
    "The backwards step is how we update our weights, $w$. Let our cost function be $C = (\\hat{y} - y)^2$.\n",
    "\n",
    "In order to update $w$ we use gradient descent, so $w_{\\text{new}} = w_\\text{old} - \\eta \\nabla C(w_\\text{old})$, where the gradient is taken with respect to $w$, $\\eta$ is a hyperparameter called the learning rate, and for the purposes of our derivation we'll assume that $C$ is differentiable with respect to all of the weights (there are work arounds for activation functions that aren't differentiable everywhere).\n",
    "\n",
    "The backwards step is where backpropagation gets its name, as we'll see now.\n",
    "\n",
    "Using the chain rule we can find $\\frac{\\partial C}{\\partial w_1}, \\frac{\\partial C}{\\partial w_2}, $ and $\\frac{\\partial C}{\\partial w_3}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_3} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_3} = 2\\left( \\hat{y} - y \\right) \\Phi'(w_3 h_2) h_2\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_2} \\frac{\\partial h_2}{\\partial w_2} = 2(\\hat{y} - y) \\Phi'(w_3 h_2) w_3 \\Phi'(w_2 h_1) h_1\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_2} \\frac{\\partial h_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_1} = 2(\\hat{y} - y) \\Phi'(w_3 h_2) w_3 \\Phi'(w_2 h_1) w_2 \\Phi'(w_1 x_1^{(i)}) x_1^{(i)}\n",
    "$$\n",
    "\n",
    "In all of the above expressions we take each of the values to be the one we found in forwards step.\n",
    "\n",
    "### The Gradient Adjustment\n",
    "\n",
    "Then before we randomly choose another training instance we update the weights by performing $w_{\\text{new}} = w_\\text{old} - \\eta \\nabla C(w_\\text{old})$\n",
    "\n",
    "### Epochs\n",
    "\n",
    "This is done cycling through all of the training points in random order, each cycle through is called an epoch.\n",
    "\n",
    "### That's It!\n",
    "\n",
    "While this was an incredibly simple example that's really all backpropagation is. The only thing that gets more complicated with more complex feed forward network architectures is the indexing, which makes for a computational nightmare in terms of keeping track, hence the simple example.\n",
    "\n",
    "### Common Adjustments to the Gradient Descent\n",
    "\n",
    "Two common adjustments come to the gradient descent steps. \n",
    "\n",
    "1. Sometimes in order to speed up calculations on all of the training points you'll perform batch gradient descent in which small batches of points are run through the forwards step with the same $w$ and then for the update you use the average of the batch's backwards step.\n",
    "\n",
    "2. Instead of selecting $\\eta$ by hand you can let it be a random value for each step. The idea being it can help you get out of local minima of the cost function.\n",
    "\n",
    "## Practice \n",
    "\n",
    "In this practice you'll do two things.\n",
    "\n",
    "1. Get practice calculating the backpropagation by hand, and\n",
    "\n",
    "2. Fitting a multiple output network on the MNIST data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "\n",
    "Look at this architecture that comes from the following blog post (don't cheat and just look up the solution though!), <a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>\n",
    "\n",
    "<img src=\"practice.png\" style=\"width:80%\"></img>\n",
    "\n",
    "This is both our first time with a multi-output network and a network with bias so I'll help you with the set up. To get started here are the formulas for $h_1$ and $h_2$.\n",
    "\n",
    "$$\n",
    "h_1 = \\Phi(w_1 x_1 + w_2 x_2 + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\Phi(w_3 x_1 + w_4 x_2 + b_1)\n",
    "$$\n",
    "\n",
    "Also in the case of a multi-output network our cost function is a sum of the two errors:\n",
    "$$\n",
    "C = (o_1 - y_1)^2 + (o_2 - y_2)^2,\n",
    "$$\n",
    "where $y = (y_1,y_2)$ and $o_1,o_2$ can be thought of as the $\\hat{y}$ in our simple example.\n",
    "\n",
    "For this problem:\n",
    "<ul>\n",
    "    <li>Set up the equations for $o_1$ and $o_2$ in terms of $h_1$ and $h_2$,</li>\n",
    "    <li>Calculate $\\partial C/\\partial w_5$, $\\partial C/ \\partial w_1$ and $\\partial C/\\partial b_2$.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may record your answers in the markdown block if you'd like to, math can be entered with typical latex commands with equations being contained in dollar signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. \n",
    "\n",
    "Load in the MNIST then make a training and test sets.\n",
    "\n",
    "Train an `MLPClassifier` on the training set and evaluate the test error. See how training different architectures affects the outcome.\n",
    "\n",
    "Note you may want to perform dimensionality reduction using PCA to speed up the run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "nums = pd.read_csv(\"https://raw.githubusercontent.com/cerndb/dist-keras/master/examples/data/mnist.csv\")\n",
    "\n",
    "X = np.array(nums.iloc[:,1:])\n",
    "y = np.array(nums.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Activation Functions\n",
    "\n",
    "Now let's briefly show the four activation functions, $\\Phi$ that are used by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the identity\n",
    "# which is technically not nonlinear but whatever\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(np.linspace(-2,2,100),np.linspace(-2,2,100), linewidth = 3)\n",
    "\n",
    "plt.xlim(-2,2)\n",
    "\n",
    "plt.title(\"The Identity Activation\",fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the logistic function\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "plt.plot(x,y, linewidth = 3)\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.text(-9,.8,\"$y = 1/(1+e^{-x})$\", fontsize=16)\n",
    "\n",
    "plt.title(\"The Logistic Activation\",fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the hyperbolic tan function\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.tanh(x)\n",
    "\n",
    "plt.plot(x,y, linewidth = 3)\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.text(-9,.8,\"$y = tanh(x)$\", fontsize=16)\n",
    "\n",
    "plt.title(\"The Hyperbolic Tangent Activation\",fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the Rectified Linear Unit [ReLU] function\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.linspace(-10,10,100)\n",
    "y[x<0] = 0\n",
    "\n",
    "plt.plot(x,y, linewidth = 3)\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.text(-9,8,\"$y = \\max\\{x,0\\}$\", fontsize=16)\n",
    "\n",
    "plt.title(\"The Rectified Linear Unit (ReLU) Activation\",fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default in both `sklearn`'s `MLPRegressor` and `MLPClassifier` is ReLU.\n",
    "\n",
    "## Universal Approximator\n",
    "\n",
    "It has been proven mathematically that \"a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters\", <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\">https://en.wikipedia.org/wiki/Universal_approximation_theorem</a>.\n",
    "\n",
    "So we've done it! We've found the best machine learning algorithm and we should end class here.\n",
    "\n",
    "<img src=\"wellyes.png\" style=\"width:60%\"><img>\n",
    "\n",
    "Continuing on from wikipedia: \"however, it does not touch upon the algorithmic learnability of those parameters.\"\n",
    "\n",
    "Meaning that while yes we can theoretically approximate any reasonable function with a high enough dimensional single hidden layer feed forward network, this is not always practically possible.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "It has been found that you can trade in the height of a single hiddent layer for increased depth and get similar results. Wanting to understand the possibilities and limitations of such architecture is where the field of deep learning comes from.\n",
    "\n",
    "Which leads us to the deficiencies of feed forward networks in practice.\n",
    "\n",
    "## Deficiencies\n",
    "\n",
    "1. Feed Forward Neural Nets can very easily overfit the training data. To control for this you can use regularization similar to lasso or ridge regression. You can also input early stopping rules which stops the backpropagation before overfitting occurs. Another technique is to employ ensembles of networks as an additional way to decrease variance of the model, for example with bagging.\n",
    "\n",
    "2. Gradients can vanish or explode when your networks get too deep because of the chain rule.\n",
    "\n",
    "3. Convergence can be slow and difficult.\n",
    "\n",
    "4. Cost functions often have many local minima that you can get stuck in when using normal gradient descent with a fixed learning rate.\n",
    "\n",
    "5. For complicated networks a normal laptop may not suffice and you'll need more powerful hardware.\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "The following were extremely helpful in understanding multilayer neural nets.\n",
    "\n",
    "This entire 4 video youtube series, <a href=\"https://www.youtube.com/watch?v=aircAruvnKk\">https://www.youtube.com/watch?v=aircAruvnKk</a>.\n",
    "\n",
    "This blog post, <a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>.\n",
    "\n",
    "This online book <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\">http://neuralnetworksanddeeplearning.com/chap2.html</a>.\n",
    "\n",
    "And the book mentioned in the perceptron notebook <a href=\"https://link.springer.com/content/pdf/10.1007/978-3-319-94463-0.pdf\">https://link.springer.com/content/pdf/10.1007/978-3-319-94463-0.pdf</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
