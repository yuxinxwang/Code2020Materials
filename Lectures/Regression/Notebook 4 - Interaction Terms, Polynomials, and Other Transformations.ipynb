{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Terms, Polynomials, and Other Transformations\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "So far we've learned the basics of regression and then expanded upon them to include multiple quantitative predictors and qualitative predictors. We've also seen how we can choose from an array of predictive models using cross-validation.\n",
    "\n",
    "In this notebook we'll continue to build up our linear regression skills with an array of techniques, specifically we'll cover:\n",
    "<ul>\n",
    "    <li>Interaction Terms by having a second round of beer,</li>\n",
    "    <li>How to include powers of features, where you'll model data from a data science interview problem set,</li>\n",
    "    <li>Encorporating other popular transformations by making a better advertising model,</li>\n",
    "    <li>We'll end by introducing some of sklearns preprocessing tools for faster model building/testing.</li>\n",
    "</ul>\n",
    "\n",
    "Let's get going like we alway do, import the packages we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms\n",
    "\n",
    "Let's look at the beer data from the MLR notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv\n",
    "beers = pd.read_csv(\"beer.csv\")\n",
    "\n",
    "# train test split\n",
    "beers_copy = beers.copy()\n",
    "\n",
    "# Then locate all the stouts and set Stout to 1\n",
    "beers_copy['Stout'] = 0\n",
    "beers_copy.loc[beers_copy.Beer_Type == \"Stout\",'Stout'] = 1\n",
    "\n",
    "beers_train = beers_copy.sample(frac=.75,random_state = 614)\n",
    "beers_test = beers_copy.drop(beers_train.index)\n",
    "\n",
    "beers_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot a regression with \"hue\" listed as Beer_Type\n",
    "sns.lmplot(data = beers_train, x=\"ABV\", y=\"IBU\", \n",
    "           hue = \"Beer_Type\", height=10,  \n",
    "           ci = False, legend=False)\n",
    "\n",
    "plt.xlabel(\"ABV\", fontsize=16)\n",
    "plt.ylabel(\"IBU\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seaborn.lmplot` suggests that our regression line depends on the `Beer_Type`. Let's recall the model we fit in MLR:\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\epsilon.\n",
    "$$\n",
    "\n",
    "We noted that this model only allows for different intercepts for Stouts and IPAs, but not slopes. \n",
    "\n",
    "\n",
    "However if we add in the term $\\beta_3 \\text{Stout} \\times \\text{ABV}$ like so,\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\beta_3 \\text{Stout} \\times \\text{ABV} + \\epsilon\n",
    "$$\n",
    "\n",
    "then when Stout $=0$ the slope of the line is $\\beta_1$ and when Stout $=1$ the slope of the line $\\beta_1 + \\beta_3$. The $\\text{Stout} \\times \\text{ABV}$ is the interaction term between Stout and ABV.\n",
    "\n",
    "Note in the Regression Homework you are asked to interpret the coefficient estimates for this model.\n",
    "\n",
    "Let's make an interaction term and then fit this new model with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the interaction term\n",
    "beers_train['Stout_ABV'] = beers_train['Stout']*beers_train['ABV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now make and fit the model\n",
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(beers_train[['ABV','Stout','Stout_ABV']],beers_train['IBU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))\n",
    "print(\"beta_3_hat is\",np.round(reg.coef_[2],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the two lines by limiting our prediction input\n",
    "stout_values = np.ones((100,3))\n",
    "\n",
    "# make the ABV values\n",
    "stout_values[:,0] = np.linspace(beers_train['ABV'].min(),beers_train['ABV'].max(),100)\n",
    "\n",
    "# now the interaction term\n",
    "stout_values[:,2] = stout_values[:,0]*stout_values[:,1]\n",
    "\n",
    "# make a stout prediction\n",
    "stout_pred = reg.predict(stout_values)\n",
    "\n",
    "# make ipa values\n",
    "ipa_values = np.zeros((100,3))\n",
    "ipa_values[:,0] = np.linspace(beers_train['ABV'].min(),beers_train['ABV'].max(),100)\n",
    "ipa_values[:,2] = ipa_values[:,0]*ipa_values[:,1]\n",
    "ipa_pred = reg.predict(ipa_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# plot stout values\n",
    "plt.scatter(beers_train.loc[beers_train.Stout == 1,'ABV'], \n",
    "               beers_train.loc[beers_train.Stout == 1,'IBU'],\n",
    "               c = 'blue', alpha = .8, label=\"Stout Training Data\")\n",
    "\n",
    "# plot the stout line\n",
    "plt.plot(stout_values[:,0], stout_pred, \"k--\", label=\"Stout Regression Line\")\n",
    "\n",
    "# plot ipa values\n",
    "plt.scatter(beers_train.loc[beers_train.Stout == 0,'ABV'], \n",
    "               beers_train.loc[beers_train.Stout == 0,'IBU'],\n",
    "               c = 'orange', alpha = .8, label=\"IPA Training Data\")\n",
    "\n",
    "# plot the ipa line\n",
    "plt.plot(ipa_values[:,0], ipa_pred, \"k\", label=\"IPA Regression Line\")\n",
    "\n",
    "plt.legend(fontsize = 14)\n",
    "\n",
    "plt.xlabel(\"ABV\",fontsize=16)\n",
    "plt.ylabel(\"IBU\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this looks more like the `seaborn.lmplot`!\n",
    "\n",
    "Adding interaction terms can also help with non-random residual plots as we'll see in a next example of this notebook.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Now I'll answer one or two questions before you go practice including interaction terms.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End of Questions\n",
    "\n",
    "## Practice\n",
    "\n",
    "Load the following data set called `inter`. Build a regression model regressing $y$ on $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = pd.read_csv(\"inter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Now not every relationship in the world is a line or a plane. \n",
    "\n",
    "Let's look at a synthetic dataset and see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"poly.csv\")\n",
    "\n",
    "print(\"There are\",len(df),\"observations.\")\n",
    "print(\"The columns are\",df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train test split\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_train = df_copy.sample(frac=.75,random_state = 614)\n",
    "df_test = df_copy.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got three variables $x_1$, $x_2$, and $y$.\n",
    "\n",
    "Let's use the scatter matrix to examine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_train, figsize=(12,12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there certainly appears to be a relationship between $x_1$ and $y$ and $x_2$ and $y$. While the relationship between $x_2$ and $y$ might be linear, the relationship between $x_1$ and $y$ is definitely not linear.\n",
    "\n",
    "One way to address this is to make a polynomial transformation of $x_1$. For instance $x_1^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['x1_sq'] = df_train['x1']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_train, figsize=(12,12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between $y$ and $x_1^2$ seems somewhat linear, let's now include it in a model:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\epsilon,\n",
    "$$\n",
    "and then we'll fit this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(df_train[['x1','x1_sq','x2']], df_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))\n",
    "print(\"beta_3_hat is\",np.round(reg.coef_[2],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the residual plot\n",
    "pred = reg.predict(df_train[['x1','x1_sq','x2']])\n",
    "\n",
    "res = df_train['y'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not random!\n",
    "\n",
    "An obviously nonrandom residual plot indicates that there is some signal in the data not being captured by our model. One way to address this is to add an interaction term. Let's try adding in $x_1 x_2$, so our model becomes:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\beta_4 x_1 x_2 + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add the interaction term to the df\n",
    "df_train['x1_x2'] = df_train['x1']*df_train['x2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(copy_X = True)\n",
    "\n",
    "reg.fit(df_train[['x1','x1_sq','x2','x1_x2']], df_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))\n",
    "print(\"beta_3_hat is\",np.round(reg.coef_[2],5))\n",
    "print(\"beta_4_hat is\",np.round(reg.coef_[3],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-examine the residual plot\n",
    "pred = reg.predict(df_train[['x1','x1_sq','x2','x1_x2']])\n",
    "\n",
    "res = df_train['y'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.scatter(pred, res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much better!\n",
    "\n",
    "### Which is the Better Model?\n",
    "\n",
    "This seems to be the best model for the data from inspecting the scatter plots, and checking on the residual plot. However, we could have performed cross-validation to see which of the two models has the lower cv average MSE. We won't do that here, but just be aware that is one way you could find the better predictive model.\n",
    "\n",
    "You may also wonder how we knew to stop at $x_1^2$, well we didn't! We could have continued on to $x_1^4$ and so on, but in general it is best to add as little to the model as you can get away with. We'll see why this is the case in the next notebook.\n",
    "\n",
    "### Do I Need to Include it?\n",
    "\n",
    "If you notice the coefficient on $x_1$ is close to $0$ in the interaction term model. It may be tempting to remove this feature from the model especially if the true relationship was:\n",
    "$$\n",
    "y = 2 + x_1^2 - 10 x_2 + x_1 x_2.\n",
    "$$\n",
    "However, there is no way for you to know ahead of time what the true relationship is between the target and the features, if there was there'd be no need for regression. \n",
    "\n",
    "To further illustrate this point, imagine the true relationship was such that:\n",
    "$$\n",
    "y \\propto x_1^2,\n",
    "$$\n",
    "if we do not include $x_1$ in our model we are limiting ourselves to parabolas of the form\n",
    "$$\n",
    "\\beta_0 + \\beta_1 x_1^2,\n",
    "$$\n",
    "which leaves out a number of possible parabolas.\n",
    "\n",
    "It is important to remember that anytime you make a model that includes a polynomial transformation you need to include all of the lesser powers as well. So with $x_1^2$ as the highest power you'd need to include $x_1$, with $x_1^3$ as the highest power you'd need to include $x_1^2$ and $x_1$, and so on for $x_1^n$.\n",
    "\n",
    "This also holds for interaction terms. If you include $x_1 x_2$ you need to include both $x_1$ and $x_2$ as predictors as well.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Again we'll take a couple of questions then time for another breakout session.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Question Time is Over\n",
    "\n",
    "## Practice\n",
    "\n",
    "The data labeled `df` below came from a job interview problem set. Eventually you'll have to build the best predictive model you can on the data. For now examine the relationship between `y` and `x1`. Build a model to predict `y` using `x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PredictiveModelingAssessmentData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Popular Transformations\n",
    "\n",
    "We can add in more than just polynomials. There are other popular transformations including $\\log$s, roots, $\\sin$, $\\cos$, $\\tan$, exponentials, and more.\n",
    "\n",
    "We'll work through an example returning to our `Advertising` data then you'll finish making a model for the interview data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "ads = pd.read_csv(\"Advertising.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train test split\n",
    "ads_copy = ads.copy()\n",
    "\n",
    "# Set aside 25% of the data\n",
    "# make 614 the random_state because I live in Columbus\n",
    "ads_train = ads_copy.sample(frac = .75, random_state = 614)\n",
    "ads_test = ads_copy.drop(ads_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first input the dataframe you want to see a \n",
    "# scatter matrix for\n",
    "# then enter figsize and other plotting arguments\n",
    "scatter_matrix(ads_train, figsize = (14,14), alpha = 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `TV` vs `sales` plot in the bottom left corner it looks like the relationship might not be linear. However let's look at a plot of `sales` vs root `TV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(np.sqrt(ads_train.TV),ads_train.sales)\n",
    "\n",
    "plt.xlabel(\"$\\sqrt{TV}$\", fontsize = 16)\n",
    "plt.ylabel(\"sales\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more linear. Let's replace `TV` in our model from Notebook 3 with root `TV`.\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\sqrt{\\text{TV}} + \\beta_2 \\text{radio} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in root tv to the df\n",
    "ads_train['sqrt_TV'] = np.sqrt(ads_train.TV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model object\n",
    "reg = LinearRegression(copy_X = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "reg.fit(ads_train[['sqrt_TV','radio']],ads_train['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"beta_0_hat is\",np.round(reg.intercept_,5))\n",
    "print(\"beta_1_hat is\",np.round(reg.coef_[0],5))\n",
    "print(\"beta_2_hat is\",np.round(reg.coef_[1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the residual plot\n",
    "pred = reg.predict(ads_train[['sqrt_TV','radio']])\n",
    "\n",
    "res = ads_train['sales'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again not random. Let's add in the interaction term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_train['sqrtTV_radio'] = ads_train['sqrt_TV'] * ads_train['radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model object\n",
    "reg = LinearRegression(copy_X = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "reg.fit(ads_train[['sqrt_TV','radio','sqrtTV_radio']],ads_train['sales'])\n",
    "\n",
    "# We'll want to look at these later\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-examine the residual plot\n",
    "pred = reg.predict(ads_train[['sqrt_TV','radio','sqrtTV_radio']])\n",
    "\n",
    "res = ads_train['sales'] - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "\n",
    "plt.scatter(pred,res)\n",
    "\n",
    "plt.xlabel(\"Predicted Values\", fontsize=16)\n",
    "plt.ylabel(\"Residuals\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect, but much better than before!\n",
    "\n",
    "Let's go with this as our model:\n",
    "$$\n",
    "\\text{sales} = \\beta_0 + \\beta_1 \\sqrt{\\text{TV}} + \\beta_2 \\text{radio} + \\beta_3 \\sqrt{\\text{TV}} \\times \\text{radio} + \\epsilon\n",
    "$$\n",
    "and use cross-validation to compare the predictive performance of this model in comparison to model we settled on in Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the KFold object from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# We'll need this when we fit models\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features used in notebook 3\n",
    "nb3_feats = ['TV','radio']\n",
    "\n",
    "# The features used here\n",
    "nb4_feats = ['sqrt_TV','radio','sqrtTV_radio']\n",
    "\n",
    "models = [nb3_feats, nb4_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copying functions from notebook 3\n",
    "\n",
    "# This gets our data for us\n",
    "def get_X_y(df,features,target):\n",
    "    # Returns X then y\n",
    "    return np.array(df[features]), np.array(df[target])\n",
    "\n",
    "# this calculates the mse\n",
    "def get_mse(model, X, y):\n",
    "    # get the prediction\n",
    "    pred = model.predict(X)\n",
    "    \n",
    "    # Returns the mse\n",
    "    return np.sum(np.power(pred-y,2))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make an array that will hold the mses\n",
    "# for both models\n",
    "# the columns represent each model\n",
    "MSEs = np.empty((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the kfold object\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 440)\n",
    "\n",
    "# Make a regression model\n",
    "reg = LinearRegression(copy_X = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each possible model\n",
    "for j in range(len(models)):\n",
    "    # get X and y\n",
    "    X, y = get_X_y(ads_train, models[j], 'sales')\n",
    "\n",
    "    # keep track of what split we're on\n",
    "    i = 0\n",
    "    \n",
    "    # Perform CV\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        # Get the cv train test split\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Cloning the regression makes a fresh regression \n",
    "        # model for each run\n",
    "        clone_reg = clone(reg)\n",
    "        \n",
    "        # fit the model\n",
    "        clone_reg.fit(X_train,y_train)\n",
    "        \n",
    "        MSEs[i,j] = get_mse(clone_reg, X_test, y_test)\n",
    "        \n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare the MSEs for each model\n",
    "mse_df = pd.DataFrame({'nb3_model':MSEs[:,0],'nb4_model':MSEs[:,1]})\n",
    "\n",
    "plt.figure(figsize = (8,10))\n",
    "\n",
    "# a strip plot just puts the points with some jittering\n",
    "# in a vertical strip, note strip can be horizontal\n",
    "# if you input the x = 'column' argument\n",
    "ax = sns.stripplot(data=mse_df, s=10)\n",
    "\n",
    "plt.xlabel(\"Model\", fontsize=16)\n",
    "plt.ylabel(\"CV Test Set MSE\", fontsize = 16)\n",
    "\n",
    "ax.set_xticklabels([\"Notebook 3 Model\", \"Notebook 4 Model\"],  fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the root and interaction terms we made in this notebook seems unequivocally better based on the cross validation outcome.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Again we'll answer a couple of questions then it is your turn.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End of Question Time\n",
    "\n",
    "## Practice\n",
    "\n",
    "Return to the dataset from an interview problem set. Using everything we've learned in this notebook try to make the best predicitve model you can. Determine this using cross-validation.\n",
    "\n",
    "Hint(s): The absolute lowest you can make the root mean square error is $1$ and you may want to consider using one of the non-polynomial transformations for $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn` for Preprocessing Data\n",
    "\n",
    "Now we've introduced a lot of data manipulation in this notebook. This can become quite tiresome and messy when we want to reread our code later.\n",
    "\n",
    "Luckily `sklearn` offers a number of preprocessing tools to make our code cleaner and easier to follow. We'll introduce `PolynomialFeatures`, `FunctionTransformer`, and `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PolynomialFeatures`\n",
    "\n",
    "We'll start with `PolynomialFeatures`. This is handy when we want to add in powers of features and interaction terms.\n",
    "\n",
    "We'll work with the following array with the features `x1` and `x2` in columns `0` and `1` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our Array\n",
    "X = np.random.randint(0,11,[10,2])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a PolynomialFeatures object\n",
    "# We want all degree 2 combinations of the features entered\n",
    "# interaction_only can be True if you only want interaction terms\n",
    "# include_bias adds in a column of ones if True\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform fits the preprocesser and then transforms the data\n",
    "# I've printed out the columns we produce\n",
    "# note it is important to write fit_transform\n",
    "# see what happens when I just write transform\n",
    "print([\"x1\",\"x2\",\"x1_sq\",\"x1_x2\",\"x2_sq\"])\n",
    "print(poly.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform fits the preprocesser and then transforms the data\n",
    "# I've printed out the columns we produce\n",
    "# This code works!\n",
    "print([\"x1\",\"x2\",\"x1_sq\",\"x1_x2\",\"x2_sq\"])\n",
    "print(poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data is to demonstrate\n",
    "# in the next code block\n",
    "X_star = np.random.randint(0,11,[10,2])\n",
    "X_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that since we've already\n",
    "# fit the transformer with fit_transform\n",
    "# we only have to transform it now\n",
    "poly.transform(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example where we only get the interactions\n",
    "poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "print([\"x1\",\"x2\",\"x1_x2\"])\n",
    "print(poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FunctionTransformer`\n",
    "\n",
    "`PolynomialFeatures` is great if we want the polynomial powers of all the predictors of the input array. However, as we've seen this is not always desirable. This is where `FunctionTransformer` comes into play. `FunctionTransformer` applies custom written functions to your data much like `PolynomialFeatures` applies the polynomial transformations. Let's use the `Advertising` data as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_train = ads_copy.sample(frac = .75, random_state = 614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn ads_train into np.arrays\n",
    "X_train = np.array(ads_train[['TV','radio']])\n",
    "y_train = np.array(ads_train['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that returns an array with the squareroot of the \n",
    "# TV column\n",
    "# and the sqrt_TV radio interaction\n",
    "def get_feats(X):\n",
    "    processed_X = np.empty((np.shape(X)[0], 3))\n",
    "    \n",
    "    # get sqrtTV\n",
    "    processed_X[:,0] = np.sqrt(X[:,0])\n",
    "    \n",
    "    # get radio\n",
    "    processed_X[:,1] = X[:,1]\n",
    "    \n",
    "    # get interaction\n",
    "    processed_X[:,2] = processed_X[:,0]*processed_X[:,1]\n",
    "    \n",
    "    return processed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A FunctionTransformer with the get_feats function\n",
    "# we defined above\n",
    "function = FunctionTransformer(get_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can transform our data with .transform\n",
    "# We'll demonstrate with the first 10 rows of the data\n",
    "print([\"sqrt_TV\",\"radio\",\"sqrtTV_radio\"])\n",
    "function.transform(X_train[1:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pipeline`s\n",
    "\n",
    "Up to this point the only data preprocessing we've done is different types of transformations.\n",
    "\n",
    "However, in the future we'll encounter data that requires more work. `sklearn` offers a nice way to package everything you'd like to do to your data with one line of code.\n",
    "\n",
    "We can think of the data process like a pipeline, we input the data at one end and output the prediction on the other. Along the way it may need to be cleaned, scaled, or transformed, and then finally used to fit a model and the prediction is spit out on the other side. The `sklearn` `Pipeline` object allows us to get a \"pipe\" ready for our data to flow through.\n",
    "\n",
    "Let's again look at the `Advertising` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Pipeline\n",
    "# The steps are put in the order you want them executed\n",
    "# They are stored in a list of arrays\n",
    "# First we want to apply our get_feats function, we'll call this 'preprocess'\n",
    "# Then we want to fit a regression model, we'll call this 'reg'\n",
    "pipe = Pipeline([('preprocess',FunctionTransformer(get_feats)),\n",
    "                ('reg',LinearRegression(copy_X = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we call the pipeline just like a regression object\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the regression coefficients like so\n",
    "# each part of the pipe is stored like a dictionary\n",
    "print(pipe['reg'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a prediction using .predict\n",
    "# just like with a normal regression object\n",
    "pipe.predict(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline`s are quite useful, especially when we have additional preprocessing steps. They also make the process of measuring testing error much quicker to implement because we don't have to make transformed data versions of the test data by hand like before. That's what the `Pipeline` is for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Make a `Pipeline` to preprocess and fit the interview data to the regression model you decided on earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook. In Notebook 5 will touch on some common linear regression issues and introduce the bias variance tradeoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
