{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting, the Bias-Variance Trade-Off, and Multicollinearity\n",
    "\n",
    "In this notebook we'll touch on a few issues that can pop up in regression. We'll also introduce the bias-variance trade-off, which is present in all of supervised learning, not just regression.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "In particular we'll:\n",
    "\n",
    "<ul>\n",
    "    <li>Touch on the multicollinearity problem for regression,</li>\n",
    "    <li>Show a few ways we can overfit the data,</li>\n",
    "    <li>Introduce the bias-variance trade-off in general,</li>\n",
    "    <li>Give a regression specific example of the bias-variance trade-off.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "One common issue that can arise when fitting regression models is multcollinearity. This is where some of your predictors are correlated with one another. Let's look at a synthetic example then you'll play around with some BMI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 and x2 are our features\n",
    "x1 = np.random.random(200)\n",
    "\n",
    "# but x2 is actually a function of x1\n",
    "x2 = 1-x1+.2*np.random.randn(200)\n",
    "\n",
    "# y is the target\n",
    "y = x1 + .2*np.random.randn(200)\n",
    "\n",
    "# Hold this in a df\n",
    "df = pd.DataFrame({'x1':x1,'x2':x2,'y':y})\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_train = df_copy.sample(frac = .75, random_state = 440)\n",
    "df_test = df_copy.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df_train, figsize=(10,10), alpha = 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't know the true relationship between $y$ and $x_1$ we may be tempted to just toss in $x_1$ and $x_2$ into a model and call it a day, but as we know $x_2$ is a function of $x_1$. Letting $X$ denote the feature matrix with a column of ones, one column of $X$ is a linear combination of the other two. When this occurs the matrix $X^T X$ is not of full rank, which means it is not invertible, but remember\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T y.\n",
    "$$\n",
    "\n",
    "So not being invertible is a big problem for us. Fortunately it is rarely the case that one predictor is a perfect linear combination of other predictors. However, one feature being highly correlated with other features can make a matrix close to singular, which is no good for your computer.\n",
    "\n",
    "When features are highly correlated with one another we can get varying model output depending on the features we put into the model. Let's bring back the `statsmodel` package to examine this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to add in the constant for sm.OLS\n",
    "df_train['const'] = 1\n",
    "\n",
    "# Fit a model with both x1 and x2\n",
    "lr = sm.OLS(df_train['y'], df_train[['const','x1','x2']])\n",
    "\n",
    "# Now fit the model\n",
    "fit = lr.fit()\n",
    "\n",
    "# fit.summary makes a snazy table for us to look at\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now one with only x2\n",
    "lr = sm.OLS(df_train['y'], df_train[['const','x2']])\n",
    "\n",
    "# Now fit the model\n",
    "fit = lr.fit()\n",
    "\n",
    "# fit.summary makes a snazy table for us to look at\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model estimates for $x_2$ change greatly depending on whether or not $x_1$ is included in the model. In addition, whether or not $x_2$ is considered a significant predictor also drastically changes.\n",
    "\n",
    "Multicollinearity can be bad news in the model building process if you pretend like it isn't there. One way to address it is to leave out one of the variables. Here it would probably be best to use only $x_1$ as the predictor, however, in the real world you don't know which predictor is the correct one.\n",
    "\n",
    "We'll learn some other tools later in the course like PCA and regression trees that address the multicollinearity problem. For now be aware that this problem exists and you should watch for it in your own data.\n",
    "\n",
    "## Questions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Questions\n",
    "\n",
    "## Practice\n",
    "\n",
    "Explore the following `bmi` data set, this data comes from <a href=\"https://plos.figshare.com/articles/Secular_trends_in_body_height_body_weight_BMI_and_fat_percentage_in_Polish_university_students_in_a_period_of_50_years/9208028\">here</a>. \n",
    "\n",
    "The goal here is to predict the variable `fat_perc`. Don't build the model, only explore the data set for multicollinearity. Do you think it is present? \n",
    "\n",
    "After you're done exploring do a web search for the BMI formula.\n",
    "\n",
    "\n",
    "When your TA thinks everyone is ready they'll instruct you to move on to the next phase of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi = pd.read_csv(\"bmi.csv\")\n",
    "bmi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting the Data\n",
    "\n",
    "In this next section we'll examine one way in which we can overfit the data with regression. You'll work through this portion of the notebook on your own and then we'll come back together to dicuss the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Degree Polynomials\n",
    "\n",
    "A common way to display overfitting the data is to add too high a degree of polynomials to the data. You'll construct an example to show the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 2 1-D arrays, x_train and x_test, both spaced from -1.5 to 1.5 with 100 \n",
    "# evenly spaced entries\n",
    "# np.linspace is helpful here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make y a 5th degree polynomial of x_train and store it in y_train\n",
    "# make all of your coefficients no larger than 1\n",
    "# add normally distributed random noise to y\n",
    "# multiply the noise by a constant no larger than 1\n",
    "# c*np.random.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make y_test in the same way, this time using \n",
    "# x_test instead of x_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of x_train by y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a loop from i = 1 to 31, skipping the even numbers,\n",
    "\n",
    "# for each i make a pipeline that transforms x\n",
    "# into a polynomial array of degree i\n",
    "# then fit a regression of y on the powers of x\n",
    "# record both the training set mse and the test set mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the degree vs the corresponding training and test mses\n",
    "# Label them so you know which is which.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What did you notice?\n",
    "\n",
    "Take a moment to write down what you observed through this exercise here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## End Breakout Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade-Off\n",
    "\n",
    "Let's return to our statistical learning framework from Notebook 1. Remember we try to fit the following model:\n",
    "$$\n",
    "y = f(X) + \\epsilon,\n",
    "$$\n",
    "where $f$ is some function.\n",
    "\n",
    "When we fit the model we produce some estimate of $f$ called $\\hat{f}$. \n",
    "\n",
    "As we've discussed we're interested in the generalization error of our algorithm, which so far has been the test MSE. Like with CV when we consider the generalization error we look at the expected value of the squared difference between $y$ and $\\hat{y}$. If we let $y_0$ and $X_0$ denote a single test set, we can write this mathematically as:\n",
    "$$\n",
    "E\\left[ \\left( y_0 - \\hat{y_0} \\right)^2 \\right]= E\\left[ \\left( y_0 - \\hat{f}(X_0) \\right)^2 \\right] = E \\left[ \\left( f(X_0) - \\hat{f}(X_0) + \\epsilon   \\right)^2 \\right],\n",
    "$$\n",
    "where all expectations are taken over the training space.\n",
    "\n",
    "With a little manipulation you can rewrite this as:\n",
    "$$\n",
    "\\text{Var}\\left(\\hat{f}(X_0)\\right) + \\left[ \\text{Bias}\\left( \\hat{f}(X_0) \\right) \\right]^2 + \\text{Var}(\\epsilon)\n",
    "$$\n",
    "$$\n",
    "= \\text{Variance of }\\hat{f} + \\text{Bias squared of }\\hat{f} + \\text{irreducible error}.\n",
    "$$\n",
    "\n",
    "If $\\text{Bias}$ is unfamiliar to you then, $\\text{Bias}\\left(\\hat{f}(X)\\right) = E\\left( f(X) - \\hat{f}(X) \\right)$. One way to think about it is how far on average is the estimator $\\hat{f}$ from the thing it is estimating, $f$.\n",
    "\n",
    "Since $\\text{Var}$ and $\\text{Bias}^2$ are both nonnegative, the best we can do is produce an algorithm with irreducible error $\\text{Var}(\\epsilon)$. This means we can reduce our generalization error by reducing our $\\text{Bias}$ or our $\\text{Var}$. However, it is often not possible to reduce both simultaneously. Many times lowering an algorithm's $\\text{Bias}$ leads to an increase in its $\\text{Var}$. Typically high bias indicates underfitting the data, while high variance means overfitting.\n",
    "\n",
    "We'll now show this in action with linear regression.\n",
    "\n",
    "### Bias-Variance Trade-Off with Linear Regression\n",
    "\n",
    "Let's build some toy data to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "y = x*(x-1) + 1.2*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.plot(x, x*(x-1),'k', label = \"True Relationship, $f$\", alpha = .7)\n",
    "plt.scatter(x, y, label = \"Training Data\")\n",
    "\n",
    "plt.ylabel(\"y\", fontsize=16)\n",
    "plt.xlabel(\"x\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model with high bias but low variance would be to use no predictors and just take the mean observed $y$ value. This would be way underfitting the data, which has a clear pattern. It is high bias because we are far from the true relationship between $y$ and $x$. It is low variance because with a large enough sample the law of large numbers tells us the sample mean should be close to $E(y)$. So as long as our training sample is large enough then we won't vary much over different samples.\n",
    "\n",
    "A linear regression model with low bias but high variance would be to use a high degree polynomial of $x$. This is low bias because a high degree polynomial will more closely fit the true relationship. It will have high variance because as the degree of the fitting polynomial increases the more likely it is that the regression polynomial will attempt to fit all of the training points perfectly, meaning that you will get wildly different fits with each training set.\n",
    "\n",
    "A goldilocks, not too cold not too hot, model would be one with a low degree polynomial. For this particular problem it would likely be a parabola model, but could be a slightly higher degree in practice.\n",
    "\n",
    "In the following code block we fit three models five times. The first model just takes the mean of the training $y$ values. The second model is the \"Goldilocks\" model where we fit a parabola to the data. The final model is the high variance model where we fit a degree $20$ polynomial to the data. We then plot the predicted values for all 15 total model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll plot 5 instances of all 3 models\n",
    "fig,ax = plt.subplots(1,3,figsize = (16,8), sharex = True, sharey = True)\n",
    "bias = []\n",
    "\n",
    "ax[0].scatter(x,y,s=10,alpha=.7)\n",
    "ax[1].scatter(x,y,s=10,alpha=.7)\n",
    "ax[2].scatter(x,y,s=10,alpha=.7)\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    # First generate a random training set\n",
    "    x = np.linspace(-3,3,100)\n",
    "    y = x*(x-1) + 1.2*np.random.randn(100)\n",
    "\n",
    "    # Now fit the high variance model\n",
    "    high_deg_pipe = Pipeline([('poly',PolynomialFeatures(20)),\n",
    "                             ('reg', LinearRegression(copy_X = True))])\n",
    "\n",
    "    high_deg_pipe.fit(x.reshape(-1,1),y)\n",
    "\n",
    "    high_deg_line = high_deg_pipe.predict(x.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "    # Now fit the \"goldilocks model\", the true correct degree\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(2)),\n",
    "                    ('reg', LinearRegression(copy_X = True))])\n",
    "    pipe.fit(x.reshape(-1,1),y)\n",
    "    pipe_line = pipe.predict(x.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plot High Bias low Variance plot\n",
    "    ax[0].plot(x, np.mean(y)*np.ones(len(x)), alpha = 1)\n",
    "    ax[0].set_title(\"High Bias, Low Variance\",fontsize=18)\n",
    "    ax[0].set_ylabel(\"y\", fontsize=16)\n",
    "    ax[0].set_xlabel(\"x\", fontsize=16)\n",
    "\n",
    "\n",
    "    # Plot Goldilocks model\n",
    "    ax[1].plot(x, pipe_line, alpha = 1)\n",
    "    ax[1].set_title(\"Just Right Model\",fontsize=18)\n",
    "    ax[1].set_xlabel(\"x\", fontsize=16)\n",
    "\n",
    "    # Plot High Va model\n",
    "    ax[2].plot(x, high_deg_line, alpha = 1)\n",
    "    ax[2].set_title(\"Low Bias, High Variance\",fontsize=18)\n",
    "    ax[2].set_xlabel(\"x\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Question Time\n",
    "\n",
    "Often the best model is the one that finds a balance between the Bias and the Variance. If you Google bias-variance trade-off, you'll see a picture like this along with links to tons of poorly written blog posts.\n",
    "<img src=\"biasvariance.png\"></img>\n",
    "\n",
    "This picture shows that the place where the minimal generalization error occurs is near to where the Bias equals the Variance. Let's picture the average testing error as a function of different degrees for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single test set\n",
    "x_test = np.linspace(-3,3,100)\n",
    "y_test = x_test*(x_test-1) + 1.2*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors = np.zeros((10,len(range(1,41))))\n",
    "\n",
    "# We'll generate 10 training sets\n",
    "for i in range(10):\n",
    "    x = np.linspace(-3,3,100)\n",
    "    y = x*(x-1) + 1.2*np.random.randn(100)\n",
    "\n",
    "    for j in range(1,41):\n",
    "        pipe = Pipeline([('poly',PolynomialFeatures(j)),\n",
    "                             ('reg', LinearRegression(copy_X = True))])\n",
    "        pipe.fit(x.reshape(-1,1),y)\n",
    "        \n",
    "        pred = pipe.predict(x_test.reshape(-1,1))\n",
    "        test_errors[i,j-1] = np.sum((pred - y_test)**2)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.plot(range(1,41), np.mean(test_errors,axis=0))\n",
    "\n",
    "plt.xlabel(\"Polynomial Degree\", fontsize = 16)\n",
    "plt.ylabel(\"Mean Test Set Error\", fontsize = 16)\n",
    "\n",
    "plt.scatter(np.argmin(np.mean(test_errors,axis=0)) + 1, \n",
    "         np.min(np.mean(test_errors,axis=0)), s=40, c=\"black\")\n",
    "\n",
    "plt.text(np.argmin(np.mean(test_errors,axis=0)) - 1, \n",
    "         np.min(np.mean(test_errors,axis=0)-.2), \n",
    "         \"Lowest Error\", fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected the lowest mean error occurred with the degree $2$ model. This is the model where Bias and Variance are equal or close to equal.\n",
    "\n",
    "\n",
    "This isn't the last time we'll see the bias-variance trade-off in this course. A general rule of thumb to remember is that the \"simpler\" the model the more likely it is to have high bias$^2$ (underfit), the more \"complex\" a model the more likely it is to have high variance (overfit). Where what \"simple\" and \"complex\" means depends on the context of the problem.\n",
    "\n",
    "That's it for this notebook. We'll learn about two techniques to have more complex models while avoiding overfitting in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
