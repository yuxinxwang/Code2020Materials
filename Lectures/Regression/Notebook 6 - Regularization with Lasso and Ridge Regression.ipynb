{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Regularization Techniques - Ridge and Lasso Regression\n",
    "\n",
    "In this notebook we'll build upon what we learned in Notebook 5. How can we address things like overfitting and collinearity? One approach is through regularization.\n",
    "\n",
    "## What We'll Accomplish in This Notebook\n",
    "\n",
    "In particular we'll:\n",
    "\n",
    "<ul>\n",
    "    <li>See what happens to our coefficients when we overfit with a polynomial regression example,</li>\n",
    "    <li>Introduce the main framework behind regularization,</li>\n",
    "    <li>Discuss how Ridge and Lasso regression work and apply it to our polynomial regression problem,</li>\n",
    "    <li>See hyperparameters for the first time,</li>\n",
    "    <li>Learn about scaling data,</li>\n",
    "    <li>Learn how to use Lasso for feature selection.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient Explosions\n",
    "\n",
    "Let's return to our example from Notebook 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "y = x*(x-1) + 1.2*np.random.randn(100)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(x,y, label=\"Observed Data\")\n",
    "\n",
    "plt.plot(x,x*(x-1),'k', label=\"True Relationship\")\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=16)\n",
    "plt.ylabel(\"y\",fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 26\n",
    "coef_holder = np.zeros((n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll fit the data with polynomials degree 1 through n\n",
    "for i in range(1,n+1):\n",
    "    # Make a pipe\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(i,include_bias = False)),\n",
    "                    ('reg',LinearRegression())])\n",
    "    \n",
    "    # fit the data\n",
    "    pipe.fit(x.reshape(-1,1),y)\n",
    "    \n",
    "    # store the coefficient estimates\n",
    "    coef_holder[i-1,:i] = np.round(pipe['reg'].coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coefficient estimates as a dataframe\n",
    "pd.DataFrame(coef_holder, \n",
    "             columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [str(i) + \"_deg_poly\" for i in range(1,n+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to remind ourselves of the overfitting\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.scatter(x, y, alpha = .8, label=\"Observed Data\")\n",
    "\n",
    "plt.plot(x,\n",
    "         pipe.predict(x.reshape(-1,1)),\n",
    "         'k', label= str(n) + \"th deg fit\")\n",
    "\n",
    "plt.plot(x,x*(x-1),'k--',label = \"True Relationship\")\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=16)\n",
    "plt.ylabel(\"y\",fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataframe we've just produced we can notice that a number of our coefficients get larger in magnitude as the model gets more complex. \n",
    "\n",
    "This observation leads to the main idea behind regularization.\n",
    "\n",
    "## The Idea Behind Regularization\n",
    "\n",
    "Suppose the non-intercept coefficients from the regression are denoted by $\\beta$, i.e. $\\beta=\\left(\\beta_1,\\beta_2,\\dots,\\beta_m\\right)^T$. Recall that in Ordinary Least Squares regression our goal is to estimate $\\beta$ so that\n",
    "$$\n",
    "MSE = \\frac{1}{n}(y - X\\beta - \\beta_0)^T(y - X\\beta - \\beta_0)\n",
    "$$\n",
    "is minimized on the training data.\n",
    "\n",
    "The main idea behind regularization is to still minimize the MSE, BUT while also ensuring that $\\beta$ doesn't get too large. \n",
    "\n",
    "#### How to Measure Large?\n",
    "\n",
    "It's reasonable to wonder how we can measure largeness of $\\beta$. Let $||\\bullet||$ denote some norm in $\\mathbb{R}^{m+1}$. If you're unfamiliar with what a norm is don't worry we'll make this more concrete when we talk about lasso and ridge regression, for now think of it as a measure of how \"long\" the $\\beta$ vector is. We measure how large $\\beta$ is by looking at $||\\beta||$.\n",
    "\n",
    "#### Constrained Optimization\n",
    "\n",
    "In regularization we still minimize the MSE, but we constrain ourselves so that we only consider $\\beta$ with $||\\beta||\\leq c$ for some constant $c$. \n",
    "\n",
    "#### An Equivalent Problem\n",
    "\n",
    "It turns out this is equivalent to minimizing the following: \n",
    "$$\n",
    "||y-X\\beta - \\beta_0||^2_2 + \\alpha||\\beta||,\n",
    "$$\n",
    "for some constant $\\alpha$ and where $||a||_2^2 = a_1^2 + a_2^2 + \\dots + a_n^2, a\\in\\mathbb{R}^n$. Note that minimizing $||y-X\\beta - \\beta_0||^2_2$ is equivalent to minimizing the MSE. \n",
    "\n",
    "To see a mathematical derivation of this equivalence look at reference 3 below.\n",
    "\n",
    "Here we can think of $\\alpha||\\beta||$ as a penalty term, which will not allow $\\beta$ to grow too large as we minimize $||y-X\\beta-\\beta_0||^2_2$. The ammount we \"penalize\" for a large $\\beta$ depends on the value of $\\alpha$. \n",
    "\n",
    "#### Our First Hyperparameter\n",
    "\n",
    "$\\alpha$ is the first instance in our course of a <i>hyperparameter</i>, but it will not be the last. A hyperparameter is a parameter we set before fitting the model. While normal parameters, like $\\beta$, are estimated during the training step.\n",
    "\n",
    "For $\\alpha=0$ we recover the OLS estimate for $\\beta$, for $\\alpha=\\infty$ we get $\\beta=0$, values of $\\alpha$ between those two extremes will give different coefficient estimates. The value of $\\alpha$ that gives the best model for your data depends on the problem and can be found through cross-validation model comparisons.\n",
    "\n",
    "\n",
    "## Ridge and Lasso\n",
    "\n",
    "<i>Ridge regression</i> and <i>lasso</i> are two forms of regularization where we make specific choices for the norm:\n",
    "<ul>\n",
    "    <li>In ridge regression we take $||\\bullet||$ to be the square of the Euclidean norm, $||\\bullet||_2^2$,</li>\n",
    "    <li>In lasso we take $||\\bullet||$ to be the $l_1$-norm, $||a||_1 = |a_1| + |a_2| + \\dots + |a_n|, \\ a\\in \\mathbb{R}^n$.\n",
    "</ul>\n",
    "\n",
    "### Containing the Explosion of $\\beta$\n",
    "\n",
    "These two algorithms make it difficult for $\\beta$ to explode. Let's use ridge regression on our polynomial example above and track the norm of $\\beta$ as we increase $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ridge regression object is called Ridge in \n",
    "# sklearn.linear_model\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array of lambda values\n",
    "alphas = np.arange(0,.001,.000001)\n",
    "norms = []\n",
    "\n",
    "# for each alpha value\n",
    "for a in alphas:\n",
    "    # We'll talk about normalizing in a second\n",
    "    pipe = Pipeline([('poly',PolynomialFeatures(30, include_bias=False)),\n",
    "                    ('ridge',Ridge(alpha=a, normalize=True))])\n",
    "    \n",
    "    pipe.fit(x.reshape(-1,1),y)\n",
    "    \n",
    "    \n",
    "    # get the beta vector\n",
    "    coefs = pipe['ridge'].coef_\n",
    "    \n",
    "    # append the square of 2-norm of beta for this alpha\n",
    "    norms.append(np.power(np.sqrt(np.sum(np.power(coefs,2))),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(alphas,norms)\n",
    "\n",
    "plt.xlabel(\"alpha\",fontsize=16)\n",
    "plt.ylabel(\"2-Norm of beta\",fontsize=16)\n",
    "\n",
    "plt.xlim((-.00001,max(alphas)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "### A Quick Aside on \"Normalizing\"\n",
    "\n",
    "These regularization techniques are very sensitive to the scale of our predictors. This is a result of the constrained optimization set up of the problem. To illustrate this let's look at an example.\n",
    "\n",
    "Suppose someone's tiredness after a walk is given by:\n",
    "$$\n",
    "\\text{tiredness} = \\text{age} + 3\\text{(distance traveled in m)}\n",
    "$$\n",
    "\n",
    "We'll use this breakout session to explore what happens in ridge regression and lasso when we go between measuring distnace on two different scales, meters and kilometers.\n",
    "\n",
    "Take the given data and assume age is in years and distance is in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = np.random.randint(20,40,100)\n",
    "distance = 3*np.random.randn(100)+10\n",
    "\n",
    "tiredness = age +  3*distance + 2*np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'age':age,'distance':distance,'tiredness':tiredness})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a linear regression model of `tiredness` on `age` and `distance`. Print the coefficients.\n",
    "\n",
    "Then create a new column in the dataframe changing `distance` to being measured in kilometers, call it `distance_km`. Fit a second model regressing `tiredness` on `age` and `distance_km`. Again print the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a ridge regression model again regressing `tiredness` on `age` and `distance`. Let $\\alpha=10$, but do NOT include `normalize=True`. Print the coefficients. How do these compare to the normal linear regression model?\n",
    "\n",
    "\n",
    "Then build a ridge regression model using the `distance_km` instead of `distance`. Again do NOT include `normalize=True`. Let $\\alpha=10$, and print the coefficients what happened this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\alpha$ increases in ridge and lasso our \"budget\" for the constrained optimization gets smaller, meaning that we can't appropriately estimate the coefficient on distance when it is measured in kilometers. This is due to the large difference in scales between meters and kilometers.\n",
    "\n",
    "#### How to Solve the Scaling Issue?\n",
    "\n",
    "One way to address the scale issue is to just adjust all of our features to have the same scale prior to fitting the model. We can do this by setting `normalize=True` in the `Ridge` and `Lasso` model objects. This centers and scales the features by subtracting off their mean and dividing by their $l_2$ norms.\n",
    "\n",
    "Refit the two ridge models you just created, this time set `normalize = True`. What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing by Hand\n",
    "\n",
    "Another way to scale down the data is to use `sklearn`'s `StandardScaler`. This is a method that scales the data to have mean $0$ and variance $1$ using the standard normal transformation:\n",
    "$$\n",
    "\\frac{X - \\overline{X}}{s_X}.\n",
    "$$\n",
    "While slightly different than `normalize=True` it still puts all of the data on the same scale and can be used instead of `normalize=True`.\n",
    "\n",
    "Build a pipe that first takes the data and scales it using `StandardScaler` then fits the ridge regression with $\\alpha=10$. This time remember to set `normalize=False` or just leave the `normalize` option out of the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# you make a scaler object like StandarScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note before moving on.\n",
    "\n",
    "`StandardScaler` has the following methods:\n",
    "- `fit` which uses the input data to fit the method, i.e. find the mean and standard deviation of the input data\n",
    "- `transform` which uses the statistics calculated in `fit` to scale the input data\n",
    "- `fit_transform` which fits and transforms all in one.\n",
    "\n",
    "##### Why is this Important?\n",
    "\n",
    "This is important because the order in which we do things matters. We first `fit` the scaler using the training data, this sets the scaler for all future data we put into the `transform` method, training and testing. The main take away here is that the scaler fit from the training data is the fit we use to predict on the testing data. This is a subtle distinction, but an important one.\n",
    "\n",
    "Okay now time to talk about lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking More Closely at Lasso\n",
    "\n",
    "Up to this point we've mainly focused on ridge regression. Let's look more closely at lasso now.\n",
    "\n",
    "As we've said the formulation of lasso is identical to that of ridge regression up to the choice of norm. Looking at the two norms $||\\bullet||_2^2$ for ridge and $||\\bullet||_1$ for lasso the main difference is that the square of the $l_2$ norm is differentiable everywhere, where as the $l_1$ norm is not.\n",
    "\n",
    "### Lasso for Feature Selection\n",
    "\n",
    "This fact gives lasso one of its best uses, feature selection. Let's return to our polynomial again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3,100)\n",
    "y = x*(x-1) + 1.2*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit this with a high degree polynomial, after normalizing of course, with both ridge and lasso models but for different values of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "n=10\n",
    "\n",
    "# These will hold our coefficient estimates\n",
    "ridge_coefs = np.empty((len(alpha),n))\n",
    "lasso_coefs = np.empty((len(alpha),n))\n",
    "\n",
    "# for each alpha value\n",
    "for i in range(len(alpha)):\n",
    "    # set up the ridge pipeline\n",
    "    ridge_pipe = Pipeline([('poly',PolynomialFeatures(n,include_bias=False)),\n",
    "                          ('ridge',Ridge(alpha = alpha[i], normalize=True))])\n",
    "    \n",
    "    # set up the lasso pipeline\n",
    "    lasso_pipe = Pipeline([('poly',PolynomialFeatures(n,include_bias=False)),\n",
    "                          ('lasso',Lasso(alpha = alpha[i], normalize=True, max_iter = 1000000))])\n",
    "    \n",
    "    # fit the ridge\n",
    "    ridge_pipe.fit(x.reshape(-1,1),y)\n",
    "    \n",
    "    # fit the lasso\n",
    "    lasso_pipe.fit(x.reshape(-1,1),y)\n",
    "    \n",
    "    # record the coefficients\n",
    "    ridge_coefs[i,:] = ridge_pipe['ridge'].coef_\n",
    "    lasso_coefs[i,:] = lasso_pipe['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(ridge_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso Coefficients\")\n",
    "\n",
    "pd.DataFrame(np.round(lasso_coefs,8),\n",
    "            columns = [\"x^\" + str(i) for i in range(1,n+1)],\n",
    "            index = [\"alpha=\" + str(a) for a in alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at our two tables we see that the ridge coefficients slowly go down to $0$, but most of the time don't actually get there. On the other hand for lasso with $\\alpha=0.1$ almost all of our coefficients are $0$, except for the ones that matter the coefficients for $x$ and $x^2$.\n",
    "\n",
    "This feature of lasso makes it quite popular, particularly when you have a lot of features which makes other popular model selection algorithms unfeasible (see Regression HW).\n",
    "\n",
    "You just fit a lasso model with all of your features and then choose the ones that are nonzero for your final model. Again this should be done with some sort of test to see which model you think will give the better test error, like cv.\n",
    "\n",
    "### Why Does it Do That?\n",
    "\n",
    "To see a geometric explanation for why this happens let's return to our constrained optimization formulation, and assume both $X$ and $y$ have mean $0$ for simplicity.\n",
    "\n",
    "##### Ridge Regression\n",
    "$$\n",
    "\\text{Minimize } || y - X\\beta||_2^2 \\text{ constrained to } ||\\beta||_2^2 \\leq c.\n",
    "$$\n",
    "\n",
    "If we have two features the constraint is $\\beta_1^2 + \\beta_2^2 \\leq (\\sqrt{c})^2$, which you may recall is the formula for a filled in circle centered at the origin with radius $\\sqrt{c}$ in $\\mathbb{R}^2$.\n",
    "\n",
    "##### Lasso\n",
    "$$\n",
    "\\text{Minimize } || y - X\\beta||_2^2 \\text{ constrained to } ||\\beta||_1 \\leq c.\n",
    "$$\n",
    "\n",
    "If we have two features the constraint is $|\\beta_1| + |\\beta_2| \\leq c$, which gives a filled in square with edges at $(c,0),(0,c),(-c,0),$ and $(0,-c)$.\n",
    "\n",
    "Let's look at a picture in the case of two features.\n",
    "<img src=\"lasso_ridge_eosl.png\" style=\"width:60%\"></img>\n",
    "Photo Credit to <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">Elements of Statistical Learning</a>.\n",
    "\n",
    "In this photo $\\hat{\\beta}$ is the OLS estimate for $\\beta$ at the minimum value of $|| y - X\\beta||_2^2$, the red ellipses are selected level curves for $|| y - X\\beta||_2^2$, and the blue square and circle are the contraint regions for the lasso and ridge respectively. We can think of lasso and ridge as finding the smallest level curve that still intersects with the constraint region. If the OLS estimate is not contained within the constraint region this will occur somewhere on the boundary. This image demonstrates that the level curve corresponding to the minimal value of $|| y - X\\beta||_2^2$ often intercepts the lasso constraint on an axis of the $\\beta$-space, which is not the case for ridge regression.\n",
    "\n",
    "As a reminder for practical purposes decreasing the value of $\\alpha$ for the `sklearn` `Lasso` and `Ridge` objects increases the size of the constraint region. Increasing the value of $\\alpha$ will shrink the constraint region.\n",
    "\n",
    "## Questions\n",
    "\n",
    "That may have been a bit confusing, let's take one or two questions then you'll use Lasso for some feature selection.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Questions\n",
    "\n",
    "## Practice\n",
    "\n",
    "\n",
    "Run the following code to generate data `X` and `y`.\n",
    "\n",
    "Use lasso to try and determine which features should be included in the model. When you're ready run the code block at the end to see how the data was constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = g.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.give_how_generated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which One is Better?\n",
    "\n",
    "Which algorithm is the better choice? Well that depends on the problem. Both are good at addressing overfitting concerns, but each has a couple unique pros and cons.\n",
    "\n",
    "##### Lasso\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Works well when you have a large number of features that don't have any effect on the target.</li>\n",
    "    <li>Feature selection is a plus, this can allow for a sparser model which is good for computational reasons.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Can have trouble with highly correlated features, it typically chooses one variable among those that are correlated, which may be random.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "##### Ridge\n",
    "\n",
    "<b>Pros</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Works well when the target depends on all or most of the features.</li>\n",
    "    <li>Can handle colinearity better than lasso.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Cons</b>\n",
    "\n",
    "<ul>\n",
    "    <li>Because ridge typically keeps most of the predictors in the model, this can be a computationally costly model type for data sets with a large number of predictors.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "##### Elastic Net\n",
    "\n",
    "Sometimes the best model will be something in between ridge and lasso. Check out the Regression HW to learn about how that is possible with an elastic net model.\n",
    "\n",
    "## Questions\n",
    "\n",
    "We'll answer a couple of questions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## End Questions\n",
    "\n",
    "That's it for this notebook and for regression lessons! For our final regression notebooks you'll be in charge of building a model to predict Boston Housing Values. In Notebook 7 you'll see an example of the entire model building process using California housing data. You can use this as a guide to work through Notebook 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Specific References\n",
    "\n",
    "To help teach this lesson I consulted some additional source I found through a Google search. Here are links to those references for you to take a deeper dive into ridge and lasso regression.\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"http://faculty.marshall.usc.edu/gareth-james/ISL/\">http://faculty.marshall.usc.edu/gareth-james/ISL/</a></li>\n",
    "    <li><a href=\"https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\">https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/</a></li>\n",
    "    <li><a href=\"https://suzyahyah.github.io/math/optimization/2018/07/20/Constrained-unconstrained-form-Ridge.html\">https://suzyahyah.github.io/math/optimization/2018/07/20/Constrained-unconstrained-form-Ridge.html</a></li>\n",
    "    <li><a href=\"https://statweb.stanford.edu/~owen/courses/305a/Rudyregularization.pdf\">https://statweb.stanford.edu/~owen/courses/305a/Rudyregularization.pdf</a></li>\n",
    "    <li><a href=\"http://web.mit.edu/zoya/www/linearRegression.pdf\">http://web.mit.edu/zoya/www/linearRegression.pdf</a></li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
