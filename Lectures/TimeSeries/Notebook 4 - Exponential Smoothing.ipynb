{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Smoothing\n",
    "\n",
    "We dive into more advanced techniques with exponential smoothing. \n",
    "\n",
    "This quote from <a href=\"https://otexts.com/fpp2/expsmooth.html\">Forecasting: Principles and Practice</a>, helps summarize the technique.\n",
    "<br>\n",
    "<br>\n",
    "<q>Exponential smoothing was proposed in the late 1950s (Brown, 1959; Holt, 1957; Winters, 1960), and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry.</q>   \n",
    " \n",
    "## What You'll Accomplish \n",
    "\n",
    "In particular you'll see:\n",
    "<ul>\n",
    "    <li>An introduction to time series analysis with the statsmodel.api package,</li>\n",
    "    <li>A blend of the average and na&iuml;ve with simple exponential smoothing,</li>\n",
    "    <li>Methods for modeling trends, and</li>\n",
    "    <li>Methods for handling seasonality.</li>\n",
    "</ul>\n",
    "\n",
    "Let's start by importing our standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "from datetime import datetime\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Exponential Smoothing\n",
    "\n",
    "Assume that our training data has $T_\\text{train}$ observations, and we're predicting with a horizon $h$.\n",
    "\n",
    "Recall that the na&iuml;ve method has the following forecast algorithm,\n",
    "$$\n",
    "\\hat{y}_{T_\\text{train} + h|T_\\text{train}} = y_{T_\\text{train}},\n",
    "$$\n",
    "and that the average method forecasts like so:\n",
    "$$\n",
    "\\hat{y}_{T_\\text{train} + h|T_\\text{train}} = \\frac{1}{T_\\text{train}}\\sum_{t=1}^{T_\\text{train}} y_t.\n",
    "$$\n",
    "\n",
    "These two approaches can be seen as completely opposite extremes. \n",
    "\n",
    "On one hand, with the na&iuml;ve method you disregard almost all of the data only going with the most recent observation as the information you predict with. If you think about multiplying each observation with a weight this corresponds placing all the weight on the last observation while giving all the other observations a weight of $0$. \n",
    "\n",
    "\n",
    "On the other hand, the average method takes the opposite approach by equally weighting all the observations.\n",
    "\n",
    "These were great first attempts at forecasting, but the true relationship for most time series is somewhere in between. This is where the idea for simple exponential smoothing arrives. With simple exponential smoothing we forecast like this\n",
    "$$\n",
    "\\hat{y}_{T_\\text{train} + h|T_\\text{train}} = \\alpha y_{T_\\text{train}} + \\alpha(1-\\alpha) y_{T_\\text{train} - 1} + \\alpha(1-\\alpha)^2 y_{T_\\text{train} - 2} + \\dots, \\alpha \\in (0,1).\n",
    "$$\n",
    "\n",
    "\n",
    "### Another Way to Write it\n",
    "\n",
    "Another way to write this that will be helpful for us later is called the <i>component form</i>.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\text{the Forecast equation } & \\hat{y}_{t+h|t} = l_t,  \\\\\n",
    "\\text{the Smoothing equation } & l_t = \\alpha y_t + (1-\\alpha)l_{t-1}, \n",
    "\\end{array}\n",
    "$$\n",
    "where $l_0$ is another parameter of the model that is estimated from training data. When we let $h=1$ we get the fitted values on the training data, and when $t = {T_\\text{train}}$ we get the forecast on test data.\n",
    "\n",
    "In the component form we write that the forecast on test data is:\n",
    "$$\n",
    "\\hat{y}_{T_\\text{train} + h|T_\\text{train}} = \\hat{y}_{T_\\text{train} + 1|T_\\text{train}} = l_{T_\\text{train}}\n",
    "$$\n",
    "\n",
    "This form may seem confusing, but it will help when we add in trend and seasonality modeling.\n",
    "\n",
    "### Simple Exponential Smoothing on Oil Production Data\n",
    "\n",
    "We'll implement this algorithm on the annual oil production from Saudi Arabia 1965-2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil = pd.read_csv(\"oil.csv\", parse_dates=['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(oil.year,oil.production)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_train = oil.loc[oil.year < datetime(2010,1,1),].copy()\n",
    "oil_test = oil.drop(oil_train.index).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return of  `statsmodels`\n",
    "\n",
    "Now that our models are slightly more complicated we no longer have to write them by hand, `statsmodels` has time series algorithms ready for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SimpleExpSmoothing object\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a SimpleExpSmoothing object like this\n",
    "# the data you want to train on is the input\n",
    "ses = SimpleExpSmoothing(oil_train.production.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you make a fitted object\n",
    "# you can put in a value for alpha\n",
    "# like so\n",
    "fit = ses.fit(smoothing_level=0.5, optimized=False)\n",
    "\n",
    "# or you can go the this route and\n",
    "# let the algorithm find the optimal alpha\n",
    "# for the training data\n",
    "# fit = ses.fit(optimized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now make a plot of the fitted values \n",
    "# and the forecast on the test data\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# plot the training data\n",
    "plt.plot(oil_train.year, oil_train.production,'b',\n",
    "            label = \"Training Data\")\n",
    "\n",
    "# plot the fit\n",
    "plt.plot(oil_train.year, fit.fittedvalues,'r-',\n",
    "            label = \"Fitted Values\")\n",
    "\n",
    "# plot the forecast\n",
    "plt.plot(oil_test.year, fit.forecast(len(oil_test)),'r--',\n",
    "            label = \"Forecast\")\n",
    "\n",
    "plt.plot(oil_test.year, oil_test.production,'b--',\n",
    "            label = \"Test Data\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Year\", fontsize=16)\n",
    "plt.ylabel(\"Production \\n in millions of tonnes\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Use exponential smoothing to predict the last $150$ days of google closing prices from this data set of $1000$ consecutive trading days. It's okay if the fit isn't very good. This is about getting used to the process of fitting models with `statsmodels` time series methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog = pd.read_csv(\"goog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(goog.closing_price)\n",
    "\n",
    "plt.xlabel(\"Day\", fontsize=16)\n",
    "plt.ylabel(\"Closing Price\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And here if needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Models\n",
    "\n",
    "After the introduction of Simple Exponential Smoothing a few researchers developed extensions that allow the forecast to have a trend as opposed to being a flat prediction.\n",
    "\n",
    "### Holt's Linear Trend Model\n",
    "\n",
    "Developed by Holt in 1957 this approach breaks the smoothing equation into two equations.\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\text{The forecast equation} & \\hat{y}_{t+h|t} = l_t + h b_t ,\\\\\n",
    "\\text{the level equation} & l_t = \\alpha y_t + (1-\\alpha)(l_{t-1} + b_{t-1}), \\\\\n",
    "\\text{the trend equation} & b_t = \\beta^* (l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}\n",
    "\\end{array}.\n",
    "$$\n",
    "\n",
    "Here both $\\alpha$ and $\\beta^*$ are parameters that are allowed to take any value in $(0,1)$.\n",
    "\n",
    "The level equation is the equivalent of what we referred to as the smoothing equation for simple exponential smoothing.\n",
    "\n",
    "The trend equation, models the trend, in particular it does this by modeling the trend with the smoothing equation from simple exponential smoothing, but this time using the pairwise differences $l_t - l_{t-1}$.\n",
    "\n",
    "If you notice here the $hb_t$ in the forecast equation is where the linear in Holt's linear trend model comes from. It is this that allows for non-level forecasts into the future. \n",
    "\n",
    "However, it is also slightly unrealistic for all time series that the trend is a linear increase or decrease forever which led to the next extension.\n",
    "\n",
    "### Damped Trend Methods\n",
    "\n",
    "This extension on Holt's linear trend method allows for a trend that dampens over time. It was developed by Gardner & McKenzie in 1985.\n",
    "\n",
    "Instead of having the forecast be a linear function of the horizon you use a power series:\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\text{The forecast equation} & \\hat{y}_{t+h|t} = l_t + (\\phi + \\phi^2 + \\dots + \\phi^h) b_t ,\\\\\n",
    "\\text{the level equation} & l_t = \\alpha y_t + (1-\\alpha)(l_{t-1} + \\phi b_{t-1}), \\\\\n",
    "\\text{the trend equation} & b_t = \\beta^* (l_t - l_{t-1}) + (1-\\beta^*) \\phi b_{t-1}\n",
    "\\end{array}.\n",
    "$$\n",
    "\n",
    "Here all of $\\alpha$, $\\beta^*$, and $\\phi$ are parameters in $(0,1)$ that you must choose, the best parameters depend on the data.\n",
    "\n",
    "### Forecasting Livestock in Asia\n",
    "\n",
    "We'll demonstrate how to implement these two methods using data on annual sheep levels in Asia from 1961-2007, then you'll return to the Google stock data in a breakout session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livestock = pd.read_csv(\"livestock.csv\",parse_dates = ['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livestock_train = livestock.loc[livestock.year < datetime(2003,1,1),].copy()\n",
    "livestock_test = livestock.drop(livestock_train.index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(livestock_train.year, livestock_train.million_head)\n",
    "\n",
    "plt.xlabel(\"Date\", fontsize=16)\n",
    "plt.ylabel(\"Million Head of Sheep\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a clear trend in this data. We'll now use the time series functionality from `statsmodels` to implement the Holt linear model and the damped linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Holt object\n",
    "from statsmodels.tsa.api import Holt\n",
    "\n",
    "# Holt can make both linear models and damped linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt's linear model \n",
    "lin_mod = Holt(livestock_train.million_head.values,\n",
    "                  damped = False)\n",
    "\n",
    "# damped linear model\n",
    "damp = Holt(livestock_train.million_head.values,\n",
    "               damped = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the linear model ##\n",
    "# Again you can fit the models with custom parameters\n",
    "# smoothing_level is alpha\n",
    "# smoothing_slope is beta\n",
    "#lin_mod_fit = lin_mod.fit(smoothing_level = .4, \n",
    " #                           smoothing_slope = .4,\n",
    "  #                          optimized=False)\n",
    "\n",
    "# or you can let the algorithm find the optimal\n",
    "# parameters for the training data\n",
    "lin_mod_fit = lin_mod.fit(optimized=True)\n",
    "\n",
    "## Fit the damped model ##\n",
    "# damping_slope is phi from the model\n",
    "#damp_fit = damp.fit(smoothing_level = .4,\n",
    " #                      smoothing_slope = .4,\n",
    "  #                     damping_slope = .4,\n",
    "   #                    optimized = False)\n",
    "\n",
    "# again you can let the algorithm find the optimal\n",
    "# parameters for the training data\n",
    "damp_fit = damp.fit(optimized = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now make a plot of the fitted values \n",
    "# and the forecast on the test data\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "## Plot the training data and the fits\n",
    "# plot the training data\n",
    "plt.plot(livestock_train.year, livestock_train.million_head,'b-',\n",
    "            label = \"Training Data\")\n",
    "\n",
    "# plot the linear fit\n",
    "plt.plot(livestock_train.year, lin_mod_fit.fittedvalues,'r-',\n",
    "            label = \"Holt Linear Fitted Values\")\n",
    "\n",
    "# plot the damped fit\n",
    "plt.plot(livestock_train.year, damp_fit.fittedvalues,'k-',\n",
    "            label = \"Damped Linear Fitted Values\")\n",
    "\n",
    "\n",
    "## Plot the test data and forecasts\n",
    "# test data\n",
    "plt.plot(livestock_test.year, livestock_test.million_head,'b--',\n",
    "            label = \"Test Data\")\n",
    "\n",
    "# plot the linear forecast\n",
    "plt.plot(livestock_test.year, lin_mod_fit.forecast(len(livestock_test)),\n",
    "         'r--', label = \"Holt Linear Forecast\")\n",
    "\n",
    "# plot the damp forecast\n",
    "plt.plot(livestock_test.year, damp_fit.forecast(len(livestock_test)),\n",
    "         'k--', label = \"Damped Linear Forecast\")\n",
    "\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Year\", fontsize=16)\n",
    "plt.ylabel(\"Million Head of Sheep\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you go the optimized=True\n",
    "# route like we did you can find the\n",
    "# parameters like so.\n",
    "damp_fit.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Implement Holt's linear model and the damped linear model on the Google stock data. Which looks like it works better?\n",
    "\n",
    "Play around with changing the parameters by hand and using the optimized model. You may want to implement cross-validation. You can if you'd like, but this practice is more about getting familiar with implementing the `statsmodel` framework than making the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holt-Winters' Seasonality Extension to Holt's Linear Model\n",
    "\n",
    "Both Holt, in 1957, and Winters, in 1960, built extensions onto Holt's Linear Model to account for seasonality in the data. This time the component has an additional component added on to account for seasonality. In their approach they again denote the level and trends as $l_t$ and $b_t$ respectively. $m$ is the number of time steps it takes to complete a single season. If we remember our measles example the season occurred once every two years or $24$ months, in this case $m=24$.\n",
    "\n",
    "There are two versions of the seasonality extension, an additive one and a multiplicative one.\n",
    "\n",
    "### Additive Model\n",
    "\n",
    "This approach is preferred when the magnitude of the seasonality is more or less constant over time.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\text{The forecast equation} & \\hat{y}_{t+h|t} = l_t + h b_t + s_{t+h - m(k+1)},\\\\\n",
    "\\text{the level equation} & l_t = \\alpha (y_t - s_{t-m}) + (1-\\alpha)(l_{t-1} + b_{t-1}), \\\\\n",
    "\\text{the trend equation} & b_t = \\beta^* (l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}, \\\\\n",
    "\\text{the seasonality equation} & s_t = \\gamma(y_t - l_{t-1}-b_{t-1}) + (1-\\gamma)s_{t-m}.\n",
    "\\end{array}.\n",
    "$$\n",
    "Where here $\\gamma \\in (0,1-\\alpha)$, and $k = (h-1) // m$, the integer part of $(h-1)/m$, which ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample.\n",
    "\n",
    "### Multiplicative Model\n",
    "\n",
    "This approach works well with seasonality that either grows or shrinks with time.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l l}\n",
    "\\text{The forecast equation} & \\hat{y}_{t+h|t} = (l_t + h b_t)s_{t+h - m(k+1)},\\\\\n",
    "\\text{the level equation} & l_t = \\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1} + b_{t-1}), \\\\\n",
    "\\text{the trend equation} & b_t = \\beta^* (l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}, \\\\\n",
    "\\text{the seasonality equation} & s_t = \\gamma\\frac{y_t}{(l_{t-1}+b_{t-1})} + (1-\\gamma)s_{t-m}.\n",
    "\\end{array}.\n",
    "$$\n",
    "Here $k$ and $\\gamma$ are as in the Additive Model.\n",
    "\n",
    "#### A Note on Damping\n",
    "\n",
    "Note that there are damped versions of these models as well, we just don't present their equations. You can find them here, <a href=\"https://otexts.com/fpp2/holt-winters.html\">https://otexts.com/fpp2/holt-winters.html</a>.\n",
    "\n",
    "### Fitting Seasonal Models on Australian Beer Data\n",
    "\n",
    "We'll return to our australian beer production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ausbeer = pd.read_csv(\"ausbeer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ausbeer_train = ausbeer.iloc[:-12,].copy()\n",
    "ausbeer_test = ausbeer.drop(ausbeer_train.index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(ausbeer_train.production)\n",
    "\n",
    "plt.xlabel(\"Quarter\", fontsize=16)\n",
    "plt.ylabel(\"Production\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we import ExponentialSmoothing\n",
    "from statsmodels.tsa.api import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two model objects\n",
    "# in both models because our season occurs every\n",
    "# 4 time steps we set season_periods = 4\n",
    "\n",
    "\n",
    "# In the additive model set seasonal to 'add'\n",
    "add_mod = ExponentialSmoothing(ausbeer_train,seasonal_periods=4, seasonal='add')\n",
    "\n",
    "# In the multiplicative model set seasonal to 'mul'\n",
    "mul_mod = ExponentialSmoothing(ausbeer_train,seasonal_periods=4, seasonal='mul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now fit them\n",
    "## By not putting in parameters we\n",
    "## go with the method defaults which is to\n",
    "## use optimized=True\n",
    "\n",
    "# The Additive Model\n",
    "add_mod_fit = add_mod.fit()\n",
    "\n",
    "# The multiplicative model\n",
    "mul_mod_fit = mul_mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Them\n",
    "fig,ax = plt.subplots(2,1,figsize=(12,16),sharex=True, sharey=True)\n",
    "\n",
    "## The additive model\n",
    "# training data\n",
    "ax[0].plot(ausbeer_train.index, ausbeer_train.production, \n",
    "           'b-', label=\"Training Data\")\n",
    "\n",
    "# fitted values\n",
    "ax[0].plot(ausbeer_train.index, add_mod_fit.fittedvalues, \n",
    "           'r-', label=\"Fitted Values\")\n",
    "\n",
    "# The test data\n",
    "ax[0].plot(ausbeer_test.index, ausbeer_test.production,\n",
    "            'b--', label=\"Test Data\")\n",
    "ax[0].plot(ausbeer_test.index, add_mod_fit.forecast(len(ausbeer_test)),\n",
    "           'r--', label=\"Forecast\")\n",
    "\n",
    "ax[0].legend(fontsize=14)\n",
    "\n",
    "ax[0].set_title(\"Additive Model\",fontsize=18)\n",
    "ax[0].set_ylabel(\"Production\",fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "## The multiplicative model\n",
    "# training data\n",
    "ax[1].plot(ausbeer_train.index, ausbeer_train.production, \n",
    "           'b-', label=\"Training Data\")\n",
    "\n",
    "# fitted values\n",
    "ax[1].plot(ausbeer_train.index, mul_mod_fit.fittedvalues, \n",
    "           'r-', label=\"Fitted Values\")\n",
    "\n",
    "# The test data\n",
    "ax[1].plot(ausbeer_test.index, ausbeer_test.production,\n",
    "            'b--', label=\"Test Data\")\n",
    "ax[1].plot(ausbeer_test.index, mul_mod_fit.forecast(len(ausbeer_test)),\n",
    "           'r--', label=\"Forecast\")\n",
    "\n",
    "\n",
    "ax[1].set_title(\"Multiplicative Model\",fontsize=18)\n",
    "ax[1].set_ylabel(\"Production\",fontsize=16)\n",
    "ax[1].set_xlabel(\"Quarter\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Return to the measles data (pre 1963). Implement these two techniques on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measles = pd.read_csv(\"measles.csv\", parse_dates=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measles_1963 = measles.loc[measles.month < datetime(1963,1,1),].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook. We don't have the time to go into detail on the theoretical details of exponential smoothing. For those that are interested read sections 7.5 and 7.6 of <a href=\"https://otexts.com/fpp2/ets.html\">Forecasting: Principles and Practice</a>.\n",
    "\n",
    "Also you can learn more about the `statsmodels` Exponential Smoothing capabilities at the documentation page here <a href=\"https://www.statsmodels.org/stable/tsa.html\">https://www.statsmodels.org/stable/tsa.html</a> with a nice tutorial page here <a href=\"https://www.statsmodels.org/stable/examples/notebooks/generated/exponential_smoothing.html#Exponential-smoothing\">https://www.statsmodels.org/stable/examples/notebooks/generated/exponential_smoothing.html#Exponential-smoothing</a>.\n",
    "\n",
    "We'll see you in Notebook 5 where we introduce our final time series material ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "Sections 7.1 to 7.4 of <a href=\"https://otexts.com/fpp2/\">Forecasting: Principles and Practice</a>, by Rob J Hyndman and George Athanasopoulos.\n",
    "\n",
    "The time series section of the `statsmodels` documentation, <a href=\"https://www.statsmodels.org/stable/tsa.html\">https://www.statsmodels.org/stable/tsa.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
