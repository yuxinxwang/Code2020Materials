{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this notebook we discuss another popular classification algorithm, decision trees. \n",
    "\n",
    "A decision tree is quite intuitive and a building block for an often used algorithm, random forests (more on those later). Trees can be made to perform both regression and classification tasks. We'll focus on classification in this notebook and if you're interested you can build a regression tree in the homework.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "<ul>\n",
    "    <li>We'll introduce the decision tree algorithm with a dumb example,</li>\n",
    "    <li>Talk about the decision tree loss functions,</li>\n",
    "    <li>Show how sklearn fits a tree,</li>\n",
    "    <li>You'll make a tree to predict heart disease,</li>\n",
    "    <li>Touch on how sklearn handles categorical features and decision trees.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Idea\n",
    "\n",
    "The basic idea for decision trees is segmenting the data space in a way that allows us to classify the data well.\n",
    "\n",
    "### A Dumb Example\n",
    "\n",
    "Let's look at a really dumb example. Below we generate some random data with an $x_1$ feature, an $x_2$ feature, and a target, $y$. We then plot the data using seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x1':np.random.random(200),'x2':np.random.random(200),'y':np.zeros(200)})\n",
    "\n",
    "df.loc[100:199,['x1']] = df.loc[100:199,['x1']] + 1\n",
    "df.loc[100:199,['y']] = df.loc[100:199,['y']] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(df.loc[df.y == 0,'x1'],df.loc[df.y == 0,'x2'],c='orange',label=\"0\")\n",
    "plt.scatter(df.loc[df.y == 1,'x1'],df.loc[df.y == 1,'x2'],c='blue',label=\"1\")\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "plt.legend(fontsize='14', title_fontsize='16')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you had to come up with an adhoc classification rule right now what would you say we should do?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "One good choice would be to say that if the data point is to the left of $x_1= 1$ then we classify it as $0$, and if it to the right of $x_1=1$ then we classify it as $1$. We can illustrate this with a logic tree\n",
    "<img src = \"SimpleTree.png\" style=\"width:60%\"></img>\n",
    "While this is a very dumb example, it is the basic idea behind decision trees. You look at a feature, make a cut point that minimizes some measure of wrongness, and keep going until some stopping criterion.\n",
    "\n",
    "Before we move on to real data, let's fit a decision tree to our dumb data and see what classification rule it comes up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# tree will be used to plot the decisiton tree\n",
    "from sklearn import tree\n",
    "\n",
    "# This is the actual out of the box algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an decisiion tree object\n",
    "tree_clf = DecisionTreeClassifier(random_state = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "# Plot the fitted tree\n",
    "fig = tree_clf.fit(df[['x1','x2']], df.y)\n",
    "tree.plot_tree(fig,filled = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is the logic tree built by the decision tree algorithm. To classify a new observation we start at the <i>root node</i> up top. If the observation satisfies the logic statement at the top we go left and are classified as a $0$, else we go right and are classified as $1$. The two <i>children</i> of the root node are known as <i>leaf nodes</i> or <i>terminal nodes</i> because they have no children of their own so we just predict the majority class contained in that node.\n",
    "\n",
    "This is essentially the decision rule we came up with (which is the objectively correct one by the way), so in this dummy example the decision tree did well.\n",
    "\n",
    "\n",
    "If we look in the plot above we'll notice a number of different stats in each node. Let's quickly break those down. \n",
    "\n",
    "<ul>\n",
    "    <li>samples - the number of samples in each node</li>\n",
    "    <li>gini - the gini impurity of the node, more on this in a moment</li>\n",
    "    <li>value - the breakdown of the number of samples of each target value in the node, for example the leaf node on the left has $100$ nodes labeled $0$ and $0$ nodes labeled $1$</li>\n",
    "    <li>A decision rule - The rule that is used for the following split, samples that would be evaluated as True for the rule go to the left child, samples that would be evaluated as False go to the right child</li>\n",
    "</ul>\n",
    "\n",
    "### How is Wrongness Measured?\n",
    "\n",
    "There are a couple of ways to measure wrongness, or rather impurity, with decision trees. We'll briefly touch on two popular measures that can implemented with out of the box `sklearn`.\n",
    "\n",
    "#### Gini Impurity\n",
    "\n",
    "Suppose that there are $N$ target classes.\n",
    "\n",
    "The Gini Impurity for class $i$ of a node estimates the probability that a randomly chosen sample of class $i$ from the node is incorrectly classified as not class $i$. The formula is:\n",
    "$$\n",
    "G_i = p_i(1-p_i)\n",
    "$$\n",
    "where $p_i$ is the proportion of samples in the node of class $i$. The total Gini Impurity is the sum of all these $G_i$\n",
    "$$\n",
    "I_G = \\sum_{i=1}^N G_i = 1 - \\sum_{i=1}^N p_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "Entropy is an alternative impurity measure you could use when building a decision tree in `sklearn`. It's origins are in thermodynamics, but it soon moved into a diverse array of fields including information theory which is how it ended up in machine learning applications.\n",
    "\n",
    "Again suppose there are $N$ target classes. The contribution made to entropy from class $i$ is:\n",
    "$$\n",
    "H_i = - p_i \\log(p_i),\n",
    "$$\n",
    "where again $p_i$ is the proportion of samples in the node of class $i$. The total entropy of the node is the sum of all the $H_i$:\n",
    "$$\n",
    "I_H = \\sum_{i=1}^N H_i = -\\sum_{i=1}^N p_i \\log(p_i) \n",
    "$$\n",
    "\n",
    "#### Which to use\n",
    "\n",
    "In most cases both measures are comparable, with Gini impurity being faster to compute (the $\\log$ in entropy takes more time for a computer) it is a good default. It has been found that entropy leads to more balanced trees, https://sebastianraschka.com/faq/docs/decision-tree-binary.html, than Gini Impurity.\n",
    "\n",
    "## How `sklearn` Fits a Tree\n",
    "\n",
    "Now that we understand how to measure the impurity of a node, we can see how `sklearn` decides to make a node cut.\n",
    "\n",
    "### The CART Algorithm\n",
    "\n",
    "`sklearn` uses the <i>Classification and Regression Tree</i> or <i>CART</i> algorithm. \n",
    "\n",
    "Suppose your data set has $n$ observations with $m$ features, and for simplicity only $2$ target classes.\n",
    "\n",
    "The algorithm starts with the root node. It then searches through each feature, $k$, and finds a split point, $t_k$, that produces the purest subsets (weighted by the number of samples in each subset), i.e. it finds a $t_k$ that minimizes:\n",
    "$$\n",
    "J(k,t_k) = \\frac{n_\\text{left}}{n} I_\\text{left} + \\frac{n_\\text{right}}{n} I_\\text{right},\n",
    "$$\n",
    "where left and right refers to being left or right of the split point, $t_k$, and $I$ is the impurity measure you choose to use (the default is Gini). \n",
    "\n",
    "Once it finds the $(k,t_k)$ pair that has smallest $J(k,t_k)$, it splits the data according to that decision split.\n",
    "\n",
    "The algorithm then repeats the entire process on each of the children nodes it just produced. This continues until some stopping condition for example:\n",
    "<ul>\n",
    "    <li>reaching a maximum depth</li>\n",
    "    <li>reaching a minimum number of samples in each node</li>\n",
    "    <li>reaching a minimum weight to be in a node</li>\n",
    "    <li>etc.</li>\n",
    "</ul>\n",
    "or until it can no longer reduce the impurity by making a cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Break\n",
    "\n",
    "Okay before we move on to building a decision tree on real data, let's take a break for any questions about the algorithm, impurity, or anything we just went over.\n",
    "\n",
    "\n",
    "Ask now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's try to build a model with non randomly generated data.\n",
    "\n",
    "## Practice - Classifying Heart Disease with a Decision Tree\n",
    "\n",
    "\n",
    "We'll be building a model to classify instances of heart disease from a data set that can be found <a href = \"https://archive.ics.uci.edu/ml/datasets/heart+Disease\">here</a>.  I walk you through some preliminary data exploration then leave you it to you to build the model.\n",
    "\n",
    "\n",
    "### The Data\n",
    "\n",
    "\n",
    "Before any data exploration let's make a test train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "# it is stored in the cleveland_heart csv\n",
    "heart = pd.read_csv(\"cleveland_heart.csv\")\n",
    "\n",
    "# our target is whether or not there was heart disease\n",
    "heart['target'] = 0\n",
    "heart.loc[heart.status > 0,'target'] = 1\n",
    "\n",
    "\n",
    "print(\"There are \",len(heart),\" observations.\")\n",
    "print(sum(heart.target),\" of those observations had heart disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice, make your train test split here!\n",
    "# Keep the default of 25% set aside\n",
    "# shuffle and stratify to ensure we keep the distr\n",
    "# Set a random state of 614\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this block if needed\n",
    "X_train,X_test,y_train,y_test = train_test_split(heart.drop(columns = ['status','target']),\n",
    "                                                heart[['target']], \n",
    "                                                shuffle = True,\n",
    "                                                stratify = heart[['target']], \n",
    "                                                random_state = 614)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now, using both the documentation and `pandas` explore the data set further before building a model. This will allow us to see if we need to do any cleaning before feeding the data into the algorithm.\n",
    "\n",
    "Here are some questions you might want to know the answer to:\n",
    "<ol>\n",
    "    <li>Which variables are continuous? Categorical? Ordinal?</li>\n",
    "    <li>What do each of the variables mean?</li>\n",
    "    <li>How are the data values stored in the pandas dataframe? Is that appropriate for the data type?</li>\n",
    "    <li>Do some variables appear to more important than others?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()\n",
    "\n",
    "\n",
    "# from uci, the data source \n",
    "# age - ordinal\n",
    "# sex - categorical/binary\n",
    "# cp - categorical/1,2,3,4\n",
    "# trestbps - cont.\n",
    "# chol - cont\n",
    "# fbs - cat/bin\n",
    "# restecg - cat/0,1,2\n",
    "# thalach - cont\n",
    "# exang - cat/bin\n",
    "# oldpeak - cont\n",
    "# slope - ordinal\n",
    "# ca - ordinal\n",
    "# thal - cat/3,6,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Code here \n",
    "# We should recode the non-binary qualitative variables\n",
    "X_train[['cp','restecg','thal']] = X_train[['cp','restecg','thal']].astype(str)\n",
    "X_test[['cp','restecg','thal']] = X_test[['cp','restecg','thal']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Code here \n",
    "# Let's examine the data by the target.\n",
    "df_train = pd.concat([X_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_train,vars = ['age','trestbps','chol','thalach','oldpeak'],\n",
    "                  hue='target')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the continuous features may not be helpful on their own. Let's try to explore the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['sex','cp','fbs','restecg','exang','slope','ca','thal']:\n",
    "    print(df_train.groupby(feature).target.value_counts(normalize=True).sort_index())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now that we have a better feel for the data, let's build a decision tree.\n",
    "\n",
    "### Non-binary Categorical Variables\n",
    "\n",
    "If you read `sklearn`'s docs on the decision tree classifier they use, it does not support the use of categorical variables at the time of the writing of these notes.\n",
    "\n",
    "This leaves two options:\n",
    "<ol>\n",
    "    <li>One hot encoding</li>\n",
    "    <li>Removing these variables from the algorithm</li>\n",
    "</ol>\n",
    "\n",
    "We'll do both here. However, in a later notebook we'll return to this issue and see if we should be okay with building tree on one hot encoded variables in practice.\n",
    "\n",
    "#### Model Without Non-Binary Categorical Variables\n",
    "\n",
    "Build a model based on the variables that aren't categorical with more than two categories.\n",
    "\n",
    "Produce an idea of how good your model is based on its:\n",
    "<ul>\n",
    "    <li>accuracy</li>\n",
    "    <li>precision</li>\n",
    "    <li>recall</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Play around with the hyper parameters of max_depth and min_samples_split.\n",
    "\n",
    "##### Caution\n",
    "\n",
    "You should use either cross validation or set aside a small chunk of data within the training set to test your trees on. This is because decision trees can be made to fit the data perfectly if they're allowed to get deep enough. While this is great for understanding the training data, it's definitely overfitting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "# Making functions to calculate various stats\n",
    "## Accuracy\n",
    "def get_acc(prediction,y):\n",
    "    return np.round(np.sum(prediction == y)/len(y),5)*100\n",
    "## Precision\n",
    "def get_prec(prediction,y):\n",
    "    positives = prediction[prediction == 1]\n",
    "    return np.round(np.sum(positives == y[prediction == 1])/len(positives),5)*100\n",
    "## Recall\n",
    "def get_recall(prediction,y):\n",
    "    ones = y[y==1]\n",
    "    return np.round(np.sum(prediction[y==1] == ones)/len(ones),5)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "# note StratifiedKFold performs stratified splits in cv\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model With Non-Binary Categorical Variables\n",
    "\n",
    "Perform one hot encoding of the non-binary categorical variables, then repeat the prior steps. Is this model better on the test data than the last one?\n",
    "\n",
    "Which measures should we use to evaluate better in this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a summary of your findings here \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts on Decision Trees\n",
    "\n",
    "Before we close the jupyter notebook on decision trees let's leave with a few take aways.\n",
    "\n",
    "### Advantages\n",
    "<ol>\n",
    "    <li>Interpret-ability - Decision Trees are known as a white box algorithm (as opposed to the black box often used to describe machine learning). This is because you are able to entirely describe how a decision tree predicts a data points target using the logic tree.</li>\n",
    "    <li>Very Fast Predictions</li>\n",
    "    <li>Very little preprocessing of data prior to training</li>\n",
    "</ol>\n",
    "\n",
    "### Disadvantages\n",
    "<ol>\n",
    "    <li>Greediness - The algorithm is greedy meaning it may not create the optimal tree. For example, maybe the best tree involves an initial suboptimal cut, the CART algorithm won't find this tree.</li>\n",
    "    <li>Overfitting - Decision trees are very prone to overfitting the data, you can control for this using regularization hyperparameters like max_depth and min_samples_split. It's also a good idea to use cv when you can.</li>\n",
    "    <li>Orthogonal Boundaries - Because of the process of determining cut points (remember the $t_k$ from the algorithm?) decision boundaries happen at right angles. This means that classes divided by a non-horizontal or non-vertical line the decision tree will have some capturing the boundary. This can be mitigated a bit with PCA.</li>\n",
    "    <li>Sensitive - Trees are very sensitive to the training data. Removing or adding a few points can greatly change the decision boundary produced by the algorithm. One way around this is to use an averaged algorithm, like a random forest. We discuss these in the next notebook.</li>\n",
    "</ol>\n",
    "\n",
    "### Getting class probabilities\n",
    "\n",
    "You can get class probabilities from a fitted decision tree classifier using the method `predict_proba(X)` where $X$ is the feature array you want to predict on.\n",
    "\n",
    "These probabilities can then be used to calculate AUC for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# An example of getting tree probabilities\n",
    "\n",
    "# Make the classifier \n",
    "tree_clf = DecisionTreeClassifier(random_state = 440,max_depth = 3)\n",
    "\n",
    "# Fit the classifier\n",
    "tree_clf.fit(X_train,y_train)\n",
    "\n",
    "# Get the probabilities for the training set\n",
    "tree_clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
