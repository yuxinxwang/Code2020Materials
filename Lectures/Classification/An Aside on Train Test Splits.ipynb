{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Aside on Train Test Splits\n",
    "\n",
    "We brought this up in notebooks 7 and 8 from Regression, but it is worth repeating as we venture into classification.\n",
    "\n",
    "Up to this point we've mostly applied train test splits without giving much thought to how we randomly sample.\n",
    "\n",
    "This may be fine for a number of regression problems, but might cause issues in classification. \n",
    "\n",
    "## Preserving Categorical Distributions\n",
    "\n",
    "For the sake of argument let's say we have a binary variable $y$ that can be $0$ or $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.random(500)\n",
    "\n",
    "y[y > .6] = 1\n",
    "y[y < .6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(sum(y)/len(y),4), \"is the proportion of y that is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform a train test split using sklearn it is likely that our split will have a similar distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_test_split(y, test_size=.25, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(sum(y_train)/len(y_train),4), \"is the proportion of y_train that is 1\")\n",
    "print(np.round(sum(y_test)/len(y_test),4), \"is the proportion of y_test that is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the past two code blocks a number of times and watch as the proportion changes each time.\n",
    "\n",
    "Now let's try it was a new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.random(1000)\n",
    "\n",
    "y[y > .99] = 1\n",
    "y[y < .99] = 0\n",
    "\n",
    "print(np.round(sum(y)/len(y),4), \"is the proportion of y that is 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_test_split(y, test_size=.25, shuffle = True)\n",
    "\n",
    "print(np.round(sum(y_train)/len(y_train),8), \"is the proportion of y_train that is 1\")\n",
    "print(np.round(sum(y_test)/len(y_test),8), \"is the proportion of y_test that is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this enough times you'll eventually end up with a split where either the test or train set doesn't have very many observations of the $1$ class. This isn't great. All of our supervised learning algorithms operate on the assumption that our training data is pulled from the same distribution as our testing data. \n",
    " \n",
    "We can address this issue by </i>stratifying</i> our dataset. \n",
    "\n",
    "If we want say a $75-25$ split, we break our dataset into $1$s and $0$s, we randomly sample $75\\%$ of the $1$s for training data and separately randomly sample $75\\%$ of the $0$s for the training data. The remaining data is set aside for the test set.\n",
    "\n",
    "Let's put this into action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_test_split(y, test_size=.3, shuffle = True, stratify=y)\n",
    "\n",
    "print(np.round(sum(y_train)/len(y_train),9), \"is the proportion of y_train that is 1\")\n",
    "print(np.round(sum(y_test)/len(y_test),9), \"is the proportion of y_test that is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get very extreme examples like say having the target only represent $.1\\%$, or even $.00001\\%$ we have to resort to other techniques, which we discuss in the classification homework. \n",
    "\n",
    "\n",
    "For the rest of the classification materials we'll perform stratified train test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
