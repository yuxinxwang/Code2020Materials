{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines are a popular classification algorithm developed by the computer science community in the 1990s. There are three forms that are often all (mis)labeled as support vector machines that we'll work through in this notebook. Each form builds upon the last one leading to the final most general support vector machine.\n",
    "\n",
    "Throughout the notebook we'll be working with synthetic data because it most easily displays how the algorithms work.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "<ul>\n",
    "    <li>Introduce maximal margin classifiers,</li>\n",
    "    <li>Extend them to soft margin classifiers,</li>\n",
    "    <li>Use a soft margin classifier to classify cancer cases,</li>\n",
    "    <li>Learn about support vector machines in earnest,</li>\n",
    "    <li>See the kernel trick for nonlinear decision boundaries.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVMs\n",
    "\n",
    "This particular branch of Support Vector Machines are used to separate data you suspect have linear decision boundaries.\n",
    "\n",
    "### Maximal Margin Classifiers\n",
    "\n",
    "We'll start with Maximal Margin Classifiers, also known as Large Margin Classifiers.\n",
    "\n",
    "Suppose you have data like so\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random data\n",
    "np.random.seed(440)\n",
    "n_rows = 100\n",
    "diff = .1\n",
    "X = np.random.random((n_rows,2))\n",
    "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
    "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
    "\n",
    "del X\n",
    "X = np.append(X_prime,X_2prime,axis = 0)\n",
    "\n",
    "y = np.empty(np.shape(X)[0])\n",
    "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
    "y[((X[:,1] - X[:,0]) >= diff)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
    "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"x_1\",fontsize = 16)\n",
    "plt.ylabel(\"x_2\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you go about writing a decision rule for this data set? We don't need a specific formula, do your best just using words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>Spoilers Below</b>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "You can separate these two classes perfectly with a simple straight line, and more generally what's known as a hyperplane. \n",
    "\n",
    "If you're unfamiliar with the term hyperplane here's how to think about it, take a high dimensional space, any subspace that is one dimension lower is a hyperplane. So in $\\mathbb{R}^1$ a hyperplane is a point, in $\\mathbb{R}^2$ a hyperplane is a line, in $\\mathbb{R}^3$ a hyperplane is a $2$-D plane, and in $\\mathbb{R}^n$ it is an $n-1$ subspace.\n",
    "\n",
    "The question becomes what is the best line with which to separate these points? That is what the maximal margin classifier finds! Below are three examples of separating lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
    "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
    "plt.plot(np.linspace(0,1,100),np.linspace(0,1,100),'k',label=\"x_2-x_1 = 0\")\n",
    "plt.plot(np.linspace(0,1,100),1.5*np.linspace(0,1,100)-.25,'b--',label=\"x_2-1.5x_1 = -0.25\")\n",
    "plt.plot(np.linspace(0,1,100),(1/1.5)*np.linspace(0,1,100)+.19,'r-.',label = \"x_2-(1/1.5)x_1 = 0.19\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 16)\n",
    "\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three of these lines separate the two classes, but which one do you think will best generalize to future data points?\n",
    "\n",
    "The red dot-dash and blue dotted lines get a bit to close to the training data at certain points. If we use one of those two lines, it is likely that new observations (which you'd want to classify) will deviate to the wrong side of the decision boundary because of random noise. So what to do?\n",
    "\n",
    "One approach is to draw a hyperplane that maximizes the total distance from all points to the hyperplane. Another way to think about it is to draw a hyperplane, find the minimum distance from the points to the hyperlane (known as the <i>margin</i>), then make that as large as possible (<i>maximize</i> it). Hence the name <i>maximal margin classifier</i>.\n",
    "\n",
    "Let's see how to implement a maximal margin classifier in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the classifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make the classifier\n",
    "# For now don't worry about c, we'll talk about it soon\n",
    "# Same with loss\n",
    "max_margin = LinearSVC(C=1, loss=\"hinge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can take over here\n",
    "# fit the model\n",
    "max_margin.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bound(clf,X,y,pts = False):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    coefs = clf.coef_[0]\n",
    "    intercept = clf.intercept_[0]\n",
    "    \n",
    "    if coefs[0] != 0:\n",
    "        plt.plot(np.linspace(np.min(X[:,0]),np.max(X[:,0]),1000),\n",
    "                     (-intercept/coefs[1])-(coefs[0]/coefs[1])*np.linspace(np.min(X[:,0]),np.max(X[:,0]),1000),\n",
    "                        'k',label = \"SVM Boundary\")\n",
    "    else:\n",
    "        plt.plot((-intercept/coefs[0])*np.ones(1000),\n",
    "                     np.linspace(np.min(X[:,1],np.max(X[:,1]),1000),\n",
    "                        'k'),label = \"SVM Boundary\")\n",
    "        \n",
    "    if pts:\n",
    "        plt.scatter(X[y == 0,0],X[y == 0,1],color=\"orange\",label=\"0\",s=50)\n",
    "        plt.scatter(X[y == 1,0],X[y == 1,1],color=\"blue\",label=\"1\", alpha = 1,s=50)\n",
    "\n",
    "        plt.legend(fontsize=16)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\",fontsize=16)\n",
    "    plt.title(\"Decision Boundary\",fontsize = 20)\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary in black\n",
    "plot_bound(max_margin,X,y,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright let's get some practice now with our old friend the iris data set.\n",
    "\n",
    "## Practice\n",
    "\n",
    "#### Maximal Margin Classifying Irises\n",
    "\n",
    "First import the data and make a training test split, set aside $20\\%$ for testing.\n",
    "\n",
    "Then play around and find two features that look like they could be used to separate setosa irises from non-setosa irises. Note that setosas are coded as $0$ in `iris[target]`. Once you've found two features that you think are linearly separable fit the classifier using them. Note that SVMs are sensitive to the scale of the variables, so you'll need to build a pipeline here using `StandardScaler` as your scaler. Remember we did this back in the regression notebook if you need a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the data here\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and the targets here \n",
    "# Grab Features\n",
    "X = iris['data']\n",
    "\n",
    "# Grab Targets\n",
    "y = np.ones(np.shape(iris['target']))\n",
    "# Reclassify as setosa and not setosa\n",
    "y = np.where(iris['target']!=0, 0, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train test split here\n",
    "# Use 440 as the random_state\n",
    "# remember to stratify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for features with plots here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your model here \n",
    "# First a note!\n",
    "# SVMs are incredibly scale sensitive\n",
    "# So use the standard scaler on your data first.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline here\n",
    "# Call the pipeline max_margin\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit the classifier\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot your boundary using plot_bounds\n",
    "# Remember you'll need to scale X again\n",
    "# Use StandardScaler().transform\n",
    "plot_bound(max_margin.named_steps['clf'],\n",
    "           max_margin.named_steps['scaler'].transform(X_train[:,[feat_1,feat_2]]),\n",
    "           y_train,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shortly, Back to Margins\n",
    "\n",
    "Earlier we defined the margin. Pop quiz what is a margin in this context?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>Spoilers</b>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Answer: The minimum distance from the hyperplane to any point in the training set.\n",
    "\n",
    "Looking at the plot above we can see that the maximal margin classifier is what is known as a hard margin classifier, meaning that the algorithm will not allow any training points within the margins of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We won't spend time optimizing here, go ahead and predict\n",
    "# on your test set and find the test accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you may have guessed the maximum margin classifier is very sensitive to the training data. In particular data points on the edges of the margin, these are known as the <i>support vectors</i>. For instance, for the iris example we just worked with, the test set had an observation almost on our decision boundary. If this point were in the training set instead we would probably have a different decision boundary.\n",
    "\n",
    "Also note, that while we only worked with two classes, this algorithm can be extended to suport mulitple classes (see the HW).\n",
    "\n",
    "### Support Vector Classifiers (or Soft Margin Classifiers)\n",
    "\n",
    "Okay so we now have an algorithm that can separate groups that are separable by hyperplanes. But, there are two possible issues:\n",
    "<ol>\n",
    "    <li>Your data may not be able to be separated by a hyperplane</li>\n",
    "    <li>A hard margin classifier may be too sensitive to training data</li>\n",
    "</ol>\n",
    "\n",
    "We saw an example of 2 with the iris data test and train sets. For an example of 1 look at our phony data from before but with some slight alterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the random data\n",
    "np.random.seed(440)\n",
    "n_rows = 100\n",
    "diff = .1\n",
    "X = np.random.random((n_rows,2))\n",
    "X_prime = X[(X[:,1] - X[:,0]) <= -diff,:]\n",
    "X_2prime = X[(X[:,1] - X[:,0]) >= diff,:]\n",
    "X_3prime = [[.4,.9],[.6,.45],[.7,.9],[.3,.19],[.1,.4]]\n",
    "\n",
    "del X\n",
    "X = np.append(X_prime,np.append(X_2prime,X_3prime,axis = 0),axis=0)\n",
    "\n",
    "y = np.empty(np.shape(X)[0])\n",
    "y[(X[:,1] - X[:,0]) <= -diff] = 1\n",
    "y[((X[:,1] - X[:,0]) >= diff)] = 0\n",
    "y[-5] = 1\n",
    "y[-4] = 0\n",
    "y[-3] = 1\n",
    "y[-2] = 0\n",
    "y[-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
    "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"x_1\",fontsize = 16)\n",
    "plt.ylabel(\"x_2\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this new version of the data set linearly separable?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>Spoilers Below</b>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "No.\n",
    "\n",
    "But, they are almost linearly separable. \n",
    "\n",
    "If we're willing to not have a perfect separation we can still use the maximal margin classifier as our guide for a new algorithm.\n",
    "\n",
    "#### Soften up the Margin Before Classifying\n",
    "\n",
    "As we said before the maximal margin classifier is a <i>hard margin classifier</i>, meaning that instances of class $0$ are not allowed to cross the decision boundary over into the area occupied by class $1$. But, what if we relaxed that rule.\n",
    "\n",
    "`LinearSVC` is constructed to be a soft margin classifier by default. This makes sense because if a dataset is linearly separable the maximal margin classifier can be achieved by the soft margin classifier. Now that we have decided to relax how hard our margins are, we have an extra feature of the algorithm to play around with, the \"softness\" of the margin.\n",
    "\n",
    "\n",
    "This is where the hyperparameter $C$ in `LinearSVC` comes into play. This is a regularization parameter and it is quite similar to what we did in ridge regression. Here's a basic rundown of how different values of $C$ affect the hyperplane. When $C$ is small the algorithm looks to find as large a margin as possible and is okay with a few observations that end up misclassified. In the small $C$ scenario we are less concerned with fitting the training data as perfectly as possible which gives a higher bias but lower variance. In contrast, a larger $C$ leads to an algorithm the tries to fit the training data as well as it can. This leads to a narrower margin with very few misclassifications (in the training data). The large $C$ approach highly fits the training data and therfore has a higher variance but lower bias.\n",
    "\n",
    "The $C$ value that works best for your data depends and you will need to do something like a grid search to find the best one.\n",
    "\n",
    "\n",
    "Below we play around with different values of $C$ when fitting a soft margin classifier to the phony data, use `plot_bound` to see how it moves the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code here\n",
    "# Sample Solution\n",
    "for i in [.01,.1,.2,.3,.5,.75,5,10,50,100]:\n",
    "    test = LinearSVC(C=i, loss=\"hinge\")\n",
    "    test.fit(X,y)\n",
    "\n",
    "    print(\"C is\", i)\n",
    "    plot_bound(test,X,y,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "#### Using C to Classify Cancer Diagnoses\n",
    "\n",
    "We'll now implement a support vector classifier on a processed version of the popular <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">Wisconsin Breast Cancer Dataset</a>. This data was collected by researchers at the University of Wisconsin and each observation represents various measurements of a breast tumor as well as whether or not the observation was benign or malignant.\n",
    "\n",
    "We can download this data set using `sklearn`, but instead we'll load a version of the data that has been put through Principal Components Analysis (PCA). We'll cover this algorithm in the Unsupervised Learning section of the course, as well as how to properly implement it in a machine learning pipeline. For now take the data we load below as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# import the data set\n",
    "cancer = pd.read_csv(\"cancer_pca.csv\")\n",
    "\n",
    "X = cancer[['pca_1','pca_2']]\n",
    "y = cancer['cancer_outcome']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=440,test_size = .25,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0,'pca_1'],X_train.loc[y_train==0,'pca_2'],c = 'orange',label=\"malignant\")\n",
    "plt.scatter(X_train.loc[y_train==1,'pca_1'],X_train.loc[y_train==1,'pca_2'],c = 'blue',label=\"benign\",alpha = .7)\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"pca 1\",fontsize = 16)\n",
    "plt.ylabel(\"pca 2\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data seem close to linearly separable. Your job is to build a linear support vector classifier to try and classify the malignant and benign classes. As always think about the best performance measure for this problem, and optimize your hyperparameters for that measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear SVMs\n",
    "\n",
    "Up until now we've dealt with data that can be separated completely (or close to completely) with a hyperplane (a line in $2$-D).\n",
    "\n",
    "What about data like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import make_circles function\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = make_circles(n_samples=500, factor=.3, noise=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1],c = \"blue\",label=\"1\")\n",
    "plt.scatter(X[y == 0,0],X[y == 0,1],c = \"orange\",label=\"0\")\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"x_1\",fontsize = 16)\n",
    "plt.ylabel(\"x_2\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this data seems separable, but not by a line.\n",
    "\n",
    "Here's another even simpler example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(440)\n",
    "\n",
    "X = 2*np.random.random((50,1)) - 1\n",
    "y = np.ones(np.shape(X))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X[(X > .3) | (X < -.3)],y[(X > .3) | (X < -.3)],c=\"blue\",label=\"1\")\n",
    "plt.scatter(X[(X <= .3) & (X >= -.3)],y[(X <= .3) & (X >= -.3)],c=\"orange\",label=\"0\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"x_1\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also not separable by a linear boundary.\n",
    "\n",
    "Not to worry in these scenarios we do still have a couple tricks up our sleeve.\n",
    "\n",
    "### Adding Nonlinear Transformations of Our Terms\n",
    "\n",
    "Just like with linear regression, one technique is to perform nonlinear transformations of our predictors. This will then lift our data into a higher dimensional space that is linearly separable.\n",
    "\n",
    "Let's try this with the data we just looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Add a feature x_2 to X, that is just x_1^2\n",
    "## make sure it is an np array with the first column as x_1\n",
    "## and the second column as x_2\n",
    "X = np.append(X,X**2,axis=1)\n",
    "y[(X[:,0] <= .3) & (X[:,0] >= -.3)] = 0\n",
    "\n",
    "## Now plot it using the below code\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],X[(X[:,0] > .3) | (X[:,0] < -.3),1],c=\"blue\",label=\"1\")\n",
    "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],X[(X[:,0] <= .3) & (X[:,0] >= -.3),1],c=\"orange\",label=\"0\")\n",
    "\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"x_1\",fontsize=16)\n",
    "plt.ylabel(\"x_2\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC can find a perfectly classifying decision boundary\n",
    "clf = Pipeline([\n",
    "    (\"poly_features\",PolynomialFeatures(degree=2,include_bias=False)),\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"svm_clf\",LinearSVC(C=5,loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X[:,0].reshape(-1,1),y.ravel())\n",
    "\n",
    "poly = clf.named_steps['poly_features']\n",
    "scaler = clf.named_steps['scaler']\n",
    "\n",
    "plot_bound(clf.named_steps['svm_clf'],scaler.fit_transform(poly.fit_transform(X[:,0].reshape(-1,1))),y.ravel(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this sort of technique works well for your data. However, a couple issues could arise:\n",
    "<ol>\n",
    "    <li>Polynomials of too low degree cannot fit complicated decision boundaries</li>\n",
    "    <li>Polynomials of high degree introduce too many features which makes the algorithm slow</li>\n",
    "</ol>\n",
    "\n",
    "Luckily there is a different way to think about the problem!\n",
    "\n",
    "### The Kernel Trick\n",
    "\n",
    "The idea behind the kernel trick builds off of what we just did. In the previous section we took a one dimensional data set and lifted it into a two dimensional space where it could be separated. This is the essential idea behind the kernel trick. We want to transform our feature space using some function $\\phi$ into a higher dimensional feature space. However, as we mentioned doing this can created a lot of features and make our algorithm too slow to be useful. This is mainly because, without opening the mathematical hood too much, the algorithm has to compute a dot product between the transformed feature vectors.\n",
    "\n",
    "For example let's say our data has two features $x_1$ and $x_2$ and our $\\phi$ is like so:\n",
    "$$\n",
    "\\phi\\left(\\left(.\\begin{array}{c} x_1 \\\\ x_2 \\end{array} \\right)\\right) = \\left(\\begin{array}{c} x_1^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ x_2^2 \\end{array} \\right).\n",
    "$$\n",
    "For two vectors $a$ and $b$ the dot product between $\\phi(a)$ and $\\phi(b)$ is:\n",
    "$$\n",
    "\\phi(a) \\bullet \\phi(b) = a_1^2 b_1^2 + 2 a_1b_1a_2b_2 + a_2^2b_2^2 = (a_1 b_1 + a_2 b_2)^2 = (a \\bullet b)^2\n",
    "$$\n",
    "\n",
    "So even though we knew what the map $\\phi$ was to the higher space, we didn't need to know what it was to compute the dot product $\\phi(a) \\bullet \\phi(b)$!\n",
    "\n",
    "In this context a map $\\phi$ has a <i>kernel function</i>, $K$, if $\\phi(a) \\bullet \\phi(b) = K(a,b)$ where $K$ is only a function of $a$ and $b$ in the original feature space. So for the example mapping we looked at the kernel function is $K(a,b) = (a \\bullet b)^2$.\n",
    "\n",
    "Other common kernel functions are:\n",
    "<ul>\n",
    "    <li>Linear, $K(a,b) = a \\bullet b$, this gives the linear classifiers we looked at earlier,</li>\n",
    "    <li>Polynomial, $K(a,b) = (\\gamma a\\bullet b + r)^d$, where $\\gamma$ is a hyperparameter to be tuned,</li>\n",
    "    <li>Gaussian Radial Kernel (Gaussian RBF), $K(a,b) = \\exp\\left( -\\gamma ||a-b||^2 \\right)$, where $||\\bullet ||$ is the Euclidean norm,</li>\n",
    "    <li>Sigmoid, $K(a,b) = \\tanh(\\gamma a \\bullet b + r)$\n",
    "</ul>\n",
    "\n",
    "To allow you more time to play around with SVMs in `sklearn` we'll devote the rest of this notebook to coding. \n",
    "\n",
    "##### Using the Kernel Trick in `sklearn`\n",
    "\n",
    "Now that we've introduced the kernel trick we can actually use the `SVC` object in `sklearn`, there's a reason we've waited so long though, but first let's return to our $1$-D data one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],np.ones((np.shape(X)[0],1))[(X[:,0] > .3) | (X[:,0] < -.3)],c=\"blue\",label=\"1\")\n",
    "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],np.ones((np.shape(X)[0],1))[(X[:,0] <= .3) & (X[:,0] >= -.3)],c=\"orange\",label=\"0\")\n",
    "plt.yticks([])\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel(\"x_1\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a polynomial transform by hand we can use the SVM with a polynomial kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import SVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create an SVC instance with polynomial kernel\n",
    "svm_clf = SVC(kernel = \"poly\", degree = 2, coef0 = 1, C=1)\n",
    "\n",
    "# Fit, remember to only use the 0th column of X\n",
    "svm_clf.fit(X[:,0].reshape(-1,1),y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this to test the SVM we just made\n",
    "test = np.linspace(-1,1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on test\n",
    "pred = svm_clf.predict(test.reshape(-1,1))\n",
    "\n",
    "# Now plot the prediction along with the true boundary\n",
    "# and the training data\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "# plot the predictions\n",
    "plt.scatter(test[pred == 1],np.ones(np.shape(test))[pred == 1],c = \"blue\",label=\"predicts 1\")\n",
    "plt.scatter(test[pred == 0],np.ones(np.shape(test))[pred == 0],c = \"orange\",label=\"predicts 0\")\n",
    "\n",
    "# Plot the true boundary\n",
    "plt.scatter([0.3,-0.3,0.3,-0.3],[1,1,.9,.9],c = \"black\",marker=\"x\",s = 150,label=\"True Decision Boundary\")\n",
    "\n",
    "# plot the training data\n",
    "plt.scatter(X[(X[:,0] > .3) | (X[:,0] < -.3),0],\n",
    "            .9*np.ones((np.shape(X)[0],1))[(X[:,0] > .3) | (X[:,0] < -.3)],\n",
    "            c=\"blue\",label=\"training 1\",marker=\"*\")\n",
    "plt.scatter(X[(X[:,0] <= .3) & (X[:,0] >= -.3),0],\n",
    "            .9*np.ones((np.shape(X)[0],1))[(X[:,0] <= .3) & (X[:,0] >= -.3)],\n",
    "            c=\"orange\",label=\"training 0\",marker=\"*\")\n",
    "\n",
    "plt.yticks([])\n",
    "plt.ylim([.8,1.1])\n",
    "plt.xlabel(\"$x_1$\",fontsize = 16)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.title(\"Prediction from SVC\",fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we let you play around with kernels a quick note. If you suspect that your boundary is linear or a low degree polynomial, use `LinearSVC`. Why? Because the algorithm being run on your computer (or server) when you use `LinearSVC` is optimized for such a problem, so it will run better than `SVC` with a linear or low degree polynomial kernel.\n",
    "\n",
    "## Practice\n",
    "\n",
    "Now go ahead and work on the following two problems for the rest of the notebook. Use a support vector machine with kernels to build a classifier for the circles data set and moons data set given below. \n",
    "\n",
    "For both what kernel and hyperparameters give you the best accuracy, precision, recall, and auc scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle Data Set\n",
    "X_train, y_train = make_circles(n_samples=500, factor=.4, noise=.15)\n",
    "X_test, y_test = make_circles(n_samples=100, factor=.4, noise=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"blue\",label=\"1\")\n",
    "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"0\")\n",
    "\n",
    "plt.legend(fontsize = 16)\n",
    "plt.xlabel(\"x_1\",fontsize = 16)\n",
    "plt.ylabel(\"x_2\",fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moons Data Set\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "X_train,y_train = make_moons(n_samples=500, shuffle=True, noise=.3, random_state=614)\n",
    "X_test,y_test = make_moons(n_samples=100, shuffle=True, noise=.3, random_state=614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with labels\n",
    "plt.figure(figsize = (12,8))\n",
    "           \n",
    "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],color=\"orange\",label=\"0\")\n",
    "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],color=\"blue\",label=\"1\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.xlabel(\"x_1\",fontsize=16)\n",
    "plt.ylabel(\"x_2\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting scores for AUC\n",
    "\n",
    "This is the first algorithm we've encountered that doesn't seem to have a natural class probability measurement. Luckily it can be done, but it is a slightly different procedure than what we've done with past algorithms.\n",
    "\n",
    "For SVMs you need to include the argument `probability=True` when you create the classifier object. Then you can generate probabilities using `predict_proba` as before. See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use whatever SVM you used to classify the circle data set here\n",
    "svm_clf = SVC(kernel = \"rbf\", C=1,probability=True)\n",
    "\n",
    "\n",
    "svm_clf.fit(X_train,y_train)\n",
    "\n",
    "svm_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
