{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Models\n",
    "\n",
    "In this notebook we build upon what we learned about decision trees with random forests.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "<ul>\n",
    "    <li>We'll talk about the wisdom of the crowd approach,</li>\n",
    "    <li>Demonstrate Random Forests with Crescent Moons Data,</li>\n",
    "    <li>See how random forests work by collecting many trees,</li>\n",
    "    <li>You'll make a random forest model to return to the heart disease data,</li>\n",
    "    <li>Finally you'll see how random forests can be usefull for feature extraction.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisdom of the Crowd\n",
    "\n",
    "Just like an actual forest is comprised of numerous trees, a random forest is comprised of numerous decision trees. The idea behind the algorithm is our first example of <i>ensemble learning</i> which is a machine learning implementation of the <i>wisdom of the crowd</i>. \n",
    "\n",
    "If you're unfamiliar with the wisdom of the crowd think of it as follows. Suppose you want to know the answer to some question. Instead of basing the answer on a single person's response, you survey thousands or millions of people. To get your answer you aggregate all of the individual answers you've collected. In many cases this aggregated answer will be more correct than a single expert's answer.\n",
    "\n",
    "So the idea with ensemble methods is to build a number of different algorithms then average their predictions into a \"wiser\" prediction. In the case of random forests this means building many decision trees that are different (usually by some random perturbation) that are then used to produce a forest.\n",
    "\n",
    "We'll talk more generally about ensemble learning in a later notebook. For this notebook we restrict ourselves to random forests.\n",
    "\n",
    "## Seeing the Forest For the Trees\n",
    "\n",
    "Now let's restrict ourselves to random forests. We will start by playing around with some synthetic data first, then you will return to the heart disease data set and compare the random forest approach to the decision tree you built in the last notebook.\n",
    "\n",
    "First the data. We'll play around with what's known as the cresent moons data, which is a popular benchmark data set for classification algorithms. `sklearn` provides an easy function to generate the data set. We'll first plot the data without any random noise. This will allow you to see what the true relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function that makes the data set\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and target data\n",
    "# We'll assume that this is the training data for simplicity\n",
    "# Because the data is randomly generated we can always make more\n",
    "X,y = make_moons(n_samples=1000, shuffle=True, noise=0, random_state=614)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with labels\n",
    "plt.figure(figsize = (12,8))\n",
    "           \n",
    "plt.scatter(X[y == 0,0],X[y == 0,1],color=\"orange\",label=\"0\")\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1],color=\"blue\",label=\"1\")\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "plt.ylabel(\"$x_2$\",fontsize=16)\n",
    "plt.title(\"No Random Noise Crescent Moons Data\",fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot the data set with some added noise, like how we get real data. We'll plot it, then you'll fit a single decision tree to the data. Then we'll build a random forest and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and target data\n",
    "# We'll assume that this is the training data for simplicity\n",
    "# Because the data is randomly generated we can always make more\n",
    "X,y = make_moons(n_samples=1000, shuffle=True, noise=.2, random_state=614)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with labels\n",
    "plt.figure(figsize = (12,8))\n",
    "           \n",
    "plt.scatter(X[y == 0,0],X[y == 0,1],color=\"orange\",label=\"0\")\n",
    "plt.scatter(X[y == 1,0],X[y == 1,1],color=\"blue\",label=\"1\", alpha = .6)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "plt.ylabel(\"$x_2$\",fontsize=16)\n",
    "plt.title(\"Crescent Moons Data With Random Noise\",fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Decision Tree on this data\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a max depth of 4 as a start\n",
    "# Use 614 as a random state\n",
    "# Call the classifier tree_clf\n",
    "max_depth = 4\n",
    "tree_clf = DecisionTreeClassifier(max_depth = max_depth,random_state = 614)\n",
    "\n",
    "# Fit the tree\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tto help visualize the decision tree further we're going to add the decision boundary to the plot from above. In the following code block is a function that will plot the decision boundary for you, set `pts=True` when you call the function if you'd like to see the data on the plot as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to define the function\n",
    "# That will plot the decision boundary\n",
    "def plot_bound(clf,X,y,pts = False):\n",
    "    plot_step = 0.02\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
    "    y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                            np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize = (12,10))\n",
    "    cs = plt.contourf(xx, yy, Z, colors=['orange','white','blue','white'])\n",
    "\n",
    "    if pts:\n",
    "        plt.scatter(X[y == 0,0],X[y == 0,1],color=\"black\",ec=\"white\",label=\"0\",s=50)\n",
    "        plt.scatter(X[y == 1,0],X[y == 1,1],color=\"white\",label=\"1\", alpha = 1,s=50)\n",
    "\n",
    "        plt.legend(fontsize=16)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\",fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\",fontsize=16)\n",
    "    plt.title(\"Crescent Moons Data With Random Noise\",fontsize = 20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boundary\n",
    "plot_bound(tree_clf,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what a random forest fit on the same data produces. If you play around with the max_depth hyperparameter, make sure you do the same thing with the decision tree plot so you can make a direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't have a random state,\n",
    "# what happens when we rerun the code block?\n",
    "forest_clf = RandomForestClassifier(max_depth = max_depth)\n",
    "\n",
    "forest_clf.fit(X,y)\n",
    "\n",
    "plot_bound(forest_clf,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dear Diary \n",
    "\n",
    "Take a moment to write down your own observations on the differences between the random forest boundary and the decision tree boundary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Going On?\n",
    "\n",
    "So what is `sklearn` doing when we train a random forest? <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>\n",
    "\n",
    "We already know that it is somehow building many decision trees and then averaging the results, but how does it build those trees?\n",
    "\n",
    "There are a number of ways that can be done.\n",
    "\n",
    "#### Sampling Subsets of the Training Data\n",
    "\n",
    "##### Random Sampling With Replacement (Bagging)\n",
    "\n",
    "One way is to randomly sample training points with replacement from the data set, then train the algorithm on the randomly sampled set. Note that this is the default for `sklearn`'s decision trees, it can be controlled with the hyperparameter `bootstrap` a value of True uses bagging, a value of False trains each tree with the entire data set. \n",
    "\n",
    "So if your training set has $n$ points, the algorithm randomly samples $n$ points with replacement (note this is the default it can be changed to be less than the entire dataset using `max_samples`) then trains a decision tree on it. \n",
    "\n",
    "Random Forests train `n_estimators` (an input to the `sklearn` method) independent trees. The default is $100$ trees, but this can be changed.\n",
    "\n",
    "This process is more generally known as <i>bagging</i>, which we'll return to in the more general ensemble learning notebook. \n",
    "\n",
    "##### Random Sampling Without Replacement (Pasting)\n",
    "\n",
    "An alternative to bagging is to randomly select data points without replacement from the training set. This is known as <i>pasting</i>. \n",
    "\n",
    "We won't spend time on this one here since the random forests in `sklearn` don't currently support this option. We'll revisit it in the ensemble learning notebook.\n",
    "\n",
    "#### Randomly Selecting Predictors\n",
    "\n",
    "In addition to the ability to randomly sample data, every decision tree is built on a random sample of the features of the data. This means that unlike in a single decision tree where the best cut is chosen from all of the features at each step, we limit ourselves to which features we consider. \n",
    "\n",
    "Also just like in decision trees you can control the maxinum number of features considered in your model with the hyperparameter `max_features`.\n",
    "\n",
    "\n",
    "## A Note on Hyperparameters\n",
    "\n",
    "Compared to all of the algorithms we've examined this far random forests have the most hyperparameters to think about. Depending on the settings you choose for the algorithm, you could wind up with vastly different predictions. It's always important to put thought into why you choose a particular hyperparameter value.\n",
    "\n",
    "## Classifying Heart Disease with Random Forests\n",
    "\n",
    "Now we return to our heart disease data set from the decision tree notebook. \n",
    "\n",
    "Our goal is to see if we can build a random forest model that outperforms our single decision tree. For time purposes we'll limit ourselves to the model built with data that included categorical variables.\n",
    "\n",
    "Ready. Set. Go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRACTICE\n",
    "# Prepare the Data Here, \n",
    "# Make sure to get a train and test set,\n",
    "# And one hot encode the categorical variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put More Code Here\n",
    "# Making function to calculate various stats\n",
    "## Accuracy\n",
    "def get_acc(prediction,y):\n",
    "    return np.round(np.sum(prediction == y)/len(y),5)*100\n",
    "## Precision\n",
    "def get_prec(prediction,y):\n",
    "    positives = prediction[prediction == 1]\n",
    "    return np.round(np.sum(positives == y[prediction == 1])/len(positives),5)*100\n",
    "## Recall\n",
    "def get_recall(prediction,y):\n",
    "    ones = y[y==1]\n",
    "    return np.round(np.sum(prediction[y==1] == ones)/len(ones),5)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put more code here \n",
    "# CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll plot the mean measure across the folds as \n",
    "# a function of max_depth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to Decision Tree\n",
    "# make an empty array that will hold all of our accuracy measures\n",
    "accs_tree = np.empty([max_depth,n_splits])\n",
    "prec_tree = np.empty([max_depth,n_splits])\n",
    "rec_tree = np.empty([max_depth,n_splits])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose a measure and plot it against the maximum depth\n",
    "# for the decision tree and the random forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forests for Feature Extraction\n",
    "\n",
    "An additional benefit of the random forest algorithm is its ability to identify which features are importnat when determining the class. \n",
    "\n",
    "The `sklearn` algorithm measures importance in the following way. For each feature it looks at every tree and identifies the nodes using that feature to make a cut. It then measures how much those cuts reduced impurity and averages that value over all the trees in the forest. After getting the average impurity reduction for each feature, `sklearn` scales the results so that the sum of all feature importances is equal to $1$.\n",
    "\n",
    "Using your results from above select a single random forest model for the heart data. We can then use that model to examine feature importance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose your model here\n",
    "## Call the model tree_clf and then fit it using X_train_prime and y_train\n",
    "forest_clf = RandomForestClassifier(n_estimators=500,max_depth = 6)\n",
    "forest_clf.fit(X_train_prime,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance\n",
    "# This snippet of code makes a feature importance dataframe\n",
    "# then it displays it in order of decreasing importance\n",
    "names = []\n",
    "scores = []\n",
    "for name, score in zip(X_train_prime.columns,forest_clf.feature_importances_):\n",
    "    names.append(name)\n",
    "    scores.append(np.round(score,4))\n",
    "    \n",
    "score_df = pd.DataFrame({'feature':names,'importance_score':scores})\n",
    "\n",
    "score_df.sort_values('importance_score',ascending=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice feature of random forests, it allows us to understand what variables are most important, which can help us explain the algorithm. It is also useful if you need to perform feature selection.\n",
    "\n",
    "## Extra-Trees\n",
    "\n",
    "If you were sitting there thinking that you'd like to make this process even more random, you're in luck.\n",
    "\n",
    "An extension of random forests is know as extra-trees. This algorithm is just like a random forest, but in addition to randomly selecting a handful of features to optimize it also randomly selects the cutpoints instead of having the tree search for the optimal one.\n",
    "\n",
    "This algorithm is faster random forests, but does tend to have a little more bias. Typically you'll have to build both classifiers and compare measures via cross-validation to decide if an extra-trees classifier is better than a standard random forest.\n",
    "\n",
    "This algorithm can be enacted using `ExtraTreesClassifier` from `sklearn`.\n",
    "\n",
    "## Random Forest Probability\n",
    "\n",
    "Remember that another measure of algorithm \"goodness\" is the AUC score, but to calculate it you need some kind of score that can be thresholded to determine the classification. For the random forest classifier in sklearn you can get class probabilities like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first column is probability of being class 0\n",
    "# the second is probability of being class 1\n",
    "forest_clf.predict_proba(X_train_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
