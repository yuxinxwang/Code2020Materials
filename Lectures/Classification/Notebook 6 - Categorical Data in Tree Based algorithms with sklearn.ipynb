{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Data In `sklearn` Tree Based Algorithms\n",
    "\n",
    "The notes in this notebook are adapted from this blog post <a href=\"https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/\">https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/</a>.\n",
    "\n",
    "Before moving on to the next algorithm we're going to take a quick aside and discuss a shortcoming in `sklearn`'s tree based algorithms. Check out the following screenshot from the `sklearn` docs page\n",
    "<img src=\"sklearn_tree_int.png\"></img>\n",
    "\n",
    "So what's the problem here? `DecisionTreeClassifier.fit(X,y)` takes our features, $X$, and casts them as floats which are decimals. This is fine for continuous predictors, binary predictors and probably ordinals. But, what about true categorical variables? Probably not for the best.\n",
    "\n",
    "However, we should be okay right? We have one-hot encoding! Even this isn't ideal. Suppose you have one categorical predictor with ten categories, that's nine additional variables. What about one with $100$ variables, or more? This quickly becomes a problem with more complicated categorical variables. As we'll see below with a cooked up example.\n",
    "\n",
    "There are two main problems with one hot encoding and tree based methods:\n",
    "<ol>\n",
    "    <li> \n",
    "        When you one-hot encode a variable with many categories you end up with many sparse columns. The resulting sparsity virtually ensures that continuous variables are assigned higher feature importance.\n",
    "    </li>\n",
    "    <li>\n",
    "        A single level of a categorical variable must meet a very high bar in order to be selected for splitting early in the tree building. This can degrade predictive performance.\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "Let's examine this with our phony data.\n",
    "\n",
    "We'll use Nick Dingwall, Chris Potts's code to generate the data but here's the idea.\n",
    "\n",
    "Take a categorical variable, $c$, that takes values from two sets $C^+$ and $C^-$ each with $100$ unique categories, and a variable $z$ that is continuous. The target value $y$ has the following true classification rule:\n",
    "$$\n",
    "y = \\left\\lbrace \\begin{array}{c c}\n",
    "    1, & \\text{ if } & c\\in C^+ \\text{ or } z > 10\\\\\n",
    "    0, & \\text{ else.}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Just like in the real world when you collect the data you don't know this real classification rule, so you collect $10,000$ samples with $y$, $z$, and $c$ values as well as $100$ additional potential predictors (all of which are continuous for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import Dingwall and Potts's python code to generate the data\n",
    "from tree_categorical_variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_categorical is the collected data set\n",
    "# data_onehot is the onehot encoded version\n",
    "data_categorical, data_onehot = generate_dataset(\n",
    "    num_x=100, n_samples=10000, n_levels=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data as it was collected\n",
    "data_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the onehot encoded data\n",
    "data_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a decision tree that goes three layers deep and see what happens. Don't worry about the train test split here. Mess around with adding more layers if you'd like as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "# Sample Solution\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code Here\n",
    "# Sample Solution\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 3)\n",
    "tree_clf.fit(data_onehot.iloc[:,1:],data_onehot.iloc[:,0])\n",
    "\n",
    "plt.figure(figsize = (15,40))\n",
    "\n",
    "plot_tree(tree_clf, filled = True, feature_names = data_onehot.columns[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dear Diary \n",
    "\n",
    "Write down what you observed here. Is the decision tree able to identify the importance of the categorical variable?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play around with `sklearn`'s random forest classifier. Build a classifier and then look at the feature importance of the variables. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "# Sample Solution\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "# Sample Solution\n",
    "forest_clf = RandomForestClassifier(n_estimators = 100, max_depth = 3)\n",
    "forest_clf.fit(data_onehot.iloc[:,1:],data_onehot.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "# Sample Solution\n",
    "names = []\n",
    "scores = []\n",
    "for name, score in zip(data_onehot.columns[1:],forest_clf.feature_importances_):\n",
    "    names.append(name)\n",
    "    scores.append(np.round(score,4))\n",
    "    \n",
    "score_df = pd.DataFrame({'feature':names,'importance_score':scores})\n",
    "\n",
    "score_df.sort_values('importance_score',ascending=False).reset_index(drop=True).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dear Diary 2\n",
    "\n",
    "Write down what you observed about the random forest classifier here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Do?\n",
    "\n",
    "So we can see that `sklearn`'s tree based methods can struggle with categorical variables. And let's point out that the phony data set we created isn't too unrealistic. Imagine you're trying to predict how a state will vote in an upcoming election. Let's say you're trying to predict if a county will vote Democrat. $c$ could be the state the county is in. We could take $C^+$ to be the states that have historically voted Democrat and $C^-$ those that either flip or historically vote Republican. Say $z$ is the latest polling data.\n",
    "\n",
    "It's not unreasonable that you may encounter problems like this in your career as a data scientist. There are a couple of approaches you could take to address this shortcoming.\n",
    "<ul>\n",
    "   <li> If you're not in a production environment, like a personal project or research project, you may want to try R. R considers all categories of $c$ at once in its `rpart` package. However, R's `randomForest` package may have limitations on the number of unique categories $c$ has.</li>\n",
    "   <li> There are python packages that address this shortcoming. One example is <a href = \"http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html\">h2o</a>, but this package requires a bit more technical work than we'll go into during this course. However, you are encouraged to explore it on your own.</li>\n",
    "</ul>\n",
    "\n",
    "The important take away is that you should be careful using `sklearn` building decision trees on data with high cardinality categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
