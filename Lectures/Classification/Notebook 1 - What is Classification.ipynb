{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Classification?\n",
    "\n",
    "In this notebook we'll introduce a new form of statistical/supervised learning problem, classification. You'll work through this notebook as a breakout session.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "<ul>\n",
    "    <li>You'll learn what a classification problem is,</li>\n",
    "    <li>You'll work through a simple algorithm, k-nearest neighbors,</li>\n",
    "    <li>We'll introduce our first classification performance measure, accuracy.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## The Idea of Classification Problems\n",
    "\n",
    "Classification Problems are supervised learning problems where the end goal is to take in unlabeled instances and give them a label.\n",
    "\n",
    "The target, $y$, now is qualitative instead of quantitative. Meaning that it is for instance a binary choice, $0$ or $1$, or a series of categories $A$, $B$, $C$, $\\dots$.\n",
    "\n",
    "Let's bring back our classic data set, the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# for data handling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "iris_df['iris_class'] = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This chunk of code is going to plot the data\n",
    "sns.lmplot(data = iris_df, x = 'sepal_length', \n",
    "            y = 'sepal_width',hue = 'iris_class',fit_reg=False,\n",
    "            height = 8,legend=False)\n",
    "\n",
    "plt.legend(title='Iris Class', loc='upper left', \n",
    "           labels=['setosa', 'versicolor', 'virginica'], \n",
    "           fontsize = 12)\n",
    "plt.xlabel(\"Sepal Length\",fontsize = 12)\n",
    "plt.ylabel(\"Sepal Width\",fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our iris data has three distinct classes, we can imagine a world in which we'd want to build a model that takes in `petal_width`,`petal_length`,`sepal_width`, and `sepal_length` then predicts what kind of iris we have.\n",
    "\n",
    "People build entire business models around classification problems. For example, <a href=\"https://www.covermymeds.com/main/\">CoverMyMeds</a> started by solving the problem \"How can I predict whether or not my prescription will need a prior authorization form?\". <a href=\"https://www.upstart.com/\">Upstart</a> tries to predict whether or not someone will be a good candidate for their loans.\n",
    "\n",
    "These problems are ubiquitous in our every day lives. Now lets start learning how we can use supervised learning techniques to solve them.\n",
    "\n",
    "## Illustrating the Classification Process With a Simple Algorithm\n",
    "\n",
    "We'll end this notebook by working through a classification problem with one of the simplest classification algorithms, $k$-nearest neighbors.\n",
    "\n",
    "### KNN\n",
    "\n",
    "$k$-nearest neighbors is quite straightforward. When you want to classify an unlabeled point, you find the $k$ closest other data points in the data space. Whichever class is most present among the $k$ neighbors is what the algorithm classifies the unlabeled point as. In the case of ties, the class is randomly assigned from the tied classes.\n",
    "\n",
    "Let's look at a picture to see what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some random data\n",
    "# to illustrate the knn concept\n",
    "np.random.seed(440)\n",
    "xs = np.random.randn(50,2)\n",
    "os = np.random.randn(50,2)-np.array([3,0])\n",
    "\n",
    "unlabeled = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now plot that data\n",
    "plt.figure(figsize = (12,9))\n",
    "\n",
    "plt.plot(xs[:,0],xs[:,1],'rx',label = \"X\",markersize=8)\n",
    "plt.plot(os[:,0],os[:,1],'bo',label = \"O\",markersize=8)\n",
    "plt.plot(unlabeled[0],unlabeled[1],'gv',label = \"Unknown\",markersize=12)\n",
    "plt.xlabel(\"Feature 1\", fontsize = 16)\n",
    "plt.ylabel(\"Feature 2\", fontsize = 16)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot above, how would would knn classify the unlabeled point for $k=1$, $k=2$, $k=10$?\n",
    "\n",
    "### Building an iris classifier\n",
    "\n",
    "We'll now demonstrate the flow of a classification problem, by using knn to build an iris classifier.\n",
    "\n",
    "#### Preparing the Data\n",
    "\n",
    "The iris dataset is the model dataset, meaning that it is already cleaned for us. In other applications in this section of the course we'll need to take an exploration and cleaning step, but for the iris data we only need to do our test/train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can turn the data in numpy arrays\n",
    "# like so\n",
    "X = iris_df[['sepal_length','sepal_width','petal_length','petal_width']].to_numpy()\n",
    "y = iris_df[['iris_class']].to_numpy()\n",
    "\n",
    " \n",
    "# make the test data 25% of the total data\n",
    "# Set a random seed\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.25, random_state=440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model\n",
    "\n",
    "Now we can fit the model to our train data, let's use $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "knn.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures - Accuracy \n",
    "\n",
    "One way to measure how well our model did is to calculate its <i>accuracy</i>. Accuracy is the number of correct predictions divided by the number of total predictions we made. Let's see how well our model does, on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on our train set\n",
    "y_predict = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy here\n",
    "# sum a list of booleans and True gets cast as 1\n",
    "# False gets cast as 0\n",
    "print(\"Our model has a \",\n",
    "      np.round(sum(y_predict == y_train.ravel())/len(y_train)*100,2),\n",
    "      \"% accuracy on the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too bad!\n",
    "\n",
    "What could we do to our model to change our accuracy? \n",
    "\n",
    "Also, is there anything else we could do to get a better idea of the generalization error? \n",
    "\n",
    "### Cross Validation for Model Assessment\n",
    "\n",
    "Just as we did for some of our regression models we can assess multiple models at once and compare the average accuracies of them all to choose the best model.\n",
    "\n",
    "### Practice\n",
    "\n",
    "Implement CV with $5$ splits. Set a random state so you could recreate your split. Going from $1$ to $25$ neighbors find the model that has the best CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kfold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cv here\n",
    "max_neighbors = 25\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how the accuracy changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot it looks like the \"best\" model here is going to be which one? $16$ neighbors.\n",
    "\n",
    "Let's go ahead and calculate accuracy on our test set using this best model. Do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a snippet of code that prints out the test accuracy here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### knn\n",
    "\n",
    "KNN is a nice little algorithm, it is easy to understand and implement. Unlike other models we've seen and will see, there is no training period. Unfortunately, this means the algorithm does not scale well with large data sets. In addition, if your data has different distance scales, you'll need to do some cleaning before using knn. You can explore it more in the HW.\n",
    "\n",
    "#### General Classification Lessons Learned\n",
    "\n",
    "While we focused on accuracy in this notebook, it is not always the best performance measure. As we continue in the classification section we'll learn more about other important performance measures. However, the problem we presented above does show a common pattern for classification problems. You choose a desired measure, then use cross validation to find the model that optimizes that measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
