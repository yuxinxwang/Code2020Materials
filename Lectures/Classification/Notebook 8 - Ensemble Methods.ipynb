{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "We end our discussion of classification with a deeper discussion of ensemble learning.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "In particular you'll learn about the following ensemble methods:\n",
    "<ul>\n",
    "    <li>Voting Methods,</li>\n",
    "    <li>Bagging,</li>\n",
    "    <li>Pasting,</li>\n",
    "    <li>Boosting.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminding Ourselves About Ensemble Learners\n",
    "\n",
    "We introduced the concept of ensemble learning with Random Forests. Recall that the idea is we take an assortment of prediction algorithms (this can be done for classification or regression) that perform pretty well then take a sort of average of their outputs to get your final answer. \n",
    "\n",
    "These methods often outperform even the best single algorithm, and were famously used in the winning solution to the netflix prize competition, <a href=\"https://netflixprize.com/\">https://netflixprize.com/</a>.\n",
    "\n",
    "### Voting Classifiers\n",
    "\n",
    "Let's say that you have a few different classifiers that you think are pretty good, for instance a logistic regression model, a knn model, a support vector machine, and a random forest model.\n",
    "\n",
    "A voting classifier is one that looks at how each of your classifiers decides to classify a point and goes with the decision that receives the most \"votes\".\n",
    "\n",
    "Let's see how to implement this with the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = make_moons(n_samples=5000, shuffle=True, noise=.3, random_state=614)\n",
    "X_test,y_test = make_moons(n_samples=1000, shuffle=True, noise=.3, random_state=614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.scatter(X_train[y_train == 0, 0],X_train[y_train == 0,1],label=\"0\")\n",
    "plt.scatter(X_train[y_train == 1, 0],X_train[y_train == 1,1],label=\"1\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"x_2\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# The VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Models each \n",
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the voting classifier\n",
    "# it is very similar to a pipeline's syntax\n",
    "voting_clf = VotingClassifier(\n",
    "                [('lr',log_clf),\n",
    "                ('rf',rf_clf),\n",
    "                ('svm',svm_clf),\n",
    "                ('knn',knn_clf)],\n",
    "                voting = \"hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each classifier's accuracy\n",
    "for name,clf in ([\"log_clf\",log_clf],[\"rf_clf\",rf_clf],\n",
    "                 [\"svm_clf\",svm_clf],[\"knn_clf\",knn_clf],\n",
    "                 [\"voting_clf\",voting_clf]):\n",
    "    # fit the model\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # get acc\n",
    "    acc = sum(y_test == y_pred)/len(y_pred)\n",
    "    \n",
    "    print(name, np.round(acc,5))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "Remember we introduced these concepts in the random forest section. The way these work is instead of training separate classifiers on the same data set we train the same kind of classifier on different data sets.\n",
    "\n",
    "This is done by randomly sampling our training data to simulate a new draw of training data. When that sampling is done <i>with replacement</i> this is known as <i>bagging</i>, which is short for bootstrap aggregating. When sampling is performed <i>without replacement</i> it is called <i>pasting</i>. \n",
    "\n",
    "Similar to the language of `VotingClassifier`, `sklearn` offers `BaggingClasifier` to build both pasting and bagging models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = make_circles(n_samples=5000, shuffle=True, noise=.2, random_state=614)\n",
    "X_test,y_test = make_circles(n_samples=1000, shuffle=True, noise=.2, random_state=614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.scatter(X_train[y_train == 0, 0],X_train[y_train == 0,1],label=\"0\")\n",
    "plt.scatter(X_train[y_train == 1, 0],X_train[y_train == 1,1],label=\"1\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"x_2\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a knn classifier\n",
    "# if bootstrap = True we use bagging\n",
    "# if bootstrap = False it is pasting\n",
    "# n_estimators is how many models we fit\n",
    "# max_samples is the number of training points sampled\n",
    "bag_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 1000,\n",
    "                            bootstrap = True\n",
    "                           )\n",
    "\n",
    "\n",
    "# Let's use a knn classifier\n",
    "# if bootstrap = True we use bagging\n",
    "# if bootstrap = False it is pasting\n",
    "# n_estimators is how many models we fit\n",
    "# max_samples is the number of training points sampled\n",
    "paste_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 1000,\n",
    "                            bootstrap = False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll compare it to a single knn\n",
    "knn = KNeighborsClassifier(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit knn\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "acc = sum(y_test == y_pred)/len(y_pred)\n",
    "\n",
    "print(\"KNN Accuracy\", np.round(acc,5))\n",
    "\n",
    "# Fit Bagged Data\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "acc = sum(y_test == y_pred)/len(y_pred)\n",
    "\n",
    "print(\"Bagging Accuracy\", np.round(acc,5))\n",
    "\n",
    "# Fit Paste Data\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "acc = sum(y_test == y_pred)/len(y_pred)\n",
    "\n",
    "print(\"Pasting Accuracy\", np.round(acc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "#### The MNIST dataset\n",
    "\n",
    "The MNIST dataset is a database of $60,000$ handwritten digits. It has been used to help create computer vision algorithms in detecting handwritten digits. It is also a common dataset used when teaching classification. Many books and course have already introduced the algorithm at this point in their course, so we're a bit behind the curve.\n",
    "\n",
    "In this breakout session you're going to build an ensemble method to classify instances as being a digit of your choice. First I'll go through the data set with you, you'll choose your favAorite integer $0$-$9$, then get to work building an algorithm. Use one of voting classifiers, bagging, or pasting. Also instead of using accuracy try to use precision as a performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the data\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "X, y = load_digits(n_class=10, return_X_y=True)\n",
    "images = load_digits()['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data set only has a \n",
    "# small sample of the 60,000\n",
    "# digits\n",
    "\n",
    "# Each row is an image, and each column\n",
    "# is the value for an 8x8 grid\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some samples for each\n",
    "# digit\n",
    "fig,ax = plt.subplots(2,5,figsize=(14,8))\n",
    "\n",
    "for i in range(0,10):\n",
    "    ax[i//5,i%5].set_axis_off()\n",
    "    ax[i//5,i%5].imshow(images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax[i//5,i%5].text(.5,0,i,fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay now choose a digit as a target\n",
    "## then make a new vector from y\n",
    "## where all instances of your digit are labeled as 1\n",
    "## and all other instances all 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now build your train test set\n",
    "## Remember to set a random_state\n",
    "## and remember to stratify\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build your classification model here\n",
    "## choose either a voting model\n",
    "## a bagging model\n",
    "## or a pasting model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging vs Pasting\n",
    "\n",
    "Bagging tends to have higher bias than pasting. This is due to the `bootstrap=True` setting. Bootstrapping leads to a greater diversity in the subsets each predictor is trained on, making it slightly harder to detect the true pattern in the data. However, this also means that predictors tend to be less correlated leading to reduced variance when compared to pasting.\n",
    "\n",
    "Overall it has been found that bagging builds better models than pasting, so it is the preferred approach. If you have the time and computing resources you can perform CV to see which works best for your data.\n",
    "\n",
    "### Boosting \n",
    "\n",
    "Boosting is a very powerful algorithm and is one of the techniques utilized in a number of winning Kaggle Projects. The theory behind the algorithm is based on the concepts of <i>weak learners</i> and <i>strong learners</i> from the subfield in Statistical Learning on <i>PAC learnability</i>, here PAC stands for <a href=\"https://en.wikipedia.org/wiki/Probably_approximately_correct_learning\">Probably Approximately Correct</a>. We touch lightly on the theory here now.\n",
    "\n",
    "#### Weak Learner vs Strong Learner\n",
    "\n",
    "A statistical learning algorithm is referred to as <i>weak learner</i> if it does slightly better than random guessing.\n",
    "\n",
    "A statistical learning algorithm is called a <i>strong learner</i> if it can be made arbitrarily close to the true relationship.\n",
    "\n",
    "Making a weak learner is much much easier than producing a strong learner in general, however, it has been shown that if a problem is weak learnable than it is stong learnable (meaning that a strong learner exists). The fact that one exists led to the creation of algorithms and techniques to produce a strong learner. This is where boosting comes into play.\n",
    "\n",
    "#### Many Weak Learners\n",
    "\n",
    "The idea for boosting is that we take an ensemble of weak learners and <a href=\"https://www.youtube.com/watch?v=mmbuD0IEvuA\">with their power combined</a> we produce a strong learner.\n",
    "\n",
    "We'll introduce a single boosting algorithm developed by Freund and Schapire (1997).\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost is short for adaptive boosting. The concept is to train a series of weak learners, with each subsequent predictor paying more attention to training instances that were misclassified by its predecessors. Each generation of weak learner focuses more and more on the hard to classify observations.\n",
    "\n",
    "Let's see demonstrate this process, once again using the moons data. We'll use AdaBoost with a Decision Stump (a decision tree with depth$=1$) as the weak learner. We plot the AdaBoost decsion boundaries as we increase the number of weak learners used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = make_moons(n_samples=100, shuffle=True, noise=.3, random_state=614)\n",
    "X_test,y_test = make_moons(n_samples=50, shuffle=True, noise=.3, random_state=614)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,3, figsize=(14,20))\n",
    "\n",
    "j = 0\n",
    "for i in range(1,13):\n",
    "    # n_estimators controls how many weak learners we use\n",
    "    # learning_rate is a hyperparameter that controls how\n",
    "    # aggressively we correct incorrect labels\n",
    "    # algorithm is the algorithm that sklearn runs to fit the model\n",
    "    # SAMME.R or SAMME are the options, SAMME.R allows calculation\n",
    "    # of probabilities.\n",
    "    ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                n_estimators = i,\n",
    "                algorithm=\"SAMME.R\",\n",
    "                learning_rate = 0.5\n",
    "            )\n",
    "    \n",
    "    ada_clf.fit(X_train, y_train)\n",
    "    \n",
    "    x1 = np.linspace(-2.5,2.5,100)\n",
    "    x2 = np.linspace(-2.5,2.5,100)\n",
    "\n",
    "    x1v, x2v = np.meshgrid(x1,x2)\n",
    "\n",
    "    X_grid = np.concatenate([x1v.reshape(-1,1),x2v.reshape(-1,1)],axis=1)\n",
    "\n",
    "    pred_grid = pd.DataFrame(np.concatenate([X_grid,ada_clf.predict(X_grid).reshape(-1,1)],axis=1),\n",
    "                             columns = ['x1','x2','y'])\n",
    "    \n",
    "    ax[j//3,j%3].scatter(pred_grid.loc[pred_grid.y == 1,'x1'],pred_grid.loc[pred_grid.y == 1,'x2'],\n",
    "           c='antiquewhite', label=\"Predicted 1\")\n",
    "    ax[j//3,j%3].scatter(pred_grid.loc[pred_grid.y == 0,'x1'],pred_grid.loc[pred_grid.y == 0,'x2'],\n",
    "               c='black', label=\"Predicted 0\")\n",
    "    \n",
    "    ax[j//3,j%3].scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"cyan\",label=\"Training 1\")\n",
    "    ax[j//3,j%3].scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"Training 0\")\n",
    "    \n",
    "    ax[j//3,j%3].set_title(str(i) + \" Weak Learners\", fontsize=16)\n",
    "\n",
    "    \n",
    "    j = j+1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice when we increase the number of weak learners we use in the model we tend to overfit to training data. One way to control for this is to not use too many estimators. Where as always the correct number of estimators depends on the data you're fitting.\n",
    "\n",
    "##### Gradient Boosting\n",
    "\n",
    "Another popular boosting algorithm is gradient boosting. If you are interested in learning the algorithm check out the homework on Classification.\n",
    "\n",
    "### Practice\n",
    "\n",
    "Train an AdaBoost model on the MNIST data from earlier. Does it seem to outperform the model you fit earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional code here if needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Model to Use?\n",
    "\n",
    "As always this is a tough question and the answer depends on your data and computational capabilities.\n",
    "\n",
    "#### Parallelizability\n",
    "\n",
    "One advantage of voter methods, bagging, and pasting is that they can be written to be performed in parallel. This can significantly cut down on computing time.\n",
    "\n",
    "This is not possible for Boosting methods because the algorithm explicitly requires the output from the first weak learner to fit the second weak learner and so on. However, one advantage of the boosting approach is that the weak learners within the ensemble may be quicker to fit than the ensemble models in the voter, bagging, and pasting approaches.\n",
    "\n",
    "## Class is Out\n",
    "\n",
    "That's it for classification! Up next is unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ensemble Learning References\n",
    "\n",
    "<a href = \"https://link.springer.com/content/pdf/10.1007/BF00058655.pdf\">Bagging Predictors</a> by Leo Breiman\n",
    "\n",
    "<a href = \"https://link.springer.com/article/10.1023/A:1007563306331\">Pasting small votes for classification in large databases and on-line</a> by Leo Breiman\n",
    "\n",
    "<a href = \"http://rob.schapire.net/papers/Schapire99c.pdf\">A Brief Introduction to Boosting</a> by Robert E. Schapire\n",
    "\n",
    "<a href = \"https://mitpress.mit.edu/books/boosting\">Boosting</a> by  Robert E. Schapire and Yoav Freund\n",
    "\n",
    "<a href = \"https://www.csie.ntu.edu.tw/~mhyang/course/u0030/papers/schapire.pdf\">A Boosting Tutorial</a> by Robert E.  Schapire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
