{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In this notebook are some exercises to gain more experience with the Regression techniques we've discussed in class. You'll get some practice fitting models, and gain a stronger theoretical understanding of the technique as well. We'll also introduce some new important concepts that we didn't have time to cover in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLR\n",
    "\n",
    "Explain how simple linear regression works. Suppose we go out and collect some data, $X$ a single feature and $y$ the target variable. If the true relationship between $y$ and $X$ is $y = X + \\epsilon$, what should the output of SLR be?  Now suppose we mistakenly misclassify $X$ as the target and $y$ as the feature and regress $X$ on $y$. What would you expect to happen to the estimate $\\hat{\\beta_1}$? What about in the limit as the variance of $\\epsilon$ goes to $\\infty$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Introduction to Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "In this question we'll introduce the concept of maximum likelihood estimation to derive the formula for $\\hat{\\beta_1}$. Assume the standard SLR assumptions. Let $y$ denote the target variable, let $X$ denote the feature variable and suppose the true relationship between $y$ and $X$ is $y = \\beta_0 + \\beta_1 X + \\epsilon$. As usual assume there are $n$ observations.\n",
    "\n",
    "For now let's look at the first observation, $(X_1,y_1)$. The likelihood of observing $y_1$ given $X_1$ is\n",
    "$$\n",
    "f\\left(y_1|x_1;\\beta_0,\\beta_1\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{\\left(y_1 - \\left(\\beta_0 + \\beta_1 x_1\\right)\\right)^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "because we have assumed that $\\epsilon\\sim N(0,\\sigma^2)$. You can think of this as the probability of observing $y_1$ given $x_1$ and our model parameters. The goal of maximum likelihood estimation is to choose the parameters, in this case $\\beta_0$ and $\\beta_1$, that maximize the likelihood. \n",
    "\n",
    "Because we've assumed independence of our observations the likelihood of observing $y$ given $X$ is:\n",
    "$$\n",
    "f\\left(y|X;\\beta_0,\\beta_1\\right) = \\prod_{i=1}^n f\\left(y_i|X_i;\\beta_0,\\beta_1\\right) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{\\left(y_i - \\left(\\beta_0 + \\beta_1 x_i\\right)\\right)^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Take the partial derivatives of $f\\left(y|X;\\beta_0,\\beta_1\\right)$ with respect to $\\beta_0$ and $\\beta_1$, then set these equal to $0$ and solve to find the maximum likelihood estimator for simple linear regression.\n",
    "\n",
    "Hint: Try maximizing $\\log\\left(f\\left(y|X;\\beta_0,\\beta_1\\right)\\right)$ instead, because $\\log$ is a strictly increasing function this is the same as maximizing $f\\left(y|X;\\beta_0,\\beta_1\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Intervals for SLR\n",
    "\n",
    "Recall our discussion on confidence intervals for $E(y|X=X^*)$ in Notebook 2.\n",
    "\n",
    "In addition to a confidence interval for the conditional mean, you can also produce what are known as prediction intervals for $y|X=X^*$, which give us a sense of what reasonable lower and upper bounds are for $y|X=X^*$ for a given confidence level, $1-\\alpha$.\n",
    "\n",
    "Recall that the $(1-\\alpha)$ confidence interval formula for $E(y|X=X^*)$ was given by:\n",
    "$$\n",
    "\\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}\\sqrt{\\frac{1}{n} + \\frac{\\left(X^* - \\overline{X}\\right)^2}{(n-1)s_X^2}},\n",
    "$$\n",
    "\n",
    "The formula for the $(1-\\alpha)$ prediction interval is quite similar:\n",
    "$$\n",
    "\\hat{y} \\pm t_{n-2,(1-\\alpha/2)}\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}\\sqrt{1 + \\frac{1}{n} + \\frac{\\left(X^* - \\overline{X}\\right)^2}{(n-1)s_X^2}},\n",
    "$$\n",
    "to see a derivation of this formula check out, <a href=\"https://online.stat.psu.edu/stat414/node/298/\">https://online.stat.psu.edu/stat414/node/298/</a>, and note that what they refer to as MSE is $\\sqrt{\\frac{\\sum_{i=1}^n\\left(y_i - \\hat{y_i}\\right)^2}{n-2}}$. The addition of $1$ in the second square root refelects the extra uncertainty involved in predicting the actual $y$ value for a value of $X$, and comes from the error term in the statistical models, $\\epsilon$. This does not show up with the confidence interval because remember $E(\\bullet)$ is linear and $E(\\epsilon)$ is assumed to be $0$.\n",
    "\n",
    "Return to the `baseball` data and produce a $98\\%$ prediction interval around the regression line created by regressing `W` on `RD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Return to the `beer` data set. Fit the following model:\n",
    "$$\n",
    "\\text{IBU} = \\beta_0 + \\beta_1 \\text{ABV} + \\beta_2 \\text{Stout} + \\beta_3 \\text{Stout} \\times \\text{ABV} + \\epsilon\n",
    "$$\n",
    "\n",
    "Then interpret the following, $\\hat{\\beta_1},\\hat{\\beta_2},\\hat{\\beta_3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again using the `beer` data build a regression model that best predicts `Rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Significance for MLR Models\n",
    "\n",
    "Below we fit a model for the `beer` data using the `statsmodel` package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer = pd.read_csv(\"https://raw.githubusercontent.com/erdosinstitute/PythonCurriculum/master/Lectures/Regression/beer.csv\")\n",
    "beer['Stout'] = 0\n",
    "beer.loc[beer.Beer_Type == 'Stout','Stout'] = 1\n",
    "beer['const'] = 1\n",
    "beer['Stout_ABV'] = beer['Stout'] * beer['ABV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = sm.OLS(beer['IBU'], beer[['const','ABV','Stout','Stout_ABV']])\n",
    "\n",
    "fit = slr.fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this annotated version of the summary.\n",
    "<img src=\"MLR_F.png\" style=\"width:70%;\"></img>\n",
    "The circled portion of the table is the $F$-statistic and the $p$-value associated with the following hypothesis test:\n",
    "$$\n",
    "\\text{H}_0: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\text{ vs. H}_A: \\text{one of }\\beta_i\\neq 0, \\ i=1,2,3.\n",
    "$$\n",
    "This test allows you to say whether any of your predictors are significantly associated with the target $y$, when compared to the baseline model of $y=E(y)$.\n",
    "\n",
    "Return to your final `carseats` model from class. What is the $F$-statistic and associated $p$-value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Algorithms\n",
    "\n",
    "Here we'll describe two additional model selection algorithms.\n",
    "\n",
    "### Forwards Selection\n",
    "\n",
    "Say you have $m$ predictors $X_1,\\dots,X_m$, and a target $y$. \n",
    "\n",
    "Starting with an empty model you build $m$ simple linear regression models and then choose the one with lowest testing error (for instance by looking at the average cv error). Call this model $1$. \n",
    "\n",
    "Take model $1$ and go through the remaining $m-1$ features and add them one at a time to model $1$. This will give you $m-1$ two feature models. Look at the one with lowest testing error, call it model $2$. If model $2$ has lower testing error than model $1$ continue in this way and look at the remaining $m-2$ predictors. If model $1$ has the lower testing error you stop and model $1$ is the model you choose.\n",
    "\n",
    "You continue until you either find a model with lowest testing error (for example if model $3$ had lower testing error than model $4$ you chose model $3$), or until you have built the model regressing $y$ on all of $X_1,\\dots,X_m$.\n",
    "\n",
    "### Backwards Selection\n",
    "\n",
    "This algorithm is sort of the opposite of forwards selection.\n",
    "\n",
    "Again say you have $m$ predictors $X_1,\\dots,X_m$ and a target $y$.\n",
    "\n",
    "Starting with the model regressing $y$ on all of $X_1,\\dots,X_m$, remove each of the $X_i$ predictors one at a time, regressing $y$ on the remaining $m-1$ features. If one of these models has lower testing error than the full model take it and call it model $1$. If none of those models has lower testing error than the full model stick with the full model.\n",
    "\n",
    "Take model $1$ and remove each of the $m-1$ predictors one at a time, regressing $y$ on the remaining $m-2$ features. If one of those models has lower testing error than model $1$ take it and call it model $2$. If none of those models has lower testing error than model $1$ stick with model $1$.\n",
    "\n",
    "Continue in this way until you have a reduced model with lowest testing error, or until you end up with the model with no predictors, i.e. $y = E(y)$.\n",
    "\n",
    "#### Greedy Algorithms\n",
    "\n",
    "These are both <i>greedy algorithms</i> because at each step you take the move that benefits you the most in the moment, but you don't explore suboptimal paths that may be better in the long run. While you may not get the best model, you're willing to go with a model that is close to correct in a faster time. Both of these algorithms at worst require fitting $m!$ models as opposed to the $2^m$ models required for the brute force approach.\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Choose one of either forwards or backwards selection and program the algorithm to build a model to predict `Sales` from the `Advertising` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "### How to Calculate Ridge Regression Coefficients\n",
    "\n",
    "Recall that finding the ridge regression coefficients involves minimizing the following:\n",
    "$$\n",
    "||y-X\\beta||_2^2 + \\alpha ||\\beta||_2^2.\n",
    "$$\n",
    "But, this can be rewritten like so:\n",
    "$$\n",
    "(y-X\\beta)^T(y-X\\beta) + \\alpha \\beta^T \\beta.\n",
    "$$\n",
    "\n",
    "Using that rewriting and some matrix calculus you can find the the formula for $\\hat{\\beta}$ is\n",
    "$$\n",
    "\\hat{\\beta} = \\left(X^T X + \\alpha I \\right)^{-1} X^T y.\n",
    "$$\n",
    "\n",
    "Write code using `numpy` to find the ridge regression coefficients for the following data. Remembering to include the normalizing step using `StandardScaler`. Fit the data with a high degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = 3*(np.pi/2)*np.random.random(500) - 2*np.pi\n",
    "y_train = np.sin(x_train) + .3*np.random.randn(500)\n",
    "\n",
    "x_test = 3*(np.pi/2)*np.random.random(500) - 2*np.pi\n",
    "y_test = np.sin(x_test) + .3*np.random.randn(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "Elastic Net is a regularization regression algorithm that strives to set a middle ground between ridge regression and lasso. Here we set out to minimize:\n",
    "$$\n",
    "MSE + r\\alpha ||\\beta||_1 + \\frac{1-r}{2}\\alpha ||\\beta||_2^2, \\text{ for } r \\in [0,1].\n",
    "$$\n",
    "\n",
    "$r$ is another hyperparameter, when $r=1$ we recover ridge regression. If $r=0$ we recover lasso.\n",
    "\n",
    "Find the best elastic net model that includes all of the features from this `auto` data set to predict `mpg`. Use cv find the best values for $r$ and $\\alpha$. You can read the `ElasticNet` documentation here, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auto = pd.read_csv(\"auto.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "So far in class and in this HW we've learned precise formulas for the coefficients of our estimators. Let's recall that for OLS we have:\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Notice that this process involves calculating the inverse of $X^T X$, which can get quite large when the number of features grows. Computing the inverse of a large matrix is computationally strenuous so it is at times desirable to have a method that doesn't involve calculating the inverse.\n",
    "\n",
    "One approach is gradient descent. \n",
    "\n",
    "Recall that we want to minimize MSE which is equal to:\n",
    "$$\n",
    "MSE(\\beta) = \\frac{1}{n}(X\\beta - y)^T(X\\beta - y)\n",
    "$$\n",
    "If we remember some Calculus III we'll remember that for a particular value of $\\beta$, say $\\beta^*$, the direction of greatest descent at $\\beta^*$, i.e. how to get to the minimum most quickly from $\\beta^*$, is the opposite direction of the gradient, $\\nabla MSE(\\beta^*)$.\n",
    "\n",
    "Using some matrix calculus we see that:\n",
    "$$\n",
    "\\nabla MSE(\\beta^*) = \\frac{2}{n} X^T (X\\beta^* - y)\n",
    "$$\n",
    "\n",
    "#### How to Turn This Into an Algorithm\n",
    "\n",
    "In order to leverage gradient descent we make a random guess for $\\beta$, say $\\beta^{(0)}$, calculate the gradient there then move a little bit, say $\\eta$, in the direction of gradient descent. We subtract $\\eta \\nabla MSE(\\beta^{(0)})$ from $\\beta^{(0)}$ and use that as $\\beta^{(1)}$. Do this process $N$ times and we'll approach $\\hat{\\beta}$. So the process is:\n",
    "\n",
    "$$\n",
    "\\beta^{(0)} = \\beta^*,\\text{ some random guess. Then for step } k+1\n",
    "$$\n",
    "$$\n",
    "\\beta^{(k+1)} = \\beta^{(k)} - \\eta \\nabla MSE(\\beta^{(k)}) = \\beta^{(k)} - \\eta \\frac{2}{n} X^T (X\\beta^{(k)} - y), k = 1,\\dots,N\n",
    "$$\n",
    "\n",
    "#### The HW Problem\n",
    "\n",
    "Write a function using `numpy`, that takes in $y, X, \\eta,$ and $N$ and returns the gradient descent estimate. Test it on the data generated below. Then compare it to what you get using `SGDRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(5000,1000)\n",
    "\n",
    "y = X.dot(np.random.randint(-5,5,1000)) + 2*np.random.randn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an important technique that is incredibly useful. We'll see it again later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellanious Regression\n",
    "\n",
    "Load the following dataset, then find the best predictive model for predicting `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hw_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here or write here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
