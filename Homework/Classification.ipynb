{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Homework\n",
    "\n",
    "In this notebook are some exercises to gain more experience with the classification techniques we've introduced as well as present material we didn't have time to cover in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Techniques in Train Test Splits\n",
    "\n",
    "Work through the Oversampling and Undersampling notebook in the Classification Folder from Lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other KNN Measures\n",
    "\n",
    "KNN can use other distance measures. Read through the `sklearn` documentation to see what the `p` and `metric` arguments are, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a>. Then look at the wikipedia entry for Minkowski distance <a href=\"https://en.wikipedia.org/wiki/Minkowski_distance\">https://en.wikipedia.org/wiki/Minkowski_distance</a>.\n",
    "\n",
    "In \"The distance function effect on k-nearest neighbor classification for medical datasets\" Hu et. al. demonstrate that the Minkowski distance can lead to horrible performance using various medical data sets, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/\">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/</a>.\n",
    "\n",
    "See how performance is affected on the iris data set when you use different distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Trade-Off\n",
    "\n",
    "Return to the logistic regression example from notebook 2.\n",
    "\n",
    "Plot the precision and the recall as a function of the probability cutoff. Notice that as one increases the other tends to decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"random_binary.csv\",delimiter = \",\")\n",
    "X = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a stratified test train split\n",
    "# Practice, write the code to do that in these two blocks\n",
    "# First import the package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split the data\n",
    "# Have 20% for testing\n",
    "# Set 614 as the random state\n",
    "# and stratify the split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                 test_size = .2,\n",
    "                                                 random_state = 614,\n",
    "                                                 shuffle = True,\n",
    "                                                 stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Classification Algorithms for Regression\n",
    "\n",
    "Virtually all of our classification techniques can also be used for regression purposes as well. These are nice because they allow you to have a diverse array of regression models. Each model makes different types of errors on the data. For instance while linear regression models may have issues with multicolinearity, random forest regression models are typically fine with multicolinearity because the decision tree structure doesn't rely on the existence of a linear relationship between the target and feature.\n",
    "\n",
    "We briefly introduce regression versions of the classification algorithms and then refer you to their documentation. Work through these and answer the questions as you go along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the following suppose $y$ is the target with feature matrix, $X$.\n",
    "\n",
    "### KNN Regression\n",
    "\n",
    "knn regression works as follows. For an observation you'd like to predict on take in the target values for its $k$ closest training neighbors in the feature space. The prediction for the observation is then calculated by taking the average of the $k$ closest training neighbors. This average can be the arithmetic mean, a mean weighted by the inverse of the distance to the observation, or any custom weight function you'd like.\n",
    "\n",
    "Go to section 1.6.3 in the sklearn docs, <a href=\"https://scikit-learn.org/stable/modules/neighbors.html\">https://scikit-learn.org/stable/modules/neighbors.html</a>, to read what `sklearn` has to say about knn regression and see how to implement it in python.\n",
    "\n",
    "Testing your understanding from the documentation to predict on the following data with a knn model. Use `np.linspace` to plot the model fit along the training data. Compare the fit to a standard linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the data\n",
    "x_train = np.linspace(-5,5,100)\n",
    "y_train = x*(x-1)*(x+2) + .5*np.random.randn(100)\n",
    "\n",
    "x_test = np.linspace(-5,5,100)\n",
    "y_test = x*(x-1)*(x+2) + .5*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model and fit it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Code here\n",
    "\n",
    "Improving Regressors using Boosting Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Regression Methods\n",
    "\n",
    "Both decision trees and random forests can be used in regression problems. One nice feature here is that you don't need to go through the process of exploring important feature by hand as you did with linear regression.\n",
    "\n",
    "The process is the same as with tree based classification the algorithm goes through all the possible features and makes a cut based on improving the training error from the split. Once a split is made the regression is done by looking at the average target value of the instances in each node of the split. So for example if the test observation you want to predict falls into node $A$ of the tree, then the prediction would be the average target value of the $A$ node.\n",
    "\n",
    "You can implement a single tree using `DecisionTreeRegressor`, or as is likely preferred `RandomForestRegressor`.\n",
    "\n",
    "You can read the documentation here, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</a>, and here <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a>. \n",
    "\n",
    "As discussed in Regression Notebook 5 these regression methods are able to avoid some of the issues of multicolinearity.\n",
    "\n",
    "Read in the following hitters data.\n",
    "\n",
    "Then make a train test split.\n",
    "\n",
    "Then using tree regression methods build a model to predict `Salary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = pd.read_csv(\"Hitters.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression\n",
    "\n",
    "Regression can also be done using support vector machines.\n",
    "\n",
    "It is implemented using `LinearSVR`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html\">https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html</a>, and `SVR`, <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html</a>.\n",
    "\n",
    "### Ensemble Regression\n",
    "\n",
    "Once you've fit a number of good regression models you can create an ensemble regressor. All of the ensemble methods we've discussed have regression versions, `VotingRegressor`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html</a>, `BaggingRegressor` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html</a>, and `AdaBoostRegressor` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html</a>.\n",
    "\n",
    "Return to any of the regression problems from the regression portion of the course. Build an ensemble of regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Fit a Logistic Regression Model\n",
    "\n",
    "We return to Maximum Likelihood Estimation from the Regression Notebook.\n",
    "\n",
    "Recall that in logistic regression we are interested in $P(y=1|X)$ let's call this $p(X;\\beta)$. In logistic regression we're modeling this as:\n",
    "$$\n",
    "p(X;\\beta) = \\frac{1}{1 + e^{-X\\beta}}.\n",
    "$$\n",
    "\n",
    "Now because our training data exists in a binary state we can't rely on the same procedure we did for linear regression. We instead use maximum likelihood estimation. We first must write out the likelihood function.\n",
    "\n",
    "First attempt to set up the $\\log$-likelihood for the logistic regression model, hint: we can think of $y_i$ as a bernouli random variable with probability parameter $p_i=p(X_i;\\beta)$.\n",
    "\n",
    "\n",
    "After you've accomplished that read through this reference starting at page 5 to see the derivation of the maximum likelihood estimate for logistic regression, <a href=\"https://cseweb.ucsd.edu/~elkan/250B/logreg.pdf\">https://cseweb.ucsd.edu/~elkan/250B/logreg.pdf</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Classes\n",
    "\n",
    "All of the algorithms we've discussed in the classification (with the exception of knn) have been limited to two output classes. We now briefly demonstrate how to extend each model to have multiple classes with `sklearn` and leave it up to you to pursue the theory involved on your own time.\n",
    "\n",
    "#### Logistic Regression -- Linear Discriminant Analysis\n",
    "\n",
    "Learn about the theory behind linear discriminant analysis here, <a href=\"https://web.stanford.edu/class/stats202/content/lec9.pdf\">https://web.stanford.edu/class/stats202/content/lec9.pdf</a>.\n",
    "\n",
    "In `sklearn` it can be implemented with `LinearDiscriminantAnalysis`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html</a>.\n",
    "\n",
    "Use it to classify the toy data set below. Plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([2*np.random.randn(50,2) + [2,2],\n",
    "                    2*np.random.randn(50,2) + [-1,-1],\n",
    "                    1*np.random.randn(50,2) + [-3,4]])\n",
    "\n",
    "y_train = np.concatenate([np.zeros(50),np.ones(50),2*np.ones(50)])\n",
    "\n",
    "\n",
    "X_test = np.concatenate([2*np.random.randn(50,2) + [2,2],\n",
    "                    2*np.random.randn(50,2) + [-1,-1],\n",
    "                    1*np.random.randn(50,2) + [-3,4]])\n",
    "\n",
    "y_test = np.concatenate([np.zeros(50),np.ones(50),2*np.ones(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.scatter(X[:50,0],X[:50,1],c='r',label=\"0\")\n",
    "plt.scatter(X[50:100,0],X[50:100,1],c='b',label=\"1\")\n",
    "plt.scatter(X[100:,0],X[100:,1],c='g',label=\"2\")\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize=14)\n",
    "plt.ylabel(\"$x_2$\",fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make and fit the model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees and Random Forests\n",
    "\n",
    "These methods naturally accept multiple classes with no additional work.\n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(test.fit(X_train,y_train),filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier can be imported like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs\n",
    "\n",
    "SVMs also naturally allow for multiple output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here\n",
    "x1 = np.linspace(-10,10,100)\n",
    "x2 = np.linspace(-10,10,100)\n",
    "\n",
    "x1v, x2v = np.meshgrid(x1,x2)\n",
    "\n",
    "X_grid = np.concatenate([x1v.reshape(-1,1),x2v.reshape(-1,1)],axis=1)\n",
    "\n",
    "pred_grid = pd.DataFrame(np.concatenate([X_grid,svc.predict(X_grid).reshape(-1,1)],axis=1),\n",
    "                         columns = ['x1','x2','y'])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(pred_grid.loc[pred_grid.y == 1,'x1'],pred_grid.loc[pred_grid.y == 1,'x2'],\n",
    "           c='antiquewhite', label=\"Predicted 1\")\n",
    "plt.scatter(pred_grid.loc[pred_grid.y == 0,'x1'],pred_grid.loc[pred_grid.y == 0,'x2'],\n",
    "           c='black', label=\"Predicted 0\")\n",
    "plt.scatter(pred_grid.loc[pred_grid.y == 2,'x1'],pred_grid.loc[pred_grid.y == 2,'x2'],\n",
    "           c='grey', label=\"Predicted 2\")\n",
    "\n",
    "plt.scatter(X_train[y_train == 1,0],X_train[y_train == 1,1],c = \"cyan\",label=\"Training 1\")\n",
    "plt.scatter(X_train[y_train == 0,0],X_train[y_train == 0,1],c = \"orange\",label=\"Training 0\")\n",
    "plt.scatter(X_train[y_train == 2,0],X_train[y_train == 2,1],c = \"red\",label=\"Training 2\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Ensemble Methods work in exactly the same way.\n",
    "\n",
    "\n",
    "Practice by building an ensemble model to predict on the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na&iuml;ve Bayes Classifier\n",
    "\n",
    "Na&iuml;ve Bayes is a popular classifyication technique, that is often used to in email <a href=\"https://www.spam.com/\">spam</a> filter examples, <a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\">https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering</a>.\n",
    "\n",
    "Read up on the algorithm in the sklearn docs <a href=\"https://scikit-learn.org/stable/modules/naive_bayes.html\">https://scikit-learn.org/stable/modules/naive_bayes.html</a>, and this pdf presentation from UPenn Engineering, <a href=\"https://www.seas.upenn.edu/~cis391/Lectures/naive-bayes-spam-2015.pdf\">https://www.seas.upenn.edu/~cis391/Lectures/naive-bayes-spam-2015.pdf</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "In addition to the popular AdaBoost there is the Gradient Boosting Algorithm. Just like AdaBoost Gradient Boosting sequentially adds a new weak learner to the model in order to improve the predictions at each step. \n",
    "\n",
    "While AdaBoost learns on the difficult to predict instance by weighting them differently, Gradient Boosting works by having the next weak learner predicting the residual errors from the previous weak learner.\n",
    "\n",
    "Let's show how we can implement this by hand using a Gradient Boosted Regression Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some toy data\n",
    "x = np.linspace(-.5,.5,50)\n",
    "y = x**2 + .025*np.random.randn(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll fit a series of regression trees\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg4 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg5 = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1.fit(x.reshape(-1,1),y)\n",
    "\n",
    "# get the residuals\n",
    "y2 = y - tree_reg1.predict(x.reshape(-1,1))\n",
    "\n",
    "tree_reg2.fit(x.reshape(-1,1),y2)\n",
    "\n",
    "# get the next round of residuals\n",
    "y3 = y2 - tree_reg2.predict(x.reshape(-1,1))\n",
    "\n",
    "tree_reg3.fit(x.reshape(-1,1),y3)\n",
    "\n",
    "# get the next round\n",
    "y4 = y3 - tree_reg3.predict(x.reshape(-1,1))\n",
    "\n",
    "tree_reg4.fit(x.reshape(-1,1),y4)\n",
    "\n",
    "# get next round of residuals\n",
    "y5 = y4 - tree_reg4.predict(x.reshape(-1,1))\n",
    "\n",
    "tree_reg5.fit(x.reshape(-1,1),y5)\n",
    "\n",
    "# get last round of residuals.\n",
    "y6 = tree_reg5.predict(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot to demonstrate\n",
    "fig, ax = plt.subplots(5,2,figsize=(14,20), sharex=True, sharey=True)\n",
    "\n",
    "# The first row has two of the same\n",
    "# plot since we have no residuals yet\n",
    "ax[0,0].scatter(x,y,c='b')\n",
    "ax[0,1].scatter(x,y,c='b')\n",
    "\n",
    "# now add in the fitted regression\n",
    "ax[0,0].plot(x,tree_reg1.predict(x.reshape(-1,1)),'-r')\n",
    "ax[0,1].plot(x,tree_reg1.predict(x.reshape(-1,1)),'-r')\n",
    "\n",
    "## second row\n",
    "ax[1,0].scatter(x,y2,c='b')\n",
    "ax[1,1].scatter(x,y,c='b')\n",
    "\n",
    "# now add in the fitted regression\n",
    "ax[1,0].plot(x,tree_reg2.predict(x.reshape(-1,1)),'-r')\n",
    "ax[1,1].plot(x,tree_reg1.predict(x.reshape(-1,1)) + \n",
    "                 tree_reg2.predict(x.reshape(-1,1)),'-r')\n",
    "\n",
    "## third row\n",
    "ax[2,0].scatter(x,y3,c='b')\n",
    "ax[2,1].scatter(x,y,c='b')\n",
    "\n",
    "# now add in the fitted regression\n",
    "ax[2,0].plot(x,tree_reg3.predict(x.reshape(-1,1)),'-r')\n",
    "ax[2,1].plot(x,tree_reg1.predict(x.reshape(-1,1)) + \n",
    "                 tree_reg2.predict(x.reshape(-1,1)) +\n",
    "                 tree_reg3.predict(x.reshape(-1,1)),'-r')\n",
    "\n",
    "## fourth row\n",
    "ax[3,0].scatter(x,y4,c='b')\n",
    "ax[3,1].scatter(x,y,c='b')\n",
    "\n",
    "# now add in the fitted regression\n",
    "ax[3,0].plot(x,tree_reg4.predict(x.reshape(-1,1)),'-r')\n",
    "ax[3,1].plot(x,tree_reg1.predict(x.reshape(-1,1)) + \n",
    "                 tree_reg2.predict(x.reshape(-1,1)) +\n",
    "                 tree_reg3.predict(x.reshape(-1,1)) +\n",
    "                 tree_reg4.predict(x.reshape(-1,1)),'-r')\n",
    "\n",
    "\n",
    "## fifth row\n",
    "ax[4,0].scatter(x,y5,c='b')\n",
    "ax[4,1].scatter(x,y,c='b')\n",
    "\n",
    "# now add in the fitted regression\n",
    "ax[4,0].plot(x,tree_reg5.predict(x.reshape(-1,1)),'-r')\n",
    "ax[4,1].plot(x,tree_reg1.predict(x.reshape(-1,1)) + \n",
    "                 tree_reg2.predict(x.reshape(-1,1)) +\n",
    "                 tree_reg3.predict(x.reshape(-1,1)) +\n",
    "                 tree_reg4.predict(x.reshape(-1,1)) +\n",
    "                 tree_reg5.predict(x.reshape(-1,1)),'-r')\n",
    "\n",
    "ax[0,0].set_title(\"Residual Plots\", fontsize=16)\n",
    "ax[0,1].set_title(\"Original Data Plot\", fontsize=16)\n",
    "\n",
    "ax[0,0].set_ylabel(\"Model 1\", fontsize=14)\n",
    "ax[1,0].set_ylabel(\"Model 2\", fontsize=14)\n",
    "ax[2,0].set_ylabel(\"Model 3\", fontsize=14)\n",
    "ax[3,0].set_ylabel(\"Model 4\", fontsize=14)\n",
    "ax[4,0].set_ylabel(\"Model 5\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do all of this at the same time with the `GradientBoostingRegressor` Class in `sklearn`. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The larger the learning rate the faster the algorithm\n",
    "# learns\n",
    "# but be careful you can easily overshoot the optimal solution this way\n",
    "# see what happens if learning_rate is 10\n",
    "gbr = GradientBoostingRegressor(max_depth = 2,\n",
    "                                n_estimators = 5,\n",
    "                                learning_rate=.5)\n",
    "\n",
    "gbr.fit(x.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the gradient boosted regressor prediction.\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(x,y,c='b')\n",
    "\n",
    "plt.plot(x,gbr.predict(x.reshape(-1,1)),'-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go through and make a test set for this synthetic data. Find the `max_depth` that produces the lowest test error. Plot the test error as a function of the `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
